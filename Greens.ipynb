{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from IPython.display import Image, SVG, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST function\n",
    "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
    "\n",
    "  valid_prop = 1 - train_prop\n",
    "\n",
    "  discard_prop = 1 - keep_prop\n",
    "\n",
    "  transform = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize((0.1307,), (0.3081,))]\n",
    "      )\n",
    "\n",
    "\n",
    "  with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "\n",
    "      full_train_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=True, download=True, transform=transform\n",
    "          )\n",
    "      full_test_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=False, download=True, transform=transform\n",
    "          )\n",
    "\n",
    "  train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "      full_train_set,\n",
    "      [train_prop * keep_prop, valid_prop * keep_prop, discard_prop]\n",
    "      )\n",
    "  test_set, _ = torch.utils.data.random_split(\n",
    "      full_test_set,\n",
    "      [keep_prop, discard_prop]\n",
    "      )\n",
    "\n",
    "  print(\"Number of examples retained:\")\n",
    "  print(f\"  {len(train_set)} (training)\")\n",
    "  print(f\"  {len(valid_set)} (validation)\")\n",
    "  print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "  return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "NUM_INPUTS = np.product(train_set.dataset.data[0].shape) # size of an MNIST image\n",
    "NUM_OUTPUTS = 10 # number of MNIST classes\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  Simple multilayer perceptron model class with one hidden layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_inputs=NUM_INPUTS,\n",
    "      num_hidden=100,\n",
    "      num_outputs=NUM_OUTPUTS,\n",
    "      activation_type=\"sigmoid\",\n",
    "      bias=False,\n",
    "      ):\n",
    "    \"\"\"\n",
    "    Initializes a multilayer perceptron with a single hidden layer.\n",
    "\n",
    "    Arguments:\n",
    "    - num_inputs (int, optional): number of input units (i.e., image size)\n",
    "    - num_hidden (int, optional): number of hidden units in the hidden layer\n",
    "    - num_outputs (int, optional): number of output units (i.e., number of\n",
    "      classes)\n",
    "    - activation_type (str, optional): type of activation to use for the hidden\n",
    "      layer ('sigmoid', 'tanh', 'relu' or 'linear')\n",
    "    - bias (bool, optional): if True, each linear layer will have biases in\n",
    "      addition to weights\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_inputs = num_inputs\n",
    "    self.num_hidden = num_hidden\n",
    "    self.num_outputs = num_outputs\n",
    "    self.activation_type = activation_type\n",
    "    self.bias = bias\n",
    "\n",
    "    # default weights (and biases, if applicable) initialization is used\n",
    "    # see https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py\n",
    "    self.lin1 = torch.nn.Linear(num_inputs, num_hidden, bias=bias)\n",
    "    self.lin2 = torch.nn.Linear(num_hidden, num_outputs, bias=bias)\n",
    "\n",
    "    self._store_initial_weights_biases()\n",
    "\n",
    "    self._set_activation() # activation on the hidden layer\n",
    "    self.softmax = torch.nn.Softmax(dim=1) # activation on the output layer\n",
    "\n",
    "\n",
    "  def _store_initial_weights_biases(self):\n",
    "    \"\"\"\n",
    "    Stores a copy of the network's initial weights and biases.\n",
    "    \"\"\"\n",
    "\n",
    "    self.init_lin1_weight = self.lin1.weight.data.clone()\n",
    "    self.init_lin2_weight = self.lin2.weight.data.clone()\n",
    "    if self.bias:\n",
    "      self.init_lin1_bias = self.lin1.bias.data.clone()\n",
    "      self.init_lin2_bias = self.lin2.bias.data.clone()\n",
    "\n",
    "  def _set_activation(self):\n",
    "    \"\"\"\n",
    "    Sets the activation function used for the hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    if self.activation_type.lower() == \"sigmoid\":\n",
    "      self.activation = torch.nn.Sigmoid() # maps to [0, 1]\n",
    "    elif self.activation_type.lower() == \"tanh\":\n",
    "      self.activation = torch.nn.Tanh() # maps to [-1, 1]\n",
    "    elif self.activation_type.lower() == \"relu\":\n",
    "      self.activation = torch.nn.ReLU() # maps to positive\n",
    "    elif self.activation_type.lower() == \"identity\":\n",
    "      self.activation = torch.nn.Identity() # maps to same\n",
    "    else:\n",
    "      raise NotImplementedError(\n",
    "          f\"{self.activation_type} activation type not recognized. Only \"\n",
    "          \"'sigmoid', 'relu' and 'identity' have been implemented so far.\"\n",
    "          )\n",
    "\n",
    "  def forward(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Runs a forward pass through the network.\n",
    "\n",
    "    Arguments:\n",
    "    - X (torch.Tensor): Batch of input images.\n",
    "    - y (torch.Tensor, optional): Batch of targets. This variable is not used\n",
    "      here. However, it may be needed for other learning rules, to it is\n",
    "      included as an argument here for compatibility.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred (torch.Tensor): Predicted targets.\n",
    "    \"\"\"\n",
    "\n",
    "    h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
    "    y_pred = self.softmax(self.lin2(h))\n",
    "    return y_pred\n",
    "\n",
    "  def forward_backprop(self, X):\n",
    "    \"\"\"\n",
    "    Identical to forward(). Should not be overwritten when creating new\n",
    "    child classes to implement other learning rules, as this method is used\n",
    "    to compare the gradients calculated with other learning rules to those\n",
    "    calculated with backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
    "    y_pred = self.softmax(self.lin2(h))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "  def list_parameters(self):\n",
    "    \"\"\"\n",
    "    Returns a list of model names for a gradient dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - params_list (list): List of parameter names.\n",
    "    \"\"\"\n",
    "\n",
    "    params_list = list()\n",
    "\n",
    "    for layer_str in [\"lin1\", \"lin2\"]:\n",
    "      params_list.append(f\"{layer_str}_weight\")\n",
    "      if self.bias:\n",
    "        params_list.append(f\"{layer_str}_bias\")\n",
    "\n",
    "    return params_list\n",
    "\n",
    "\n",
    "  def gather_gradient_dict(self):\n",
    "    \"\"\"\n",
    "    Gathers a gradient dictionary for the model's parameters. Raises a\n",
    "    runtime error if any parameters have no gradients.\n",
    "\n",
    "    Returns:\n",
    "    - gradient_dict (dict): A dictionary of gradients for each parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    params_list = self.list_parameters()\n",
    "\n",
    "    gradient_dict = dict()\n",
    "    for param_name in params_list:\n",
    "      layer_str, param_str = param_name.split(\"_\")\n",
    "      layer = getattr(self, layer_str)\n",
    "      grad = getattr(layer, param_str).grad\n",
    "      if grad is None:\n",
    "        raise RuntimeError(\"No gradient was computed\")\n",
    "      gradient_dict[param_name] = grad.detach().clone().numpy()\n",
    "\n",
    "    return gradient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and dataloaders\n",
    "\n",
    "# Model\n",
    "NUM_HIDDEN = 100\n",
    "ACTIVATION = \"sigmoid\" # output constrained between 0 and 1\n",
    "BIAS = False\n",
    "\n",
    "MLP = MultiLayerPerceptron(\n",
    "    num_hidden=NUM_HIDDEN,\n",
    "    activation_type=ACTIVATION,\n",
    "    bias=BIAS,\n",
    "    )\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an SGD Optimizer\n",
    "class BasicOptimizer(torch.optim.Optimizer):\n",
    "  \"\"\"\n",
    "  Simple optimizer class based on the SGD optimizer.\n",
    "  \"\"\"\n",
    "  def __init__(self, params, lr=0.01, weight_decay=0):\n",
    "    \"\"\"\n",
    "    Initializes a basic optimizer object.\n",
    "\n",
    "    Arguments:\n",
    "    - params (generator): Generator for torch model parameters.\n",
    "    - lr (float, optional): Learning rate.\n",
    "    - weight_decay (float, optional): Weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    if lr < 0.0:\n",
    "        raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "    if weight_decay < 0.0:\n",
    "        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "\n",
    "    defaults = dict(\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "    super().__init__(params, defaults)\n",
    "\n",
    "  def step(self):\n",
    "      \"\"\"\n",
    "      Performs a single optimization step.\n",
    "      \"\"\"\n",
    "\n",
    "      for group in self.param_groups:\n",
    "        for p in group[\"params\"]:\n",
    "\n",
    "          # only update parameters with gradients\n",
    "          if p.grad is not None:\n",
    "\n",
    "            # apply weight decay to gradient, if applicable\n",
    "            if group[\"weight_decay\"] != 0:\n",
    "              p.grad = p.grad.add(p, alpha=group[\"weight_decay\"])\n",
    "\n",
    "            # apply gradient-based update\n",
    "            p.data.add_(p.grad, alpha=-group[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing SGD optimizer\n",
    "\n",
    "# note, can add a weight decay term to optimizer as well...\n",
    "LR = 0.01\n",
    "backprop_optimizer = BasicOptimizer(MLP.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolen-Pollack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
