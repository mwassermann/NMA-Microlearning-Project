{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Much code taken from Neuromatch NeuroAI 2024 Microlearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'net_ff_model' from 'd:\\\\MyFolders\\\\project\\\\2024summer\\\\NeuroAI\\\\NMA-Microlearning-Project\\\\net_ff_model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependencies\n",
    "from IPython.display import Image, SVG, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "import io\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Plotting and metrics imports\n",
    "from metrics import get_plotting_color, plot_examples, plot_class_distribution, plot_results, plot_scores_per_class, plot_weights\n",
    "\n",
    "## Other functions imports\n",
    "from helpers import sigmoid, ReLU, add_bias, create_batches, calculate_accuracy, calculate_cosine_similarity, calculate_grad_snr\n",
    "\n",
    "## MLP imports\n",
    "from MLP import MLP, NodePerturbMLP, KolenPollackMLP\n",
    "\n",
    "## FF imports\n",
    "import net_ff_model\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import importlib\n",
    "importlib.reload(net_ff_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 9,
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST function\n",
    "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
    "\n",
    "  valid_prop = 1 - train_prop\n",
    "\n",
    "  discard_prop = 1 - keep_prop\n",
    "\n",
    "  transform = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.onlineize((0.1307,), (0.3081,))]\n",
    "      )\n",
    "\n",
    "\n",
    "  with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    \n",
    "    rng_data = np.random.default_rng(seed=42)\n",
    "    train_num = 50000\n",
    "    shuffled_train_idx = rng_data.permutation(train_num)\n",
    "\n",
    "    full_train_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=True, download=True, transform=transform)\n",
    "    full_test_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=False, download=True, transform=transform)\n",
    "    \n",
    "    full_train_images = full_train_set.data.numpy().astype(float) / 255\n",
    "    train_images = full_train_images[shuffled_train_idx[:train_num]].reshape((-1, 784)).T.copy()\n",
    "    valid_images = full_train_images[shuffled_train_idx[train_num:]].reshape((-1, 784)).T.copy()\n",
    "    test_images = (full_test_set.data.numpy().astype(float) / 255).reshape((-1, 784)).T\n",
    "\n",
    "    full_train_labels = torch.nn.functional.one_hot(full_train_set.targets, num_classes=10).numpy()\n",
    "    train_labels = full_train_labels[shuffled_train_idx[:train_num]].T.copy()\n",
    "    valid_labels = full_train_labels[shuffled_train_idx[train_num:]].T.copy()\n",
    "    test_labels = torch.nn.functional.one_hot(full_test_set.targets, num_classes=10).numpy().T\n",
    "\n",
    "    train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "      full_train_set, [train_prop * keep_prop, valid_prop * keep_prop, discard_prop])\n",
    "    test_set, _ = torch.utils.data.random_split(\n",
    "      full_test_set, [keep_prop, discard_prop])\n",
    "\n",
    "  print(\"Number of examples retained:\")\n",
    "  print(f\"  {len(train_set)} (training)\")\n",
    "  print(f\"  {len(valid_set)} (validation)\")\n",
    "  print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "  return train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 10,
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples retained:\n",
      "  24001 (training)\n",
      "  5999 (validation)\n",
      "  5000 (test)\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels = download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 11,
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HYPERPARAMETERS\n",
    "NUM_INPUTS = 784\n",
    "NUM_OUTPUTS = 10\n",
    "numhidden = 500\n",
    "batchsize = 128\n",
    "initweight = 0.1\n",
    "learnrate = 0.001\n",
    "noise = 0.1\n",
    "numepochs = 25\n",
    "numrepeats = 1\n",
    "numbatches = int(train_images.shape[1] / batchsize)\n",
    "numupdates = numepochs * numbatches\n",
    "activation = 'sigmoid'\n",
    "report = True\n",
    "rep_rate = 1\n",
    "seed = 12345\n",
    "numupdates*batchsize"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 12,
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
    "indices = np.random.choice(test_images.shape[1], size=1000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current training loss:  0.68  epochs of training. Current testing loss:  0.66\n",
      "...completed  2.0  epochs of training. Current training loss:  0.49  epochs of training. Current testing loss:  0.46\n",
      "...completed  3.0  epochs of training. Current training loss:  0.4  epochs of training. Current testing loss:  0.36\n",
      "...completed  4.0  epochs of training. Current training loss:  0.35  epochs of training. Current testing loss:  0.31\n",
      "...completed  5.0  epochs of training. Current training loss:  0.31  epochs of training. Current testing loss:  0.28\n",
      "...completed  6.0  epochs of training. Current training loss:  0.29  epochs of training. Current testing loss:  0.26\n",
      "...completed  7.0  epochs of training. Current training loss:  0.27  epochs of training. Current testing loss:  0.24\n",
      "...completed  8.0  epochs of training. Current training loss:  0.26  epochs of training. Current testing loss:  0.23\n",
      "...completed  9.0  epochs of training. Current training loss:  0.25  epochs of training. Current testing loss:  0.22\n",
      "...completed  10.0  epochs of training. Current training loss:  0.24  epochs of training. Current testing loss:  0.21\n",
      "...completed  11.0  epochs of training. Current training loss:  0.23  epochs of training. Current testing loss:  0.21\n",
      "...completed  12.0  epochs of training. Current training loss:  0.22  epochs of training. Current testing loss:  0.2\n",
      "...completed  13.0  epochs of training. Current training loss:  0.22  epochs of training. Current testing loss:  0.2\n",
      "...completed  14.0  epochs of training. Current training loss:  0.21  epochs of training. Current testing loss:  0.19\n",
      "...completed  15.0  epochs of training. Current training loss:  0.21  epochs of training. Current testing loss:  0.19\n",
      "...completed  16.0  epochs of training. Current training loss:  0.21  epochs of training. Current testing loss:  0.19\n",
      "...completed  17.0  epochs of training. Current training loss:  0.2  epochs of training. Current testing loss:  0.18\n",
      "...completed  18.0  epochs of training. Current training loss:  0.2  epochs of training. Current testing loss:  0.18\n",
      "...completed  19.0  epochs of training. Current training loss:  0.2  epochs of training. Current testing loss:  0.18\n",
      "...completed  20.0  epochs of training. Current training loss:  0.19  epochs of training. Current testing loss:  0.18\n",
      "...completed  21.0  epochs of training. Current training loss:  0.19  epochs of training. Current testing loss:  0.17\n",
      "...completed  22.0  epochs of training. Current training loss:  0.19  epochs of training. Current testing loss:  0.17\n",
      "...completed  23.0  epochs of training. Current training loss:  0.19  epochs of training. Current testing loss:  0.17\n",
      "...completed  24.0  epochs of training. Current training loss:  0.19  epochs of training. Current testing loss:  0.17\n",
      "...completed  25.0  epochs of training. Current training loss:  0.18  epochs of training. Current testing loss:  0.17\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Normal learning\n",
    "netbackprop = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_normal, accuracy_bp_normal, test_loss_bp_normal, snr_bp_normal, cosine_similarity_bp_normal) = \\\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal learning\n",
    "netbackprop = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_normal, accuracy_bp_normal, test_loss_bp_normal, snr1_bp_normal, snr2_bp_normal, cosine_similarity_bp_normal) = \\\n",
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
    "    netbackprop.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9750,), (25,), (25,), (), (25, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_bp_normal.shape, accuracy_bp_normal.shape, test_loss_bp_normal.shape, snr_bp_normal.shape, cosine_similarity_bp_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 13,
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  50000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  0.9419 .\n"
     ]
    }
   ],
   "source": [
    "# Online learning\n",
    "from MLP import MLP, NodePerturbMLP, KolenPollackMLP\n",
    "\n",
    "net_bp_online = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
<<<<<<< HEAD
    "(losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr_bp_online, cosine_similarity_bp_online) = \\\n",
=======
    "(losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr1_bp_online, snr2_bp_online, cosine_similarity_bp_online) = \\\n",
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
    "    net_bp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232,) (231,) (231,) 0.10301087401898926 (231, 2)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/net_bp_online/losses_bp_normal.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(losses_bp_online)\u001b[38;5;241m.\u001b[39mshape, \n\u001b[0;32m      3\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(accuracy_bp_online)\u001b[38;5;241m.\u001b[39mshape, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(cosine_similarity_bp_online)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/net_bp_online/losses_bp_normal.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses_bp_online\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/net_bp_online/accuracy_bp_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(accuracy_bp_online))\n\u001b[0;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/net_bp_online/test_loss_bp_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(test_loss_bp_online))\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/net_bp_online/losses_bp_normal.npy'"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.asarray(losses_bp_online).shape, \n",
    "    np.asarray(accuracy_bp_online).shape, \n",
    "    np.asarray(test_loss_bp_online).shape, \n",
    "    np.asarray(snr_bp_online), \n",
    "    np.asarray(cosine_similarity_bp_online).shape\n",
    ")\n",
    "np.save('results/net_bp_online/losses_bp_normal.npy', np.asarray(losses_bp_online))\n",
    "np.save('results/net_bp_online/accuracy_bp_normal.npy', np.asarray(accuracy_bp_online))\n",
    "np.save('results/net_bp_online/test_loss_bp_normal.npy', np.asarray(test_loss_bp_online))\n",
    "np.save('results/net_bp_online/snr_bp_normal.npy', np.asarray(snr_bp_online))\n",
    "np.save('results/net_bp_online/cosine_similarity_bp_normal.npy', np.asarray(cosine_similarity_bp_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.92 GiB for an array with shape (1000, 500, 785) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Noisy Input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# create a network and train it using backprop\u001b[39;00m\n\u001b[0;32m      3\u001b[0m netbackprop_noisy \u001b[38;5;241m=\u001b[39m MLP\u001b[38;5;241m.\u001b[39mMLP(rng, numhidden, sigma\u001b[38;5;241m=\u001b[39minitweight, activation\u001b[38;5;241m=\u001b[39mactivation)\n\u001b[0;32m      4\u001b[0m (losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr_bp_noisy, cosine_similarity_bp_noisy) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mnetbackprop_noisy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearnrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackprop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnoise_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgauss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrep_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\MLP.py:393\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self, rng, images, labels, num_epochs, test_images, test_labels, learning_rate, batch_size, algorithm, noise, noise_type, report, report_rate)\u001b[0m\n\u001b[0;32m    391\u001b[0m     targets \u001b[38;5;241m=\u001b[39m test_labels[:, [t]]\n\u001b[0;32m    392\u001b[0m     grad[t, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_grad(rng, inputs, targets, algorithm\u001b[38;5;241m=\u001b[39malgorithm, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[1;32m--> 393\u001b[0m snr \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_grad_snr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgauss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms&p\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    396\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malter_inputs(np\u001b[38;5;241m.\u001b[39mcopy(images), noise_type)\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\helpers.py:98\u001b[0m, in \u001b[0;36mcalculate_grad_snr\u001b[1;34m(grad, epsilon)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_grad_snr\u001b[39m(grad, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m):\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    Calculate the average SNR |mean|/std across all parameters in a gradient update\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mmean(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m epsilon))\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3645\u001b[0m, in \u001b[0;36mstd\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m   3642\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3643\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m std(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3646\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\core\\_methods.py:206\u001b[0m, in \u001b[0;36m_std\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    205\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 206\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m               \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    210\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\core\\_methods.py:173\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    168\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marrmean\u001b[49m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.92 GiB for an array with shape (1000, 500, 785) and data type float64"
     ]
    }
   ],
   "source": [
    "# Noisy Input\n",
    "# create a network and train it using backprop\n",
    "netbackprop_noisy = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
<<<<<<< HEAD
    "(losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr_bp_noisy, cosine_similarity_bp_noisy) = \\\n",
=======
    "(losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr1_bp_noisy, snr2_bp_noisy, cosine_similarity_bp_noisy) = \\\n",
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
    "    netbackprop_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_bp_noisy.shape, accuracy_bp_noisy.shape, test_loss_bp_noisy.shape, snr_bp_noisy, cosine_similarity_bp_noisy.shape\n",
    "np.save('results/netbackprop/losses_bp_normal.npy', losses_bp_noisy)\n",
    "np.save('results/netbackprop/accuracy_bp_normal.npy', accuracy_bp_noisy)\n",
    "np.save('results/netbackprop/test_loss_bp_normal.npy', test_loss_bp_noisy)\n",
    "np.save('results/netbackprop/snr_bp_normal.npy', snr_bp_noisy)\n",
    "np.save('results/netbackprop/cosine_similarity_bp_normal.npy', cosine_similarity_bp_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_bp_nonstat = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
<<<<<<< HEAD
    "(losses_bp_nonstat, accuracy_bp_nonstat, test_loss_bp_nonstat, snr_bp_nonstat, cosine_similarity_bp_nonstat) = \\\n",
=======
    "(losses_bp_nonstat, accuracy_bp_nonstat, test_loss_bp_nonstat, snr1_bp_nonstat, snr2_bp_nonstat, cosine_similarity_bp_nonstat) = \\\n",
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
    "    net_bp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cpu\n",
      "input:\n",
      "  path: datasets\n",
      "  batch_size: 128\n",
      "model:\n",
      "  peer_normalization: 0.03\n",
      "  momentum: 0.9\n",
      "  hidden_dim: 500\n",
      "  num_layers: 2\n",
      "training:\n",
      "  epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0\n",
      "  momentum: 0\n",
      "  downstream_learning_rate: 0.001\n",
      "  downstream_weight_decay: 0\n",
      "  val_idx: 0\n",
      "  final_test: true\n",
      "\n",
      "FF_model(\n",
      "  (model): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      "  (ff_loss): BCEWithLogitsLoss()\n",
      "  (linear_classifier): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=10, bias=False)\n",
      "  )\n",
      "  (classification_loss): CrossEntropyLoss()\n",
      ") \n",
      "\n",
      "...completed  0  epochs of training. \n",
      "\n",
      "            Current innate loss:  58.63556840358636, \n",
      "\n",
      "            Current training cross_entropy loss:  1.7526681716625507, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.2275530099868774, \n",
      "   \n",
      "            Current testing accuracy:  0.7907 \n",
      "         \n",
      "            \n",
      "...completed  1  epochs of training. \n",
      "\n",
      "            Current innate loss:  44.71706792146732, \n",
      "\n",
      "            Current training cross_entropy loss:  1.343608803015489, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.7227679491043091, \n",
      "   \n",
      "            Current testing accuracy:  0.8537 \n",
      "         \n",
      "            \n",
      "...completed  2  epochs of training. \n",
      "\n",
      "            Current innate loss:  35.94339129822886, \n",
      "\n",
      "            Current training cross_entropy loss:  1.1089714631820335, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.5488615036010742, \n",
      "   \n",
      "            Current testing accuracy:  0.8784 \n",
      "         \n",
      "            \n",
      "...completed  3  epochs of training. \n",
      "\n",
      "            Current innate loss:  29.887251834991652, \n",
      "\n",
      "            Current training cross_entropy loss:  0.9642073383507056, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.4746421277523041, \n",
      "   \n",
      "            Current testing accuracy:  0.8913 \n",
      "         \n",
      "            \n",
      "...completed  4  epochs of training. \n",
      "\n",
      "            Current innate loss:  25.790816731697475, \n",
      "\n",
      "            Current training cross_entropy loss:  0.8651232407643245, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.42174336314201355, \n",
      "   \n",
      "            Current testing accuracy:  0.8984 \n",
      "         \n",
      "            \n",
      "...completed  5  epochs of training. \n",
      "\n",
      "            Current innate loss:  22.71232744241372, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7925254569603847, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.39569881558418274, \n",
      "   \n",
      "            Current testing accuracy:  0.9032 \n",
      "         \n",
      "            \n",
      "...completed  6  epochs of training. \n",
      "\n",
      "            Current innate loss:  20.47049979718177, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7362035607134466, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3689563572406769, \n",
      "   \n",
      "            Current testing accuracy:  0.9095 \n",
      "         \n",
      "            \n",
      "...completed  7  epochs of training. \n",
      "\n",
      "            Current innate loss:  18.512340567089044, \n",
      "\n",
      "            Current training cross_entropy loss:  0.691766263009646, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3579031825065613, \n",
      "   \n",
      "            Current testing accuracy:  0.9123 \n",
      "         \n",
      "            \n",
      "...completed  8  epochs of training. \n",
      "\n",
      "            Current innate loss:  17.03147812463619, \n",
      "\n",
      "            Current training cross_entropy loss:  0.6551408405614715, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3399561643600464, \n",
      "   \n",
      "            Current testing accuracy:  0.916 \n",
      "         \n",
      "            \n",
      "...completed  9  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.832343904696978, \n",
      "\n",
      "            Current training cross_entropy loss:  0.624267397549672, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3215690851211548, \n",
      "   \n",
      "            Current testing accuracy:  0.9195 \n",
      "         \n",
      "            \n",
      "...completed  10  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.789481434232982, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5976818415564257, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3121406137943268, \n",
      "   \n",
      "            Current testing accuracy:  0.9202 \n",
      "         \n",
      "            \n",
      "...completed  11  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.796160554376423, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5750501604225391, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3080938458442688, \n",
      "   \n",
      "            Current testing accuracy:  0.9227 \n",
      "         \n",
      "            \n",
      "...completed  12  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.044622030542682, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5548375979124677, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2955418527126312, \n",
      "   \n",
      "            Current testing accuracy:  0.9235 \n",
      "         \n",
      "            \n",
      "...completed  13  epochs of training. \n",
      "\n",
      "            Current innate loss:  12.338397360099105, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5370463697765118, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.28331825137138367, \n",
      "   \n",
      "            Current testing accuracy:  0.9245 \n",
      "         \n",
      "            \n",
      "...completed  14  epochs of training. \n",
      "\n",
      "            Current innate loss:  11.736596516890403, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5209330544359664, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2844664454460144, \n",
      "   \n",
      "            Current testing accuracy:  0.9248 \n",
      "         \n",
      "            \n",
      "...completed  15  epochs of training. \n",
      "\n",
      "            Current innate loss:  11.225830697917786, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5064669040031731, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2718605101108551, \n",
      "   \n",
      "            Current testing accuracy:  0.9267 \n",
      "         \n",
      "            \n",
      "...completed  16  epochs of training. \n",
      "\n",
      "            Current innate loss:  10.737181999438848, \n",
      "\n",
      "            Current training cross_entropy loss:  0.49326414424100073, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2706184983253479, \n",
      "   \n",
      "            Current testing accuracy:  0.9271 \n",
      "         \n",
      "            \n",
      "...completed  17  epochs of training. \n",
      "\n",
      "            Current innate loss:  10.296551365169705, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4812522433719404, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2650071680545807, \n",
      "   \n",
      "            Current testing accuracy:  0.9284 \n",
      "         \n",
      "            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     GlobalHydra()\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     10\u001b[0m netffa \u001b[38;5;241m=\u001b[39m net_ff_model\u001b[38;5;241m.\u001b[39mnet_FF_model(rng)\n\u001b[1;32m---> 11\u001b[0m (losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal) \u001b[38;5;241m=\u001b[39m \u001b[43mnetffa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_over\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mff_com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcross_entropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\net_ff_model.py:118\u001b[0m, in \u001b[0;36mnet_FF_model.train_over\u001b[1;34m(self, train_images, train_targets, test_images, test_targets, batch_size, epochs, model, return_loss)\u001b[0m\n\u001b[0;32m    112\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_conversion(inputs, labels, model)\n\u001b[0;32m    114\u001b[0m     running_real_losses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch(inputs, labels, model)\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m     running_return_losses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 118\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_inference_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    120\u001b[0m real_losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(running_real_losses))\n\u001b[0;32m    121\u001b[0m return_losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(running_return_losses))\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\net_ff_model.py:384\u001b[0m, in \u001b[0;36mnet_FF_model._get_inference_loss\u001b[1;34m(self, inputs, labels, model, return_loss)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_inference_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, labels, \n\u001b[0;32m    382\u001b[0m                         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_sim\u001b[39m\u001b[38;5;124m'\u001b[39m, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_entropy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_com\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 384\u001b[0m         preds_ori \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_ori\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m return_loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    386\u001b[0m             preds_ori \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(preds_ori, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\net_ff_model.py:362\u001b[0m, in \u001b[0;36mnet_FF_model.inference_ori\u001b[1;34m(self, inputs, labels, model)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference_ori\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, labels, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_sim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_com\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m         scalar_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_downstream_classification_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m scalar_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_sim\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\forward_forward_complex\\src\\ff_model.py:128\u001b[0m, in \u001b[0;36mFF_model.forward_downstream_classification_model\u001b[1;34m(self, inputs, labels, scalar_outputs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_downstream_classification_model\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs, labels, scalar_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scalar_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m         scalar_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    129\u001b[0m         }\n\u001b[0;32m    131\u001b[0m     z \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    132\u001b[0m     z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mreshape(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normal learning\n",
    "# create a network and train it using ffa\n",
    "# netffa = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal, snr1_ffa_normal, snr2_ffa_normal, cosine_similarity_ffa_normal) = \\\n",
    "#     netffa.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=rep_rate)\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "netffa = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal) = netffa.train_over(\n",
    "    train_images, train_labels, \n",
    "    test_images, test_labels,\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlosses_ffa_normal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, accuracy_ffa_normal\u001b[38;5;241m.\u001b[39mshape, test_loss_ffa_normal\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/netffa/losses_ffa_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(losses_ffa_normal))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "losses_ffa_normal.shape, accuracy_ffa_normal.shape, test_loss_ffa_normal.shape\n",
    "np.save('results/netffa/losses_ffa_normal.npy', np.asarray(losses_ffa_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "netffa_online = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_online, accuracy_ffa_online, test_loss_ffa_online) = netffa_online.train_online(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    max_it=25000, conv_loss=0.1, \n",
    "    report_rate=100, lr=0.01,\n",
    "    model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_ffa_online.shape, accuracy_ffa_online.shape, test_loss_ffa_online.shape"
=======
    "# Online Learning\n",
    "# net_ffa_online = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_online, accuracy_ffa_online, test_loss_ffa_online, snr1_ffa_online, snr2_ffa_online, cosine_similarity_ffa_online) = \\\n",
    "#     net_ffa_online.train_online(rng_bp2, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-4, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=batchsize)"
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network and train it using ffa\n",
    "# net_ffa_noisy = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy, snr1_ffa_noisy, snr2_ffa_noisy, cosine_similarity_ffa_noisy) = \\\n",
    "#     net_ffa_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       noise_type='gauss',report=report, report_rate=rep_rate)\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "net_ffa_noisy = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy) = net_ffa_noisy.train_nonstationary(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# net_ffa_nonstat = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat, snr1_ffa_nonstat, snr2_ffa_nonstat, cosine_similarity_ffa_nonstat) = \\\n",
    "#     net_ffa_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=1)\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "net_ffa_nonstat = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat) = net_ffa_nonstat.train_noisydata(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/test/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_test = data[0]\n",
    "accuracy_node_perturb_test = data[1]\n",
    "test_losses_node_perturb_test = data[2]\n",
    "snr1_node_perturb_test = data[3]\n",
    "snr2_node_perturb_test = data[4]\n",
    "cosine_similarity_node_perturb_test = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "# create a network and train it using node perturbation\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     netnodeperturb = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr_np_normal, cosine_similarity_np_normal) = \\\n",
    "#         netnodeperturb.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                              learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, report=report, report_rate=rep_rate)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/normal/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_np_normal = data[0]\n",
    "accuracy_np_normal = data[1]\n",
    "test_losses_np_normal = data[2]\n",
    "snr1_np_normal = data[3]\n",
    "snr2_np_normal = data[4]\n",
    "cosine_similarity_np_normal = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     netnodeperturb_online = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_online, accuracy_np_online, test_loss_np_online, snr_np_online, cosine_similarity_np_online) = \\\n",
    "#         netnodeperturb_online.train_online(rng_bp2, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "#         netnodeperturb_online.train_online(rng_bp2, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-4, algorithm='node_perturb', noise=noise, \\\n",
    "#                       report=report, report_rate=batchsize)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/online/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_np_online = data[0]\n",
    "accuracy_np_online = data[1]\n",
    "test_losses_np_online = data[2]\n",
    "snr1_np_online = data[3]\n",
    "snr2_np_online = data[4]\n",
    "cosine_similarity_np_online = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     nodeperturb_noisy = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr_np_noisy, cosine_similarity_np_noisy) = \\\n",
    "#         nodeperturb_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#         nodeperturb_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                         learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, \\\n",
    "#                         noise_type='gauss',report=report, report_rate=rep_rate)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/noisy/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_np_noisy = data[0]\n",
    "accuracy_np_noisy = data[1]\n",
    "test_losses_np_noisy = data[2]\n",
    "snr1_np_noisy = data[3]\n",
    "snr2_np_noisy = data[4]\n",
    "cosine_similarity_np_noisy = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     nodeperturb_nonstat = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr_np_nonstat, cosine_similarity_np_nonstat) = \\\n",
    "#         nodeperturb_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#         nodeperturb_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                         learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, \\\n",
    "#                         report=report, report_rate=1)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/non-stat/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_np_nonstat = data[0]\n",
    "accuracy_np_nonstat = data[1]\n",
    "test_losses_np_nonstat = data[2]\n",
    "snr1_np_nonstat = data[3]\n",
    "snr2_np_nonstat = data[4]\n",
    "cosine_similarity_np_nonstat = data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolen-Pollack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "net_kp_normal = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_normal, accuracy_kp_normal, test_loss_kp_normal, snr1_kp_normal, snr2_kp_normal, cosine_similarity_kp_normal) = \\\n",
    "    net_kp_normal.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "net_kp_online = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_online, accuracy_kp_online, test_loss_kp_online, snr1_kp_online, snr2_kp_online, cosine_similarity_kp_online) = \\\n",
    "    net_kp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "net_kp_noisy = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_noisy, accuracy_kp_noisy, test_loss_kp_noisy, snr1_kp_noisy, snr2_kp_noisy, cosine_similarity_kp_noisy) = \\\n",
    "    net_kp_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_kp_nonstat = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_nonstat, accuracy_kp_nonstat, test_loss_kp_nonstat, snr1_kp_nonstat, snr2_kp_nonstat, cosine_similarity_kp_nonstat) = \\\n",
    "    net_kp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run without node perturb and ffa\n",
    "# if run without node perturb and ffa\n",
    "# FFA\n",
<<<<<<< HEAD
    "snr_ffa_normal, cosine_similarity_ffa_normal = [0], [0]\n",
    "snr_ffa_online, cosine_similarity_ffa_online = [0], [0]\n",
    "snr_ffa_noisy, cosine_similarity_ffa_noisy = [0], [0]\n",
    "snr_ffa_nonstat, cosine_similarity_ffa_nonstat = [0], [0]\n",
=======
    "losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal, snr1_ffa_normal, snr2_ffa_normal, cosine_similarity_ffa_normal = [0], [0], [0], [0], [0]\n",
    "losses_ffa_online, accuracy_ffa_online, test_loss_ffa_online, snr1_ffa_online, snr2_ffa_online, cosine_similarity_ffa_online = [0], [0], [0], [0], [0]\n",
    "losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy, snr1_ffa_noisy, snr2_ffa_noisy, cosine_similarity_ffa_noisy = [0], [0], [0], [0], [0]\n",
    "losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat, snr1_ffa_nonstat, snr2_ffa_nonstat, cosine_similarity_ffa_nonstat = [0], [0], [0], [0], [0]\n",
>>>>>>> f2b738de8b95c159ffd0dbd55dd39aa33adf18eb
    "\n",
    "# Node Perturb\n",
    "losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr1_np_normal, snr2_np_normal, cosine_similarity_np_normal = [0], [0], [0], [0], [0]\n",
    "losses_np_online, accuracy_np_online, test_loss_np_online, snr1_np_online, snr2_np_online, cosine_similarity_np_online = [0], [0], [0], [0], [0]\n",
    "losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr1_np_noisy, snr2_np_noisy, cosine_similarity_np_noisy = [0], [0], [0], [0], [0]\n",
    "losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr1_np_nonstat, snr2_np_nonstat, cosine_similarity_np_nonstat = [0], [0], [0], [0], [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays\n",
    "# normal \n",
    "tr_loss_normal = [losses_bp_normal, losses_ffa_normal, losses_np_normal, losses_kp_normal]\n",
    "te_acc_normal = [accuracy_bp_normal, accuracy_ffa_normal, accuracy_np_normal, accuracy_kp_normal]\n",
    "te_loss_normal = [test_loss_bp_normal, test_loss_ffa_normal, test_loss_np_normal, test_loss_kp_normal]\n",
    "snr1_normal = [snr1_bp_normal, snr1_ffa_normal, snr1_np_normal, snr1_kp_normal]\n",
    "snr2_normal = [snr2_bp_normal, snr2_ffa_normal, snr2_np_normal, snr2_kp_normal]\n",
    "cosine_similarity_normal = [cosine_similarity_bp_normal, cosine_similarity_ffa_normal, cosine_similarity_np_normal, cosine_similarity_kp_normal]\n",
    "\n",
    "# online\n",
    "# Calculate the moving average\n",
    "window_size = 100\n",
    "losses_bp_online_mean = uniform_filter1d(losses_bp_online, size=window_size)\n",
    "losses_ffa_online_mean = uniform_filter1d(losses_ffa_online, size=window_size)\n",
    "losses_np_online_mean = uniform_filter1d(losses_np_online, size=window_size)\n",
    "losses_kp_online_mean = uniform_filter1d(losses_kp_online, size=window_size)\n",
    "tr_loss_online_mean = [losses_bp_online_mean, losses_ffa_online_mean, losses_np_online_mean, losses_kp_online_mean]\n",
    "\n",
    "tr_loss_online = [losses_bp_online, losses_ffa_online, losses_np_online, losses_kp_online]\n",
    "te_acc_online = [accuracy_bp_online, accuracy_ffa_online, accuracy_np_online, accuracy_kp_online]\n",
    "te_loss_online = [test_loss_bp_online, test_loss_ffa_online, test_loss_np_online, test_loss_kp_online]\n",
    "snr1_online = [snr1_bp_online, snr1_ffa_online, snr1_np_online, snr1_kp_online]\n",
    "snr2_online = [snr2_bp_online, snr2_ffa_online, snr2_np_online, snr2_kp_online]\n",
    "cosine_similarity_online = [cosine_similarity_bp_online, cosine_similarity_ffa_online, cosine_similarity_np_online, cosine_similarity_kp_online]\n",
    "\n",
    "# noisy\n",
    "tr_loss_noisy = [losses_bp_noisy, losses_ffa_noisy, losses_np_noisy, losses_kp_noisy]\n",
    "te_acc_noisy = [accuracy_bp_noisy, accuracy_ffa_noisy, accuracy_np_noisy, accuracy_kp_noisy]\n",
    "te_loss_noisy = [test_loss_bp_noisy, test_loss_ffa_noisy, test_loss_np_noisy, test_loss_kp_noisy]\n",
    "snr1_noisy = [snr1_bp_noisy, snr1_ffa_noisy, snr1_np_noisy, snr1_kp_noisy]\n",
    "snr2_noisy = [snr2_bp_noisy, snr2_ffa_noisy, snr2_np_noisy, snr2_kp_noisy]\n",
    "cosine_similarity_noisy = [cosine_similarity_bp_noisy, cosine_similarity_ffa_noisy, cosine_similarity_np_noisy, cosine_similarity_kp_noisy]\n",
    "\n",
    "# nonstat\n",
    "tr_loss_nonstat = [losses_bp_nonstat, losses_ffa_nonstat, losses_np_nonstat, losses_kp_nonstat]\n",
    "te_acc_nonstat = [accuracy_bp_nonstat, accuracy_ffa_nonstat, accuracy_np_nonstat, accuracy_kp_nonstat]\n",
    "te_loss_nonstat = [test_loss_bp_nonstat, test_loss_ffa_nonstat, test_loss_np_nonstat, test_loss_kp_nonstat]\n",
    "snr1_nonstat = [snr1_bp_nonstat, snr1_ffa_nonstat, snr1_np_nonstat, snr1_kp_nonstat]\n",
    "snr2_nonstat = [snr2_bp_nonstat, snr2_ffa_nonstat, snr2_np_nonstat, snr2_kp_nonstat]\n",
    "cosine_similarity_nonstat = [cosine_similarity_bp_nonstat, cosine_similarity_ffa_nonstat, cosine_similarity_np_nonstat, cosine_similarity_kp_nonstat]\n",
    "\n",
    "# algorithms\n",
    "# algos = ['normal', 'online', 'noisy', 'nonstat']\n",
    "algos = ['normal', 'noisy', 'nonstat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "for i, algo in enumerate(algos):\n",
    "    if algo == 'normal':\n",
    "        tr_loss, te_acc, te_loss, snr1, snr2, cos_sim = tr_loss_normal, te_acc_normal, te_loss_normal, snr1_normal, snr2_normal, cosine_similarity_normal\n",
    "    # elif algo == 'online':\n",
    "    #     tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_online, te_acc_online, te_loss_online, snr_online, cosine_similarity_online\n",
    "    elif algo == 'noisy':\n",
    "        tr_loss, te_acc, te_loss, snr1, snr2, cos_sim = tr_loss_noisy, te_acc_noisy, te_loss_noisy, snr1_noisy, snr2_noisy, cosine_similarity_noisy\n",
    "    elif algo == 'nonstat':\n",
    "        tr_loss, te_acc, te_loss, snr1, snr2, cos_sim = tr_loss_nonstat, te_acc_nonstat, te_loss_nonstat, snr1_nonstat, snr2_nonstat, cosine_similarity_nonstat\n",
    "        \n",
    "    # plot\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(151)\n",
    "\n",
    "    plt.plot(tr_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(tr_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(tr_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(tr_loss[3], label=\"Kollen-Pollack\", color='k')\n",
    "    \n",
    "    plt.xlabel(\"Updates (every batch)\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training loss\")\n",
    "\n",
    "    plt.subplot(152)\n",
    "    plt.plot(te_acc[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_acc[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_acc[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_acc[3], label=\"Kollen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Performance over time\")\n",
    "\n",
    "    plt.subplot(153)\n",
    "    plt.plot(te_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_loss[3], label=\"Kollen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Test loss\")\n",
    "\n",
    "    plt.subplot(154)\n",
    "    with plt.xkcd():\n",
    "            x = [0, 1, 2, 3]\n",
    "            snr1_vals = [snr1[1], snr1[2], snr1[3], snr1[0]]\n",
    "            snr2_vals = [snr2[1], snr2[2], snr2[3], snr2[0]]\n",
    "            colors = ['gold', 'c', 'k', 'r']\n",
    "            labels = ['FFA', 'Node Perturbation', 'Kollen Pollack', 'Backprop']\n",
    "            plt.bar(x, snr1_vals, color=colors, tick_label=labels)\n",
    "            plt.bar(x, snr2_vals, color=colors, tick_label=labels, alpha=0.5)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.ylabel('SNR')\n",
    "            plt.xlabel('Algorithm')\n",
    "            plt.title('Gradient SNR')\n",
    "\n",
    "    plt.subplot(155)\n",
    "    with plt.xkcd():\n",
    "        plt.plot(cos_sim[0], label=\"Backprop\", color='r')\n",
    "        plt.plot(cos_sim[1], label=\"FFA\", color='gold')\n",
    "        plt.plot(cos_sim[2], label=\"Node Perturbation\", color='c')\n",
    "        plt.plot(cos_sim[3], label=\"Kollen-Pollack\", color='k')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cosine Sim\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Cosine Similarity to Backprop\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning separately becuase of different lenghts and additional smoothing operation\n",
    "# plot\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(151)\n",
    "\n",
    "plt.plot(tr_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(tr_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(tr_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(tr_loss_online[3], label=\"Kollen-Pollack\", color='k')\n",
    "\n",
    "plt.xlabel(\"Updates (every batch)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Training loss\")\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(te_acc_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_acc_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_acc_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_acc_online[3], label=\"Kollen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(te_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_loss_online[3], label=\"Kollen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Test loss\")\n",
    "\n",
    "plt.subplot(154)\n",
    "with plt.xkcd():\n",
    "        x = [0, 1, 2, 3]\n",
    "        snr1_vals = [snr1_online[1], snr1_online[2], snr1_online[3], snr1_online[0]]\n",
    "        snr1_vals = [snr2_online[1], snr2_online[2], snr2_online[3], snr2_online[0]]\n",
    "        colors = ['gold', 'c', 'k', 'r']\n",
    "        labels = ['FFA', 'Node Perturbation', 'Kollen Pollack', 'Backprop']\n",
    "        plt.bar(x, snr1_vals, color=colors, tick_label=labels)\n",
    "        plt.bar(x, snr2_vals, color=colors, tick_label=labels, alpha=0.5)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('SNR')\n",
    "        plt.xlabel('Algorithm')\n",
    "        plt.title('Gradient SNR')\n",
    "\n",
    "plt.subplot(155)\n",
    "with plt.xkcd():\n",
    "    plt.plot(cosine_similarity_online[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(cosine_similarity_online[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(cosine_similarity_online[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(cosine_similarity_online[3], label=\"Kollen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cosine Sim\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cosine Similarity to Backprop\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
