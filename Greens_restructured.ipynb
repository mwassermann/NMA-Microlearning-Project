{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Much code taken from Neuromatch NeuroAI 2024 Microlearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'MLP' from 'd:\\\\MyFolders\\\\project\\\\2024summer\\\\NeuroAI\\\\NMA-Microlearning-Project\\\\MLP.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependencies\n",
    "from IPython.display import Image, SVG, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "import io\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "## Plotting and metrics imports\n",
    "from metrics import get_plotting_color, plot_examples, plot_class_distribution, plot_results, plot_scores_per_class, plot_weights\n",
    "\n",
    "## Other functions imports\n",
    "from helpers import sigmoid, ReLU, add_bias, create_batches, calculate_accuracy, calculate_cosine_similarity, calculate_grad_snr\n",
    "\n",
    "## MLP imports\n",
    "from MLP import MLP, NodePerturbMLP, KolenPollackMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST function\n",
    "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
    "\n",
    "  valid_prop = 1 - train_prop\n",
    "\n",
    "  discard_prop = 1 - keep_prop\n",
    "\n",
    "  transform = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize((0.1307,), (0.3081,))]\n",
    "      )\n",
    "\n",
    "\n",
    "  with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    \n",
    "    rng_data = np.random.default_rng(seed=42)\n",
    "    train_num = 50000\n",
    "    shuffled_train_idx = rng_data.permutation(train_num)\n",
    "\n",
    "    full_train_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=True, download=True, transform=transform)\n",
    "    full_test_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=False, download=True, transform=transform)\n",
    "    \n",
    "    full_train_images = full_train_set.data.numpy().astype(float) / 255\n",
    "    train_images = full_train_images[shuffled_train_idx[:train_num]].reshape((-1, 784)).T.copy()\n",
    "    valid_images = full_train_images[shuffled_train_idx[train_num:]].reshape((-1, 784)).T.copy()\n",
    "    test_images = (full_test_set.data.numpy().astype(float) / 255).reshape((-1, 784)).T\n",
    "\n",
    "    full_train_labels = torch.nn.functional.one_hot(full_train_set.targets, num_classes=10).numpy()\n",
    "    train_labels = full_train_labels[shuffled_train_idx[:train_num]].T.copy()\n",
    "    valid_labels = full_train_labels[shuffled_train_idx[train_num:]].T.copy()\n",
    "    test_labels = torch.nn.functional.one_hot(full_test_set.targets, num_classes=10).numpy().T\n",
    "\n",
    "    train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "      full_train_set, [train_prop * keep_prop, valid_prop * keep_prop, discard_prop])\n",
    "    test_set, _ = torch.utils.data.random_split(\n",
    "      full_test_set, [keep_prop, discard_prop])\n",
    "\n",
    "  print(\"Number of examples retained:\")\n",
    "  print(f\"  {len(train_set)} (training)\")\n",
    "  print(f\"  {len(valid_set)} (validation)\")\n",
    "  print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "  return train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples retained:\n",
      "  24001 (training)\n",
      "  5999 (validation)\n",
      "  5000 (test)\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels = download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "NUM_INPUTS = 784\n",
    "NUM_OUTPUTS = 10\n",
    "numhidden = 500\n",
    "batchsize = 128\n",
    "initweight = 0.1\n",
    "learnrate = 0.001\n",
    "noise = 0.1\n",
    "numepochs = 25\n",
    "numrepeats = 1\n",
    "numbatches = int(train_images.shape[1] / batchsize)\n",
    "numupdates = numepochs * numbatches\n",
    "activation = 'sigmoid'\n",
    "report = True\n",
    "rep_rate = 1\n",
    "seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "indices = np.random.choice(test_images.shape[1], size=1000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current loss:  59.83\n",
      "...completed  2.0  epochs of training. Current loss:  57.78\n",
      "...completed  3.0  epochs of training. Current loss:  56.73\n",
      "...completed  4.0  epochs of training. Current loss:  56.09\n",
      "...completed  5.0  epochs of training. Current loss:  55.66\n",
      "...completed  6.0  epochs of training. Current loss:  55.35\n",
      "...completed  7.0  epochs of training. Current loss:  55.12\n",
      "...completed  8.0  epochs of training. Current loss:  54.93\n",
      "...completed  9.0  epochs of training. Current loss:  54.78\n",
      "...completed  10.0  epochs of training. Current loss:  54.66\n",
      "...completed  11.0  epochs of training. Current loss:  54.55\n",
      "...completed  12.0  epochs of training. Current loss:  54.46\n",
      "...completed  13.0  epochs of training. Current loss:  54.38\n",
      "...completed  14.0  epochs of training. Current loss:  54.31\n",
      "...completed  15.0  epochs of training. Current loss:  54.25\n",
      "...completed  16.0  epochs of training. Current loss:  54.19\n",
      "...completed  17.0  epochs of training. Current loss:  54.14\n",
      "...completed  18.0  epochs of training. Current loss:  54.09\n",
      "...completed  19.0  epochs of training. Current loss:  54.04\n",
      "...completed  20.0  epochs of training. Current loss:  54.0\n",
      "...completed  21.0  epochs of training. Current loss:  53.96\n",
      "...completed  22.0  epochs of training. Current loss:  53.92\n",
      "...completed  23.0  epochs of training. Current loss:  53.88\n",
      "...completed  24.0  epochs of training. Current loss:  53.85\n",
      "...completed  25.0  epochs of training. Current loss:  53.82\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Normal learning\n",
    "netbackprop = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_normal, accuracy_bp_normal, test_loss_bp_normal, snr1_bp_normal, snr2_bp_normal, cosine_similarity_bp_normal) = \\\n",
    "    netbackprop.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_bp_normal, label=\"Backprop\", color='r')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_loss_bp_normal, label=\"Backprop\", color='r')\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Test loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_bp_normal, label=\"Backprop\", color='r')\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  50000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  0.9419 .\n"
     ]
    }
   ],
   "source": [
    "# Online learning\n",
    "from MLP import MLP, NodePerturbMLP, KolenPollackMLP\n",
    "\n",
    "net_bp_online = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr1_bp_online, snr2_bp_online, cosine_similarity_bp_online) = \\\n",
    "(losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr1_bp_online, snr2_bp_online, cosine_similarity_bp_online) = \\\n",
    "    net_bp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current loss:  60.55\n",
      "...completed  2.0  epochs of training. Current loss:  58.61\n",
      "...completed  3.0  epochs of training. Current loss:  57.42\n",
      "...completed  4.0  epochs of training. Current loss:  56.71\n",
      "...completed  5.0  epochs of training. Current loss:  56.23\n",
      "...completed  6.0  epochs of training. Current loss:  55.88\n",
      "...completed  7.0  epochs of training. Current loss:  55.62\n",
      "...completed  8.0  epochs of training. Current loss:  55.41\n",
      "...completed  9.0  epochs of training. Current loss:  55.24\n",
      "...completed  10.0  epochs of training. Current loss:  55.09\n",
      "...completed  11.0  epochs of training. Current loss:  54.97\n",
      "...completed  12.0  epochs of training. Current loss:  54.86\n",
      "...completed  13.0  epochs of training. Current loss:  54.77\n",
      "...completed  14.0  epochs of training. Current loss:  54.68\n",
      "...completed  15.0  epochs of training. Current loss:  54.61\n",
      "...completed  16.0  epochs of training. Current loss:  54.54\n",
      "...completed  17.0  epochs of training. Current loss:  54.48\n",
      "...completed  18.0  epochs of training. Current loss:  54.42\n",
      "...completed  19.0  epochs of training. Current loss:  54.37\n",
      "...completed  20.0  epochs of training. Current loss:  54.32\n",
      "...completed  21.0  epochs of training. Current loss:  54.28\n",
      "...completed  22.0  epochs of training. Current loss:  54.24\n",
      "...completed  23.0  epochs of training. Current loss:  54.2\n",
      "...completed  24.0  epochs of training. Current loss:  54.16\n",
      "...completed  25.0  epochs of training. Current loss:  54.13\n",
      "Training complete.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/net_bp_online/losses_bp_normal.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(losses_bp_online)\u001b[38;5;241m.\u001b[39mshape, \n\u001b[0;32m      3\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(accuracy_bp_online)\u001b[38;5;241m.\u001b[39mshape, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(cosine_similarity_bp_online)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/net_bp_online/losses_bp_normal.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses_bp_online\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/net_bp_online/accuracy_bp_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(accuracy_bp_online))\n\u001b[0;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/net_bp_online/test_loss_bp_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(test_loss_bp_online))\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/net_bp_online/losses_bp_normal.npy'"
     ]
    }
   ],
   "source": [
    "# Noisy Input\n",
    "# create a network and train it using backprop\n",
    "netbackprop_noisy = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr1_bp_noisy, snr2_bp_noisy, cosine_similarity_bp_noisy) = \\\n",
    "    netbackprop_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_bp_nonstat = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_nonstat, accuracy_bp_nonstat, test_loss_bp_nonstat, snr1_bp_nonstat, snr2_bp_nonstat, cosine_similarity_bp_nonstat) = \\\n",
    "    net_bp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cpu\n",
      "input:\n",
      "  path: datasets\n",
      "  batch_size: 128\n",
      "model:\n",
      "  peer_normalization: 0.03\n",
      "  momentum: 0.9\n",
      "  hidden_dim: 500\n",
      "  num_layers: 2\n",
      "training:\n",
      "  epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0\n",
      "  momentum: 0\n",
      "  downstream_learning_rate: 0.001\n",
      "  downstream_weight_decay: 0\n",
      "  val_idx: 0\n",
      "  final_test: true\n",
      "\n",
      "FF_model(\n",
      "  (model): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      "  (ff_loss): BCEWithLogitsLoss()\n",
      "  (linear_classifier): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=10, bias=False)\n",
      "  )\n",
      "  (classification_loss): CrossEntropyLoss()\n",
      ") \n",
      "\n",
      "...completed  1  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.06137740612030029, \n",
      "\n",
      "                Current testing accuracy:  0.113 \n",
      "\n",
      "                Current testing loss:  4.456904888153076 \n",
      " \n",
      "...completed  129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.49282130858046, \n",
      "\n",
      "                Current testing accuracy:  0.104 \n",
      "\n",
      "                Current testing loss:  4.498246192932129 \n",
      " \n",
      "...completed  257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.69406203678227, \n",
      "\n",
      "                Current testing accuracy:  0.104 \n",
      "\n",
      "                Current testing loss:  4.751631736755371 \n",
      " \n",
      "...completed  385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.505632612796035, \n",
      "\n",
      "                Current testing accuracy:  0.104 \n",
      "\n",
      "                Current testing loss:  4.307456970214844 \n",
      " \n",
      "...completed  513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.506413144204998, \n",
      "\n",
      "                Current testing accuracy:  0.113 \n",
      "\n",
      "                Current testing loss:  4.839576721191406 \n",
      " \n",
      "...completed  641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.32862094862503, \n",
      "\n",
      "                Current testing accuracy:  0.104 \n",
      "\n",
      "                Current testing loss:  4.55812406539917 \n",
      " \n",
      "...completed  769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.243188118067337, \n",
      "\n",
      "                Current testing accuracy:  0.105 \n",
      "\n",
      "                Current testing loss:  4.130993366241455 \n",
      " \n",
      "...completed  897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.4887741177517455, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  3.9181880950927734 \n",
      " \n",
      "...completed  1025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.663624407432508, \n",
      "\n",
      "                Current testing accuracy:  0.109 \n",
      "\n",
      "                Current testing loss:  4.285031795501709 \n",
      " \n",
      "...completed  1153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.216533324637567, \n",
      "\n",
      "                Current testing accuracy:  0.105 \n",
      "\n",
      "                Current testing loss:  4.558902263641357 \n",
      " \n",
      "...completed  1281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.493098247330636, \n",
      "\n",
      "                Current testing accuracy:  0.097 \n",
      "\n",
      "                Current testing loss:  4.37053918838501 \n",
      " \n",
      "...completed  1409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.53915730243898, \n",
      "\n",
      "                Current testing accuracy:  0.113 \n",
      "\n",
      "                Current testing loss:  4.596261024475098 \n",
      " \n",
      "...completed  1537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.327555978496093, \n",
      "\n",
      "                Current testing accuracy:  0.105 \n",
      "\n",
      "                Current testing loss:  4.0014214515686035 \n",
      " \n",
      "...completed  1665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.326874578517163, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  4.591428756713867 \n",
      " \n",
      "...completed  1793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.42219578003278, \n",
      "\n",
      "                Current testing accuracy:  0.105 \n",
      "\n",
      "                Current testing loss:  4.549188137054443 \n",
      " \n",
      "...completed  1921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.240667915990343, \n",
      "\n",
      "                Current testing accuracy:  0.097 \n",
      "\n",
      "                Current testing loss:  4.68412446975708 \n",
      " \n",
      "...completed  2049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.3584181062324205, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  4.446433067321777 \n",
      " \n",
      "...completed  2177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.245986080757575, \n",
      "\n",
      "                Current testing accuracy:  0.091 \n",
      "\n",
      "                Current testing loss:  3.581174850463867 \n",
      " \n",
      "...completed  2305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.45475030617672, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  4.6154561042785645 \n",
      " \n",
      "...completed  2433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.204289705507108, \n",
      "\n",
      "                Current testing accuracy:  0.096 \n",
      "\n",
      "                Current testing loss:  4.433156967163086 \n",
      " \n",
      "...completed  2561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.172287840468925, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  4.469615936279297 \n",
      " \n",
      "...completed  2689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.073316727794008, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  4.240513324737549 \n",
      " \n",
      "...completed  2817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.050901824288303, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  3.7376372814178467 \n",
      " \n",
      "...completed  2945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.396425068931421, \n",
      "\n",
      "                Current testing accuracy:  0.096 \n",
      "\n",
      "                Current testing loss:  4.7031569480896 \n",
      " \n",
      "...completed  3073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.14442595816945, \n",
      "\n",
      "                Current testing accuracy:  0.097 \n",
      "\n",
      "                Current testing loss:  4.3357672691345215 \n",
      " \n",
      "...completed  3201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.351558740410837, \n",
      "\n",
      "                Current testing accuracy:  0.096 \n",
      "\n",
      "                Current testing loss:  4.267838954925537 \n",
      " \n",
      "...completed  3329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.295560218641185, \n",
      "\n",
      "                Current testing accuracy:  0.109 \n",
      "\n",
      "                Current testing loss:  4.214486122131348 \n",
      " \n",
      "...completed  3457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.236712020545383, \n",
      "\n",
      "                Current testing accuracy:  0.117 \n",
      "\n",
      "                Current testing loss:  4.14381217956543 \n",
      " \n",
      "...completed  3585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.083941512595629, \n",
      "\n",
      "                Current testing accuracy:  0.091 \n",
      "\n",
      "                Current testing loss:  4.526726245880127 \n",
      " \n",
      "...completed  3713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.340107149429969, \n",
      "\n",
      "                Current testing accuracy:  0.109 \n",
      "\n",
      "                Current testing loss:  3.8902041912078857 \n",
      " \n",
      "...completed  3841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.211127921487787, \n",
      "\n",
      "                Current testing accuracy:  0.105 \n",
      "\n",
      "                Current testing loss:  3.882974624633789 \n",
      " \n",
      "...completed  3969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.995198448839801, \n",
      "\n",
      "                Current testing accuracy:  0.098 \n",
      "\n",
      "                Current testing loss:  3.6997339725494385 \n",
      " \n",
      "...completed  4097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.193885448206856, \n",
      "\n",
      "                Current testing accuracy:  0.105 \n",
      "\n",
      "                Current testing loss:  4.131247043609619 \n",
      " \n",
      "...completed  4225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.966741313561215, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  3.684644937515259 \n",
      " \n",
      "...completed  4353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.034046016895445, \n",
      "\n",
      "                Current testing accuracy:  0.098 \n",
      "\n",
      "                Current testing loss:  4.193429946899414 \n",
      " \n",
      "...completed  4481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.8351545189216267, \n",
      "\n",
      "                Current testing accuracy:  0.096 \n",
      "\n",
      "                Current testing loss:  4.29706335067749 \n",
      " \n",
      "...completed  4609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.040573429600045, \n",
      "\n",
      "                Current testing accuracy:  0.096 \n",
      "\n",
      "                Current testing loss:  4.217624664306641 \n",
      " \n",
      "...completed  4737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.9637173291121144, \n",
      "\n",
      "                Current testing accuracy:  0.097 \n",
      "\n",
      "                Current testing loss:  4.150228977203369 \n",
      " \n",
      "...completed  4865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.798225860855382, \n",
      "\n",
      "                Current testing accuracy:  0.121 \n",
      "\n",
      "                Current testing loss:  3.670684576034546 \n",
      " \n",
      "...completed  4993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.035244602153398, \n",
      "\n",
      "                Current testing accuracy:  0.101 \n",
      "\n",
      "                Current testing loss:  4.331151485443115 \n",
      " \n",
      "...completed  5121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.9617248597569414, \n",
      "\n",
      "                Current testing accuracy:  0.094 \n",
      "\n",
      "                Current testing loss:  4.051617622375488 \n",
      " \n",
      "...completed  5249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  4.029525573205319, \n",
      "\n",
      "                Current testing accuracy:  0.1 \n",
      "\n",
      "                Current testing loss:  3.9731738567352295 \n",
      " \n",
      "...completed  5377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.685203509463463, \n",
      "\n",
      "                Current testing accuracy:  0.107 \n",
      "\n",
      "                Current testing loss:  4.157733917236328 \n",
      " \n",
      "...completed  5505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.8273072837037034, \n",
      "\n",
      "                Current testing accuracy:  0.163 \n",
      "\n",
      "                Current testing loss:  3.2013115882873535 \n",
      " \n",
      "...completed  5633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.890681406373915, \n",
      "\n",
      "                Current testing accuracy:  0.108 \n",
      "\n",
      "                Current testing loss:  3.6933822631835938 \n",
      " \n",
      "...completed  5761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.8807627320747997, \n",
      "\n",
      "                Current testing accuracy:  0.119 \n",
      "\n",
      "                Current testing loss:  3.4787282943725586 \n",
      " \n",
      "...completed  5889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.8334102994649584, \n",
      "\n",
      "                Current testing accuracy:  0.114 \n",
      "\n",
      "                Current testing loss:  4.125405788421631 \n",
      " \n",
      "...completed  6017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.4503256402822444, \n",
      "\n",
      "                Current testing accuracy:  0.163 \n",
      "\n",
      "                Current testing loss:  3.2942986488342285 \n",
      " \n",
      "...completed  6145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.531457657882129, \n",
      "\n",
      "                Current testing accuracy:  0.125 \n",
      "\n",
      "                Current testing loss:  3.400735855102539 \n",
      " \n",
      "...completed  6273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.5632309680295293, \n",
      "\n",
      "                Current testing accuracy:  0.124 \n",
      "\n",
      "                Current testing loss:  3.6093013286590576 \n",
      " \n",
      "...completed  6401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.724362664288492, \n",
      "\n",
      "                Current testing accuracy:  0.109 \n",
      "\n",
      "                Current testing loss:  4.003568649291992 \n",
      " \n",
      "...completed  6529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.4110101013211533, \n",
      "\n",
      "                Current testing accuracy:  0.11 \n",
      "\n",
      "                Current testing loss:  4.124059200286865 \n",
      " \n",
      "...completed  6657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.4849060810192896, \n",
      "\n",
      "                Current testing accuracy:  0.148 \n",
      "\n",
      "                Current testing loss:  3.4617691040039062 \n",
      " \n",
      "...completed  6785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.5153123474074164, \n",
      "\n",
      "                Current testing accuracy:  0.182 \n",
      "\n",
      "                Current testing loss:  3.08951997756958 \n",
      " \n",
      "...completed  6913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.392367820677464, \n",
      "\n",
      "                Current testing accuracy:  0.119 \n",
      "\n",
      "                Current testing loss:  3.7153379917144775 \n",
      " \n",
      "...completed  7041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.6528975231049117, \n",
      "\n",
      "                Current testing accuracy:  0.139 \n",
      "\n",
      "                Current testing loss:  2.8642237186431885 \n",
      " \n",
      "...completed  7169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.209142426557264, \n",
      "\n",
      "                Current testing accuracy:  0.139 \n",
      "\n",
      "                Current testing loss:  3.5526316165924072 \n",
      " \n",
      "...completed  7297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.2534322735918977, \n",
      "\n",
      "                Current testing accuracy:  0.142 \n",
      "\n",
      "                Current testing loss:  3.2236196994781494 \n",
      " \n",
      "...completed  7425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.2786992254841607, \n",
      "\n",
      "                Current testing accuracy:  0.122 \n",
      "\n",
      "                Current testing loss:  3.655290365219116 \n",
      " \n",
      "...completed  7553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.534109565684048, \n",
      "\n",
      "                Current testing accuracy:  0.155 \n",
      "\n",
      "                Current testing loss:  3.5945208072662354 \n",
      " \n",
      "...completed  7681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.5087105356324173, \n",
      "\n",
      "                Current testing accuracy:  0.125 \n",
      "\n",
      "                Current testing loss:  3.8072915077209473 \n",
      " \n",
      "...completed  7809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.4174553511875274, \n",
      "\n",
      "                Current testing accuracy:  0.176 \n",
      "\n",
      "                Current testing loss:  2.8016011714935303 \n",
      " \n",
      "...completed  7937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.1793333406517377, \n",
      "\n",
      "                Current testing accuracy:  0.161 \n",
      "\n",
      "                Current testing loss:  3.499176502227783 \n",
      " \n",
      "...completed  8065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.1847269569334458, \n",
      "\n",
      "                Current testing accuracy:  0.112 \n",
      "\n",
      "                Current testing loss:  3.9930355548858643 \n",
      " \n",
      "...completed  8193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.289746338568875, \n",
      "\n",
      "                Current testing accuracy:  0.201 \n",
      "\n",
      "                Current testing loss:  2.3073105812072754 \n",
      " \n",
      "...completed  8321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.124361564521678, \n",
      "\n",
      "                Current testing accuracy:  0.123 \n",
      "\n",
      "                Current testing loss:  2.776186943054199 \n",
      " \n",
      "...completed  8449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.4263974571970266, \n",
      "\n",
      "                Current testing accuracy:  0.103 \n",
      "\n",
      "                Current testing loss:  3.5910942554473877 \n",
      " \n",
      "...completed  8577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.7980156422763685, \n",
      "\n",
      "                Current testing accuracy:  0.214 \n",
      "\n",
      "                Current testing loss:  2.600963830947876 \n",
      " \n",
      "...completed  8705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.9151840334234294, \n",
      "\n",
      "                Current testing accuracy:  0.261 \n",
      "\n",
      "                Current testing loss:  2.262669086456299 \n",
      " \n",
      "...completed  8833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.8339390851615462, \n",
      "\n",
      "                Current testing accuracy:  0.231 \n",
      "\n",
      "                Current testing loss:  2.9898324012756348 \n",
      " \n",
      "...completed  8961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.9747491674625053, \n",
      "\n",
      "                Current testing accuracy:  0.286 \n",
      "\n",
      "                Current testing loss:  2.3149607181549072 \n",
      " \n",
      "...completed  9089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.869052780628408, \n",
      "\n",
      "                Current testing accuracy:  0.186 \n",
      "\n",
      "                Current testing loss:  3.1166539192199707 \n",
      " \n",
      "...completed  9217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  3.0919535579014337, \n",
      "\n",
      "                Current testing accuracy:  0.337 \n",
      "\n",
      "                Current testing loss:  2.0118117332458496 \n",
      " \n",
      "...completed  9345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.8513880536467013, \n",
      "\n",
      "                Current testing accuracy:  0.178 \n",
      "\n",
      "                Current testing loss:  2.7287025451660156 \n",
      " \n",
      "...completed  9473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.626061036529677, \n",
      "\n",
      "                Current testing accuracy:  0.199 \n",
      "\n",
      "                Current testing loss:  2.481731414794922 \n",
      " \n",
      "...completed  9601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.543901687537584, \n",
      "\n",
      "                Current testing accuracy:  0.22 \n",
      "\n",
      "                Current testing loss:  2.581613302230835 \n",
      " \n",
      "...completed  9729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.8383597891088357, \n",
      "\n",
      "                Current testing accuracy:  0.25 \n",
      "\n",
      "                Current testing loss:  2.425382614135742 \n",
      " \n",
      "...completed  9857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.5011067482738554, \n",
      "\n",
      "                Current testing accuracy:  0.242 \n",
      "\n",
      "                Current testing loss:  2.2208681106567383 \n",
      " \n",
      "...completed  9985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.449645071582836, \n",
      "\n",
      "                Current testing accuracy:  0.19 \n",
      "\n",
      "                Current testing loss:  2.6520493030548096 \n",
      " \n",
      "...completed  10113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.477142282294153, \n",
      "\n",
      "                Current testing accuracy:  0.167 \n",
      "\n",
      "                Current testing loss:  3.0205891132354736 \n",
      " \n",
      "...completed  10241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.4168888826822297, \n",
      "\n",
      "                Current testing accuracy:  0.155 \n",
      "\n",
      "                Current testing loss:  2.932710647583008 \n",
      " \n",
      "...completed  10369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.6839755468363364, \n",
      "\n",
      "                Current testing accuracy:  0.184 \n",
      "\n",
      "                Current testing loss:  2.862492322921753 \n",
      " \n",
      "...completed  10497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.5197926702362565, \n",
      "\n",
      "                Current testing accuracy:  0.433 \n",
      "\n",
      "                Current testing loss:  1.6369540691375732 \n",
      " \n",
      "...completed  10625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.5040733973655733, \n",
      "\n",
      "                Current testing accuracy:  0.217 \n",
      "\n",
      "                Current testing loss:  2.7960336208343506 \n",
      " \n",
      "...completed  10753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.5050352920848127, \n",
      "\n",
      "                Current testing accuracy:  0.17 \n",
      "\n",
      "                Current testing loss:  3.3854691982269287 \n",
      " \n",
      "...completed  10881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.4607039768035026, \n",
      "\n",
      "                Current testing accuracy:  0.355 \n",
      "\n",
      "                Current testing loss:  1.8990317583084106 \n",
      " \n",
      "...completed  11009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.49991644891702, \n",
      "\n",
      "                Current testing accuracy:  0.287 \n",
      "\n",
      "                Current testing loss:  1.8648282289505005 \n",
      " \n",
      "...completed  11137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.621376073464944, \n",
      "\n",
      "                Current testing accuracy:  0.258 \n",
      "\n",
      "                Current testing loss:  2.4497313499450684 \n",
      " \n",
      "...completed  11265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.2898607784004525, \n",
      "\n",
      "                Current testing accuracy:  0.283 \n",
      "\n",
      "                Current testing loss:  2.3348453044891357 \n",
      " \n",
      "...completed  11393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.373753838076766, \n",
      "\n",
      "                Current testing accuracy:  0.285 \n",
      "\n",
      "                Current testing loss:  2.0867013931274414 \n",
      " \n",
      "...completed  11521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.3448594945826926, \n",
      "\n",
      "                Current testing accuracy:  0.263 \n",
      "\n",
      "                Current testing loss:  2.3382155895233154 \n",
      " \n",
      "...completed  11649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.552588800715636, \n",
      "\n",
      "                Current testing accuracy:  0.268 \n",
      "\n",
      "                Current testing loss:  2.1917192935943604 \n",
      " \n",
      "...completed  11777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.254681081092798, \n",
      "\n",
      "                Current testing accuracy:  0.424 \n",
      "\n",
      "                Current testing loss:  1.5719492435455322 \n",
      " \n",
      "...completed  11905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.5893705633002355, \n",
      "\n",
      "                Current testing accuracy:  0.353 \n",
      "\n",
      "                Current testing loss:  2.0103213787078857 \n",
      " \n",
      "...completed  12033  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.4254636219402155, \n",
      "\n",
      "                Current testing accuracy:  0.474 \n",
      "\n",
      "                Current testing loss:  1.4987963438034058 \n",
      " \n",
      "...completed  12161  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.0731283261694387, \n",
      "\n",
      "                Current testing accuracy:  0.192 \n",
      "\n",
      "                Current testing loss:  2.7395503520965576 \n",
      " \n",
      "...completed  12289  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.098037499927784, \n",
      "\n",
      "                Current testing accuracy:  0.173 \n",
      "\n",
      "                Current testing loss:  3.422086477279663 \n",
      " \n",
      "...completed  12417  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.2366552269259046, \n",
      "\n",
      "                Current testing accuracy:  0.238 \n",
      "\n",
      "                Current testing loss:  2.1313700675964355 \n",
      " \n",
      "...completed  12545  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.5077344491019176, \n",
      "\n",
      "                Current testing accuracy:  0.259 \n",
      "\n",
      "                Current testing loss:  3.1624813079833984 \n",
      " \n",
      "...completed  12673  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.168273062696244, \n",
      "\n",
      "                Current testing accuracy:  0.244 \n",
      "\n",
      "                Current testing loss:  2.407569646835327 \n",
      " \n",
      "...completed  12801  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.0333189445351945, \n",
      "\n",
      "                Current testing accuracy:  0.461 \n",
      "\n",
      "                Current testing loss:  1.7060275077819824 \n",
      " \n",
      "...completed  12929  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.2201775347962744, \n",
      "\n",
      "                Current testing accuracy:  0.22 \n",
      "\n",
      "                Current testing loss:  2.670497179031372 \n",
      " \n",
      "...completed  13057  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9160426709165606, \n",
      "\n",
      "                Current testing accuracy:  0.33 \n",
      "\n",
      "                Current testing loss:  2.3584980964660645 \n",
      " \n",
      "...completed  13185  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.2747822700154074, \n",
      "\n",
      "                Current testing accuracy:  0.214 \n",
      "\n",
      "                Current testing loss:  2.500241756439209 \n",
      " \n",
      "...completed  13313  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9052949727017676, \n",
      "\n",
      "                Current testing accuracy:  0.308 \n",
      "\n",
      "                Current testing loss:  2.0679843425750732 \n",
      " \n",
      "...completed  13441  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.1122852264402328, \n",
      "\n",
      "                Current testing accuracy:  0.454 \n",
      "\n",
      "                Current testing loss:  1.629886269569397 \n",
      " \n",
      "...completed  13569  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.1178631984629415, \n",
      "\n",
      "                Current testing accuracy:  0.53 \n",
      "\n",
      "                Current testing loss:  1.547133445739746 \n",
      " \n",
      "...completed  13697  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.725977875648141, \n",
      "\n",
      "                Current testing accuracy:  0.341 \n",
      "\n",
      "                Current testing loss:  2.521256685256958 \n",
      " \n",
      "...completed  13825  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8955289247860492, \n",
      "\n",
      "                Current testing accuracy:  0.321 \n",
      "\n",
      "                Current testing loss:  2.1185824871063232 \n",
      " \n",
      "...completed  13953  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.02138179987827, \n",
      "\n",
      "                Current testing accuracy:  0.416 \n",
      "\n",
      "                Current testing loss:  1.706096887588501 \n",
      " \n",
      "...completed  14081  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7175249808374815, \n",
      "\n",
      "                Current testing accuracy:  0.477 \n",
      "\n",
      "                Current testing loss:  1.7469820976257324 \n",
      " \n",
      "...completed  14209  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8944009671937607, \n",
      "\n",
      "                Current testing accuracy:  0.418 \n",
      "\n",
      "                Current testing loss:  1.951827883720398 \n",
      " \n",
      "...completed  14337  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.776108946102795, \n",
      "\n",
      "                Current testing accuracy:  0.321 \n",
      "\n",
      "                Current testing loss:  2.0811078548431396 \n",
      " \n",
      "...completed  14465  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8688832400060917, \n",
      "\n",
      "                Current testing accuracy:  0.632 \n",
      "\n",
      "                Current testing loss:  1.247835397720337 \n",
      " \n",
      "...completed  14593  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6323784084792123, \n",
      "\n",
      "                Current testing accuracy:  0.48 \n",
      "\n",
      "                Current testing loss:  1.5353718996047974 \n",
      " \n",
      "...completed  14721  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8849061998664745, \n",
      "\n",
      "                Current testing accuracy:  0.52 \n",
      "\n",
      "                Current testing loss:  1.6631296873092651 \n",
      " \n",
      "...completed  14849  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7370781580849792, \n",
      "\n",
      "                Current testing accuracy:  0.537 \n",
      "\n",
      "                Current testing loss:  1.5156474113464355 \n",
      " \n",
      "...completed  14977  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4346925127490238, \n",
      "\n",
      "                Current testing accuracy:  0.376 \n",
      "\n",
      "                Current testing loss:  1.9323413372039795 \n",
      " \n",
      "...completed  15105  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6792169125055239, \n",
      "\n",
      "                Current testing accuracy:  0.179 \n",
      "\n",
      "                Current testing loss:  4.015705585479736 \n",
      " \n",
      "...completed  15233  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6054244980582553, \n",
      "\n",
      "                Current testing accuracy:  0.369 \n",
      "\n",
      "                Current testing loss:  1.853664755821228 \n",
      " \n",
      "...completed  15361  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6227176858903718, \n",
      "\n",
      "                Current testing accuracy:  0.643 \n",
      "\n",
      "                Current testing loss:  1.1451886892318726 \n",
      " \n",
      "...completed  15489  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7341555741417505, \n",
      "\n",
      "                Current testing accuracy:  0.543 \n",
      "\n",
      "                Current testing loss:  1.4655156135559082 \n",
      " \n",
      "...completed  15617  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8222644252635725, \n",
      "\n",
      "                Current testing accuracy:  0.636 \n",
      "\n",
      "                Current testing loss:  1.088464379310608 \n",
      " \n",
      "...completed  15745  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7845624676883358, \n",
      "\n",
      "                Current testing accuracy:  0.379 \n",
      "\n",
      "                Current testing loss:  1.9935458898544312 \n",
      " \n",
      "...completed  15873  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.099187750644532, \n",
      "\n",
      "                Current testing accuracy:  0.398 \n",
      "\n",
      "                Current testing loss:  2.242048740386963 \n",
      " \n",
      "...completed  16001  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5008017758193173, \n",
      "\n",
      "                Current testing accuracy:  0.52 \n",
      "\n",
      "                Current testing loss:  1.7616645097732544 \n",
      " \n",
      "...completed  16129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9231613679247062, \n",
      "\n",
      "                Current testing accuracy:  0.404 \n",
      "\n",
      "                Current testing loss:  1.9247359037399292 \n",
      " \n",
      "...completed  16257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3272552563850013, \n",
      "\n",
      "                Current testing accuracy:  0.55 \n",
      "\n",
      "                Current testing loss:  1.3877813816070557 \n",
      " \n",
      "...completed  16385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6565272884032538, \n",
      "\n",
      "                Current testing accuracy:  0.495 \n",
      "\n",
      "                Current testing loss:  2.0360395908355713 \n",
      " \n",
      "...completed  16513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8185563153141935, \n",
      "\n",
      "                Current testing accuracy:  0.422 \n",
      "\n",
      "                Current testing loss:  1.8932712078094482 \n",
      " \n",
      "...completed  16641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.021751466339081, \n",
      "\n",
      "                Current testing accuracy:  0.276 \n",
      "\n",
      "                Current testing loss:  3.083988666534424 \n",
      " \n",
      "...completed  16769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3185442962953358, \n",
      "\n",
      "                Current testing accuracy:  0.634 \n",
      "\n",
      "                Current testing loss:  1.1695630550384521 \n",
      " \n",
      "...completed  16897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9290058808667248, \n",
      "\n",
      "                Current testing accuracy:  0.508 \n",
      "\n",
      "                Current testing loss:  1.5113434791564941 \n",
      " \n",
      "...completed  17025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5998742588483879, \n",
      "\n",
      "                Current testing accuracy:  0.556 \n",
      "\n",
      "                Current testing loss:  1.2555691003799438 \n",
      " \n",
      "...completed  17153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3439228948052815, \n",
      "\n",
      "                Current testing accuracy:  0.436 \n",
      "\n",
      "                Current testing loss:  1.6889716386795044 \n",
      " \n",
      "...completed  17281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4841257884112902, \n",
      "\n",
      "                Current testing accuracy:  0.466 \n",
      "\n",
      "                Current testing loss:  1.5647847652435303 \n",
      " \n",
      "...completed  17409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.411855800024533, \n",
      "\n",
      "                Current testing accuracy:  0.522 \n",
      "\n",
      "                Current testing loss:  1.3825401067733765 \n",
      " \n",
      "...completed  17537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7079735347374498, \n",
      "\n",
      "                Current testing accuracy:  0.464 \n",
      "\n",
      "                Current testing loss:  1.594395637512207 \n",
      " \n",
      "...completed  17665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.229961822220576, \n",
      "\n",
      "                Current testing accuracy:  0.551 \n",
      "\n",
      "                Current testing loss:  1.3997821807861328 \n",
      " \n",
      "...completed  17793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2749541531495936, \n",
      "\n",
      "                Current testing accuracy:  0.428 \n",
      "\n",
      "                Current testing loss:  1.9579848051071167 \n",
      " \n",
      "...completed  17921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7562133463733431, \n",
      "\n",
      "                Current testing accuracy:  0.54 \n",
      "\n",
      "                Current testing loss:  1.6515183448791504 \n",
      " \n",
      "...completed  18049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4398482633295089, \n",
      "\n",
      "                Current testing accuracy:  0.609 \n",
      "\n",
      "                Current testing loss:  1.3856018781661987 \n",
      " \n",
      "...completed  18177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4530765958624148, \n",
      "\n",
      "                Current testing accuracy:  0.337 \n",
      "\n",
      "                Current testing loss:  2.399567127227783 \n",
      " \n",
      "...completed  18305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.2671360765958752, \n",
      "\n",
      "                Current testing accuracy:  0.417 \n",
      "\n",
      "                Current testing loss:  2.0655720233917236 \n",
      " \n",
      "...completed  18433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.2107043992826902, \n",
      "\n",
      "                Current testing accuracy:  0.378 \n",
      "\n",
      "                Current testing loss:  2.455764055252075 \n",
      " \n",
      "...completed  18561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.020966270526685, \n",
      "\n",
      "                Current testing accuracy:  0.543 \n",
      "\n",
      "                Current testing loss:  1.767783522605896 \n",
      " \n",
      "...completed  18689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6706735659908603, \n",
      "\n",
      "                Current testing accuracy:  0.405 \n",
      "\n",
      "                Current testing loss:  2.2605843544006348 \n",
      " \n",
      "...completed  18817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.8475029175560849, \n",
      "\n",
      "                Current testing accuracy:  0.508 \n",
      "\n",
      "                Current testing loss:  1.6759159564971924 \n",
      " \n",
      "...completed  18945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.1761310760208517, \n",
      "\n",
      "                Current testing accuracy:  0.615 \n",
      "\n",
      "                Current testing loss:  1.5265045166015625 \n",
      " \n",
      "...completed  19073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.094297474445682, \n",
      "\n",
      "                Current testing accuracy:  0.462 \n",
      "\n",
      "                Current testing loss:  1.7625210285186768 \n",
      " \n",
      "...completed  19201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9297259258455597, \n",
      "\n",
      "                Current testing accuracy:  0.385 \n",
      "\n",
      "                Current testing loss:  1.9948415756225586 \n",
      " \n",
      "...completed  19329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9857824663054089, \n",
      "\n",
      "                Current testing accuracy:  0.495 \n",
      "\n",
      "                Current testing loss:  1.6893093585968018 \n",
      " \n",
      "...completed  19457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5342361760815493, \n",
      "\n",
      "                Current testing accuracy:  0.342 \n",
      "\n",
      "                Current testing loss:  2.467844009399414 \n",
      " \n",
      "...completed  19585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.718261558619247, \n",
      "\n",
      "                Current testing accuracy:  0.394 \n",
      "\n",
      "                Current testing loss:  2.1051323413848877 \n",
      " \n",
      "...completed  19713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3675851151200504, \n",
      "\n",
      "                Current testing accuracy:  0.538 \n",
      "\n",
      "                Current testing loss:  1.3591406345367432 \n",
      " \n",
      "...completed  19841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  2.025488024566698, \n",
      "\n",
      "                Current testing accuracy:  0.61 \n",
      "\n",
      "                Current testing loss:  1.1513683795928955 \n",
      " \n",
      "...completed  19969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.600580978593598, \n",
      "\n",
      "                Current testing accuracy:  0.529 \n",
      "\n",
      "                Current testing loss:  1.4754403829574585 \n",
      " \n",
      "...completed  20097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7821034713700499, \n",
      "\n",
      "                Current testing accuracy:  0.642 \n",
      "\n",
      "                Current testing loss:  1.0930951833724976 \n",
      " \n",
      "...completed  20225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6940070303879224, \n",
      "\n",
      "                Current testing accuracy:  0.556 \n",
      "\n",
      "                Current testing loss:  1.3097095489501953 \n",
      " \n",
      "...completed  20353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6148192607179226, \n",
      "\n",
      "                Current testing accuracy:  0.405 \n",
      "\n",
      "                Current testing loss:  1.8890886306762695 \n",
      " \n",
      "...completed  20481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.741021978881463, \n",
      "\n",
      "                Current testing accuracy:  0.706 \n",
      "\n",
      "                Current testing loss:  1.0412653684616089 \n",
      " \n",
      "...completed  20609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3158929582794485, \n",
      "\n",
      "                Current testing accuracy:  0.594 \n",
      "\n",
      "                Current testing loss:  1.1982802152633667 \n",
      " \n",
      "...completed  20737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.533211362488629, \n",
      "\n",
      "                Current testing accuracy:  0.491 \n",
      "\n",
      "                Current testing loss:  1.4593933820724487 \n",
      " \n",
      "...completed  20865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5057580883993893, \n",
      "\n",
      "                Current testing accuracy:  0.618 \n",
      "\n",
      "                Current testing loss:  1.1302536725997925 \n",
      " \n",
      "...completed  20993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4357917796796755, \n",
      "\n",
      "                Current testing accuracy:  0.478 \n",
      "\n",
      "                Current testing loss:  1.5616803169250488 \n",
      " \n",
      "...completed  21121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5332899069967425, \n",
      "\n",
      "                Current testing accuracy:  0.695 \n",
      "\n",
      "                Current testing loss:  0.9377365708351135 \n",
      " \n",
      "...completed  21249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6822437535233803, \n",
      "\n",
      "                Current testing accuracy:  0.661 \n",
      "\n",
      "                Current testing loss:  1.1640254259109497 \n",
      " \n",
      "...completed  21377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6043454753651076, \n",
      "\n",
      "                Current testing accuracy:  0.541 \n",
      "\n",
      "                Current testing loss:  1.3478342294692993 \n",
      " \n",
      "...completed  21505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4214261014194562, \n",
      "\n",
      "                Current testing accuracy:  0.346 \n",
      "\n",
      "                Current testing loss:  2.1215248107910156 \n",
      " \n",
      "...completed  21633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3338289978246394, \n",
      "\n",
      "                Current testing accuracy:  0.448 \n",
      "\n",
      "                Current testing loss:  1.842074990272522 \n",
      " \n",
      "...completed  21761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5514000104694787, \n",
      "\n",
      "                Current testing accuracy:  0.348 \n",
      "\n",
      "                Current testing loss:  2.3412857055664062 \n",
      " \n",
      "...completed  21889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6894177286280865, \n",
      "\n",
      "                Current testing accuracy:  0.565 \n",
      "\n",
      "                Current testing loss:  1.3426368236541748 \n",
      " \n",
      "...completed  22017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4334738808158818, \n",
      "\n",
      "                Current testing accuracy:  0.431 \n",
      "\n",
      "                Current testing loss:  1.8756217956542969 \n",
      " \n",
      "...completed  22145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.671958192921693, \n",
      "\n",
      "                Current testing accuracy:  0.553 \n",
      "\n",
      "                Current testing loss:  1.3988878726959229 \n",
      " \n",
      "...completed  22273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5859802155829925, \n",
      "\n",
      "                Current testing accuracy:  0.675 \n",
      "\n",
      "                Current testing loss:  0.99604332447052 \n",
      " \n",
      "...completed  22401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6531290316270315, \n",
      "\n",
      "                Current testing accuracy:  0.653 \n",
      "\n",
      "                Current testing loss:  1.0175350904464722 \n",
      " \n",
      "...completed  22529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6307164223827044, \n",
      "\n",
      "                Current testing accuracy:  0.522 \n",
      "\n",
      "                Current testing loss:  1.3763399124145508 \n",
      " \n",
      "...completed  22657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5280229716054805, \n",
      "\n",
      "                Current testing accuracy:  0.499 \n",
      "\n",
      "                Current testing loss:  1.705710768699646 \n",
      " \n",
      "...completed  22785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3691430825595106, \n",
      "\n",
      "                Current testing accuracy:  0.643 \n",
      "\n",
      "                Current testing loss:  1.0641518831253052 \n",
      " \n",
      "...completed  22913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.527798468322544, \n",
      "\n",
      "                Current testing accuracy:  0.502 \n",
      "\n",
      "                Current testing loss:  1.7276599407196045 \n",
      " \n",
      "...completed  23041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5741200707612961, \n",
      "\n",
      "                Current testing accuracy:  0.427 \n",
      "\n",
      "                Current testing loss:  2.1814558506011963 \n",
      " \n",
      "...completed  23169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2509104545645187, \n",
      "\n",
      "                Current testing accuracy:  0.727 \n",
      "\n",
      "                Current testing loss:  0.9097192883491516 \n",
      " \n",
      "...completed  23297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.646771010416387, \n",
      "\n",
      "                Current testing accuracy:  0.626 \n",
      "\n",
      "                Current testing loss:  1.083359956741333 \n",
      " \n",
      "...completed  23425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.9522165943345726, \n",
      "\n",
      "                Current testing accuracy:  0.665 \n",
      "\n",
      "                Current testing loss:  1.0365808010101318 \n",
      " \n",
      "...completed  23553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5806417298329052, \n",
      "\n",
      "                Current testing accuracy:  0.462 \n",
      "\n",
      "                Current testing loss:  1.7946475744247437 \n",
      " \n",
      "...completed  23681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0725683515065327, \n",
      "\n",
      "                Current testing accuracy:  0.447 \n",
      "\n",
      "                Current testing loss:  1.6603953838348389 \n",
      " \n",
      "...completed  23809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5315574843534705, \n",
      "\n",
      "                Current testing accuracy:  0.689 \n",
      "\n",
      "                Current testing loss:  0.9384265542030334 \n",
      " \n",
      "...completed  23937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4501061308118892, \n",
      "\n",
      "                Current testing accuracy:  0.342 \n",
      "\n",
      "                Current testing loss:  2.6578354835510254 \n",
      " \n",
      "...completed  24065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2359924070181023, \n",
      "\n",
      "                Current testing accuracy:  0.54 \n",
      "\n",
      "                Current testing loss:  1.3705037832260132 \n",
      " \n",
      "...completed  24193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3914063126594556, \n",
      "\n",
      "                Current testing accuracy:  0.692 \n",
      "\n",
      "                Current testing loss:  1.0496952533721924 \n",
      " \n",
      "...completed  24321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.325524030577128, \n",
      "\n",
      "                Current testing accuracy:  0.617 \n",
      "\n",
      "                Current testing loss:  1.2248374223709106 \n",
      " \n",
      "...completed  24449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9743693219327838, \n",
      "\n",
      "                Current testing accuracy:  0.58 \n",
      "\n",
      "                Current testing loss:  1.344921350479126 \n",
      " \n",
      "...completed  24577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3184767846179284, \n",
      "\n",
      "                Current testing accuracy:  0.671 \n",
      "\n",
      "                Current testing loss:  0.9564566612243652 \n",
      " \n",
      "...completed  24705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4936433281148993, \n",
      "\n",
      "                Current testing accuracy:  0.484 \n",
      "\n",
      "                Current testing loss:  1.8406093120574951 \n",
      " \n",
      "...completed  24833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4346427013570775, \n",
      "\n",
      "                Current testing accuracy:  0.344 \n",
      "\n",
      "                Current testing loss:  2.7179441452026367 \n",
      " \n",
      "...completed  24961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.645185598200726, \n",
      "\n",
      "                Current testing accuracy:  0.469 \n",
      "\n",
      "                Current testing loss:  1.7042655944824219 \n",
      " \n",
      "...completed  25089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5350368860561048, \n",
      "\n",
      "                Current testing accuracy:  0.508 \n",
      "\n",
      "                Current testing loss:  1.8514201641082764 \n",
      " \n",
      "...completed  25217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9792431551205709, \n",
      "\n",
      "                Current testing accuracy:  0.495 \n",
      "\n",
      "                Current testing loss:  1.6023643016815186 \n",
      " \n",
      "...completed  25345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2155334630786, \n",
      "\n",
      "                Current testing accuracy:  0.679 \n",
      "\n",
      "                Current testing loss:  1.0115792751312256 \n",
      " \n",
      "...completed  25473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3601668350984681, \n",
      "\n",
      "                Current testing accuracy:  0.678 \n",
      "\n",
      "                Current testing loss:  1.0468101501464844 \n",
      " \n",
      "...completed  25601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.154892669098274, \n",
      "\n",
      "                Current testing accuracy:  0.52 \n",
      "\n",
      "                Current testing loss:  1.42026948928833 \n",
      " \n",
      "...completed  25729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4315575971671706, \n",
      "\n",
      "                Current testing accuracy:  0.619 \n",
      "\n",
      "                Current testing loss:  1.1450086832046509 \n",
      " \n",
      "...completed  25857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1655923716272127, \n",
      "\n",
      "                Current testing accuracy:  0.568 \n",
      "\n",
      "                Current testing loss:  1.3176934719085693 \n",
      " \n",
      "...completed  25985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.815925003748589, \n",
      "\n",
      "                Current testing accuracy:  0.459 \n",
      "\n",
      "                Current testing loss:  1.8006255626678467 \n",
      " \n",
      "...completed  26113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3044798800510762, \n",
      "\n",
      "                Current testing accuracy:  0.539 \n",
      "\n",
      "                Current testing loss:  1.6490222215652466 \n",
      " \n",
      "...completed  26241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3646698459640376, \n",
      "\n",
      "                Current testing accuracy:  0.511 \n",
      "\n",
      "                Current testing loss:  1.5522717237472534 \n",
      " \n",
      "...completed  26369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3312807136799165, \n",
      "\n",
      "                Current testing accuracy:  0.362 \n",
      "\n",
      "                Current testing loss:  2.3682756423950195 \n",
      " \n",
      "...completed  26497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6535496167371093, \n",
      "\n",
      "                Current testing accuracy:  0.619 \n",
      "\n",
      "                Current testing loss:  1.0936143398284912 \n",
      " \n",
      "...completed  26625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2673332296084912, \n",
      "\n",
      "                Current testing accuracy:  0.734 \n",
      "\n",
      "                Current testing loss:  0.8343526721000671 \n",
      " \n",
      "...completed  26753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5773669077537988, \n",
      "\n",
      "                Current testing accuracy:  0.675 \n",
      "\n",
      "                Current testing loss:  0.936336100101471 \n",
      " \n",
      "...completed  26881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1501707093602818, \n",
      "\n",
      "                Current testing accuracy:  0.597 \n",
      "\n",
      "                Current testing loss:  1.1765780448913574 \n",
      " \n",
      "...completed  27009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2002918741551696, \n",
      "\n",
      "                Current testing accuracy:  0.552 \n",
      "\n",
      "                Current testing loss:  1.6890711784362793 \n",
      " \n",
      "...completed  27137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5217278250277104, \n",
      "\n",
      "                Current testing accuracy:  0.544 \n",
      "\n",
      "                Current testing loss:  1.5296205282211304 \n",
      " \n",
      "...completed  27265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2817186664271674, \n",
      "\n",
      "                Current testing accuracy:  0.593 \n",
      "\n",
      "                Current testing loss:  1.3106578588485718 \n",
      " \n",
      "...completed  27393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9982656245885551, \n",
      "\n",
      "                Current testing accuracy:  0.749 \n",
      "\n",
      "                Current testing loss:  0.7616904377937317 \n",
      " \n",
      "...completed  27521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0512687658703044, \n",
      "\n",
      "                Current testing accuracy:  0.563 \n",
      "\n",
      "                Current testing loss:  1.464848279953003 \n",
      " \n",
      "...completed  27649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2026341049630105, \n",
      "\n",
      "                Current testing accuracy:  0.506 \n",
      "\n",
      "                Current testing loss:  1.8252592086791992 \n",
      " \n",
      "...completed  27777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3679020562165078, \n",
      "\n",
      "                Current testing accuracy:  0.68 \n",
      "\n",
      "                Current testing loss:  1.0126065015792847 \n",
      " \n",
      "...completed  27905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.486236558978125, \n",
      "\n",
      "                Current testing accuracy:  0.742 \n",
      "\n",
      "                Current testing loss:  0.7731810808181763 \n",
      " \n",
      "...completed  28033  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4100197198877567, \n",
      "\n",
      "                Current testing accuracy:  0.593 \n",
      "\n",
      "                Current testing loss:  1.1858763694763184 \n",
      " \n",
      "...completed  28161  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2431323351330832, \n",
      "\n",
      "                Current testing accuracy:  0.563 \n",
      "\n",
      "                Current testing loss:  1.1884843111038208 \n",
      " \n",
      "...completed  28289  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2293968869907985, \n",
      "\n",
      "                Current testing accuracy:  0.587 \n",
      "\n",
      "                Current testing loss:  1.3918687105178833 \n",
      " \n",
      "...completed  28417  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2870529144340708, \n",
      "\n",
      "                Current testing accuracy:  0.656 \n",
      "\n",
      "                Current testing loss:  1.1598598957061768 \n",
      " \n",
      "...completed  28545  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3110011822707577, \n",
      "\n",
      "                Current testing accuracy:  0.629 \n",
      "\n",
      "                Current testing loss:  1.2073405981063843 \n",
      " \n",
      "...completed  28673  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3018152962570184, \n",
      "\n",
      "                Current testing accuracy:  0.538 \n",
      "\n",
      "                Current testing loss:  1.6210824251174927 \n",
      " \n",
      "...completed  28801  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3579371836982261, \n",
      "\n",
      "                Current testing accuracy:  0.636 \n",
      "\n",
      "                Current testing loss:  1.245340347290039 \n",
      " \n",
      "...completed  28929  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6643963772490906, \n",
      "\n",
      "                Current testing accuracy:  0.327 \n",
      "\n",
      "                Current testing loss:  2.828120231628418 \n",
      " \n",
      "...completed  29057  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4058616939719286, \n",
      "\n",
      "                Current testing accuracy:  0.661 \n",
      "\n",
      "                Current testing loss:  1.1486848592758179 \n",
      " \n",
      "...completed  29185  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9452243257760529, \n",
      "\n",
      "                Current testing accuracy:  0.662 \n",
      "\n",
      "                Current testing loss:  1.0656840801239014 \n",
      " \n",
      "...completed  29313  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2709069200280396, \n",
      "\n",
      "                Current testing accuracy:  0.666 \n",
      "\n",
      "                Current testing loss:  1.0686012506484985 \n",
      " \n",
      "...completed  29441  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2670124152342055, \n",
      "\n",
      "                Current testing accuracy:  0.663 \n",
      "\n",
      "                Current testing loss:  1.1322394609451294 \n",
      " \n",
      "...completed  29569  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3685218585479788, \n",
      "\n",
      "                Current testing accuracy:  0.587 \n",
      "\n",
      "                Current testing loss:  1.5076864957809448 \n",
      " \n",
      "...completed  29697  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3280388781468773, \n",
      "\n",
      "                Current testing accuracy:  0.77 \n",
      "\n",
      "                Current testing loss:  0.8006487488746643 \n",
      " \n",
      "...completed  29825  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1761027320265782, \n",
      "\n",
      "                Current testing accuracy:  0.545 \n",
      "\n",
      "                Current testing loss:  1.5006111860275269 \n",
      " \n",
      "...completed  29953  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.647549678449927, \n",
      "\n",
      "                Current testing accuracy:  0.703 \n",
      "\n",
      "                Current testing loss:  1.1470493078231812 \n",
      " \n",
      "...completed  30081  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4034005562206744, \n",
      "\n",
      "                Current testing accuracy:  0.576 \n",
      "\n",
      "                Current testing loss:  1.4410662651062012 \n",
      " \n",
      "...completed  30209  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9945091012802436, \n",
      "\n",
      "                Current testing accuracy:  0.637 \n",
      "\n",
      "                Current testing loss:  1.1121879816055298 \n",
      " \n",
      "...completed  30337  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.223911137922002, \n",
      "\n",
      "                Current testing accuracy:  0.696 \n",
      "\n",
      "                Current testing loss:  0.9242022633552551 \n",
      " \n",
      "...completed  30465  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2310367881405, \n",
      "\n",
      "                Current testing accuracy:  0.599 \n",
      "\n",
      "                Current testing loss:  1.246107578277588 \n",
      " \n",
      "...completed  30593  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.489432365658999, \n",
      "\n",
      "                Current testing accuracy:  0.519 \n",
      "\n",
      "                Current testing loss:  1.6802170276641846 \n",
      " \n",
      "...completed  30721  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0428546059171708, \n",
      "\n",
      "                Current testing accuracy:  0.617 \n",
      "\n",
      "                Current testing loss:  1.2029552459716797 \n",
      " \n",
      "...completed  30849  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.328981700717776, \n",
      "\n",
      "                Current testing accuracy:  0.604 \n",
      "\n",
      "                Current testing loss:  1.31472647190094 \n",
      " \n",
      "...completed  30977  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3229377844001817, \n",
      "\n",
      "                Current testing accuracy:  0.733 \n",
      "\n",
      "                Current testing loss:  0.8059448003768921 \n",
      " \n",
      "...completed  31105  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1628962768718338, \n",
      "\n",
      "                Current testing accuracy:  0.67 \n",
      "\n",
      "                Current testing loss:  1.2340279817581177 \n",
      " \n",
      "...completed  31233  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4528517157989427, \n",
      "\n",
      "                Current testing accuracy:  0.647 \n",
      "\n",
      "                Current testing loss:  1.0715335607528687 \n",
      " \n",
      "...completed  31361  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0096427046254917, \n",
      "\n",
      "                Current testing accuracy:  0.524 \n",
      "\n",
      "                Current testing loss:  1.3783589601516724 \n",
      " \n",
      "...completed  31489  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1283560197149851, \n",
      "\n",
      "                Current testing accuracy:  0.687 \n",
      "\n",
      "                Current testing loss:  1.0020203590393066 \n",
      " \n",
      "...completed  31617  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2086385061583798, \n",
      "\n",
      "                Current testing accuracy:  0.622 \n",
      "\n",
      "                Current testing loss:  1.1173208951950073 \n",
      " \n",
      "...completed  31745  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.6962649151407732, \n",
      "\n",
      "                Current testing accuracy:  0.637 \n",
      "\n",
      "                Current testing loss:  1.2949007749557495 \n",
      " \n",
      "...completed  31873  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2889374705360126, \n",
      "\n",
      "                Current testing accuracy:  0.757 \n",
      "\n",
      "                Current testing loss:  0.7551093697547913 \n",
      " \n",
      "...completed  32001  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1839039343351985, \n",
      "\n",
      "                Current testing accuracy:  0.646 \n",
      "\n",
      "                Current testing loss:  1.0586051940917969 \n",
      " \n",
      "...completed  32129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0573524288872704, \n",
      "\n",
      "                Current testing accuracy:  0.661 \n",
      "\n",
      "                Current testing loss:  1.0678210258483887 \n",
      " \n",
      "...completed  32257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8652384731540224, \n",
      "\n",
      "                Current testing accuracy:  0.685 \n",
      "\n",
      "                Current testing loss:  1.0131391286849976 \n",
      " \n",
      "...completed  32385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.271531290137176, \n",
      "\n",
      "                Current testing accuracy:  0.691 \n",
      "\n",
      "                Current testing loss:  0.9514980316162109 \n",
      " \n",
      "...completed  32513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8625595240318376, \n",
      "\n",
      "                Current testing accuracy:  0.666 \n",
      "\n",
      "                Current testing loss:  1.0536696910858154 \n",
      " \n",
      "...completed  32641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3796637386944326, \n",
      "\n",
      "                Current testing accuracy:  0.742 \n",
      "\n",
      "                Current testing loss:  0.9054133296012878 \n",
      " \n",
      "...completed  32769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1790410829620441, \n",
      "\n",
      "                Current testing accuracy:  0.638 \n",
      "\n",
      "                Current testing loss:  1.1734447479248047 \n",
      " \n",
      "...completed  32897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.111861780289324, \n",
      "\n",
      "                Current testing accuracy:  0.731 \n",
      "\n",
      "                Current testing loss:  0.9340934157371521 \n",
      " \n",
      "...completed  33025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.988319201764341, \n",
      "\n",
      "                Current testing accuracy:  0.749 \n",
      "\n",
      "                Current testing loss:  0.8092184066772461 \n",
      " \n",
      "...completed  33153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0444037708016936, \n",
      "\n",
      "                Current testing accuracy:  0.66 \n",
      "\n",
      "                Current testing loss:  1.0363656282424927 \n",
      " \n",
      "...completed  33281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1411456596554785, \n",
      "\n",
      "                Current testing accuracy:  0.694 \n",
      "\n",
      "                Current testing loss:  1.0550742149353027 \n",
      " \n",
      "...completed  33409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.041578767496489, \n",
      "\n",
      "                Current testing accuracy:  0.701 \n",
      "\n",
      "                Current testing loss:  0.930000364780426 \n",
      " \n",
      "...completed  33537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2355811641076642, \n",
      "\n",
      "                Current testing accuracy:  0.591 \n",
      "\n",
      "                Current testing loss:  1.4903967380523682 \n",
      " \n",
      "...completed  33665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2969588144686401, \n",
      "\n",
      "                Current testing accuracy:  0.675 \n",
      "\n",
      "                Current testing loss:  0.9238306879997253 \n",
      " \n",
      "...completed  33793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.153000184246288, \n",
      "\n",
      "                Current testing accuracy:  0.557 \n",
      "\n",
      "                Current testing loss:  1.524043321609497 \n",
      " \n",
      "...completed  33921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3910593075220383, \n",
      "\n",
      "                Current testing accuracy:  0.542 \n",
      "\n",
      "                Current testing loss:  1.4368386268615723 \n",
      " \n",
      "...completed  34049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.7163698406602634, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.696158230304718 \n",
      " \n",
      "...completed  34177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.071672805812966, \n",
      "\n",
      "                Current testing accuracy:  0.751 \n",
      "\n",
      "                Current testing loss:  0.8517275452613831 \n",
      " \n",
      "...completed  34305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9084831326590006, \n",
      "\n",
      "                Current testing accuracy:  0.659 \n",
      "\n",
      "                Current testing loss:  1.0406628847122192 \n",
      " \n",
      "...completed  34433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9896499223608943, \n",
      "\n",
      "                Current testing accuracy:  0.721 \n",
      "\n",
      "                Current testing loss:  0.8735339641571045 \n",
      " \n",
      "...completed  34561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9556456071502062, \n",
      "\n",
      "                Current testing accuracy:  0.657 \n",
      "\n",
      "                Current testing loss:  1.2136223316192627 \n",
      " \n",
      "...completed  34689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9890497276832377, \n",
      "\n",
      "                Current testing accuracy:  0.616 \n",
      "\n",
      "                Current testing loss:  1.2449101209640503 \n",
      " \n",
      "...completed  34817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0750504434677595, \n",
      "\n",
      "                Current testing accuracy:  0.724 \n",
      "\n",
      "                Current testing loss:  1.0880610942840576 \n",
      " \n",
      "...completed  34945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5071294246835194, \n",
      "\n",
      "                Current testing accuracy:  0.792 \n",
      "\n",
      "                Current testing loss:  0.659164547920227 \n",
      " \n",
      "...completed  35073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9343902813639033, \n",
      "\n",
      "                Current testing accuracy:  0.654 \n",
      "\n",
      "                Current testing loss:  1.2219239473342896 \n",
      " \n",
      "...completed  35201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.023970876067125, \n",
      "\n",
      "                Current testing accuracy:  0.739 \n",
      "\n",
      "                Current testing loss:  0.8679381608963013 \n",
      " \n",
      "...completed  35329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0472809567227728, \n",
      "\n",
      "                Current testing accuracy:  0.767 \n",
      "\n",
      "                Current testing loss:  0.7362958192825317 \n",
      " \n",
      "...completed  35457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0264159280063723, \n",
      "\n",
      "                Current testing accuracy:  0.799 \n",
      "\n",
      "                Current testing loss:  0.6612153053283691 \n",
      " \n",
      "...completed  35585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3434748313999876, \n",
      "\n",
      "                Current testing accuracy:  0.626 \n",
      "\n",
      "                Current testing loss:  1.3962079286575317 \n",
      " \n",
      "...completed  35713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1954102852847086, \n",
      "\n",
      "                Current testing accuracy:  0.705 \n",
      "\n",
      "                Current testing loss:  1.014901876449585 \n",
      " \n",
      "...completed  35841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3158438891466844, \n",
      "\n",
      "                Current testing accuracy:  0.682 \n",
      "\n",
      "                Current testing loss:  1.0600781440734863 \n",
      " \n",
      "...completed  35969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.235243408309998, \n",
      "\n",
      "                Current testing accuracy:  0.734 \n",
      "\n",
      "                Current testing loss:  0.8719943165779114 \n",
      " \n",
      "...completed  36097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.272383326483661, \n",
      "\n",
      "                Current testing accuracy:  0.632 \n",
      "\n",
      "                Current testing loss:  1.405643343925476 \n",
      " \n",
      "...completed  36225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7978182079502854, \n",
      "\n",
      "                Current testing accuracy:  0.72 \n",
      "\n",
      "                Current testing loss:  0.8982208371162415 \n",
      " \n",
      "...completed  36353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8514278180120911, \n",
      "\n",
      "                Current testing accuracy:  0.708 \n",
      "\n",
      "                Current testing loss:  1.0030219554901123 \n",
      " \n",
      "...completed  36481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0673485604959296, \n",
      "\n",
      "                Current testing accuracy:  0.641 \n",
      "\n",
      "                Current testing loss:  1.3137098550796509 \n",
      " \n",
      "...completed  36609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9386623505707234, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.7747305631637573 \n",
      " \n",
      "...completed  36737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2439147727895374, \n",
      "\n",
      "                Current testing accuracy:  0.732 \n",
      "\n",
      "                Current testing loss:  0.7960142493247986 \n",
      " \n",
      "...completed  36865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1232930329679363, \n",
      "\n",
      "                Current testing accuracy:  0.751 \n",
      "\n",
      "                Current testing loss:  0.7944260835647583 \n",
      " \n",
      "...completed  36993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9736070452012129, \n",
      "\n",
      "                Current testing accuracy:  0.75 \n",
      "\n",
      "                Current testing loss:  0.7401831746101379 \n",
      " \n",
      "...completed  37121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1300751425417275, \n",
      "\n",
      "                Current testing accuracy:  0.714 \n",
      "\n",
      "                Current testing loss:  0.8665966987609863 \n",
      " \n",
      "...completed  37249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.379889880068319, \n",
      "\n",
      "                Current testing accuracy:  0.597 \n",
      "\n",
      "                Current testing loss:  1.4815077781677246 \n",
      " \n",
      "...completed  37377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2528776849387668, \n",
      "\n",
      "                Current testing accuracy:  0.66 \n",
      "\n",
      "                Current testing loss:  1.1026921272277832 \n",
      " \n",
      "...completed  37505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1429044447838805, \n",
      "\n",
      "                Current testing accuracy:  0.8 \n",
      "\n",
      "                Current testing loss:  0.6549254059791565 \n",
      " \n",
      "...completed  37633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0478378021513208, \n",
      "\n",
      "                Current testing accuracy:  0.789 \n",
      "\n",
      "                Current testing loss:  0.7152040004730225 \n",
      " \n",
      "...completed  37761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1243669459385046, \n",
      "\n",
      "                Current testing accuracy:  0.741 \n",
      "\n",
      "                Current testing loss:  0.9003294706344604 \n",
      " \n",
      "...completed  37889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1198878358788988, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.8207573294639587 \n",
      " \n",
      "...completed  38017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9768892319999196, \n",
      "\n",
      "                Current testing accuracy:  0.588 \n",
      "\n",
      "                Current testing loss:  1.4218560457229614 \n",
      " \n",
      "...completed  38145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.225823010982289, \n",
      "\n",
      "                Current testing accuracy:  0.714 \n",
      "\n",
      "                Current testing loss:  0.905363142490387 \n",
      " \n",
      "...completed  38273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.24844551532113, \n",
      "\n",
      "                Current testing accuracy:  0.773 \n",
      "\n",
      "                Current testing loss:  0.7253289222717285 \n",
      " \n",
      "...completed  38401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0258883611225542, \n",
      "\n",
      "                Current testing accuracy:  0.645 \n",
      "\n",
      "                Current testing loss:  1.1716396808624268 \n",
      " \n",
      "...completed  38529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.859506090093936, \n",
      "\n",
      "                Current testing accuracy:  0.752 \n",
      "\n",
      "                Current testing loss:  0.7613711357116699 \n",
      " \n",
      "...completed  38657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2595809368036726, \n",
      "\n",
      "                Current testing accuracy:  0.832 \n",
      "\n",
      "                Current testing loss:  0.5513008832931519 \n",
      " \n",
      "...completed  38785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0422058547617468, \n",
      "\n",
      "                Current testing accuracy:  0.622 \n",
      "\n",
      "                Current testing loss:  1.367301106452942 \n",
      " \n",
      "...completed  38913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0626366360469888, \n",
      "\n",
      "                Current testing accuracy:  0.714 \n",
      "\n",
      "                Current testing loss:  1.1051931381225586 \n",
      " \n",
      "...completed  39041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3321182574122838, \n",
      "\n",
      "                Current testing accuracy:  0.518 \n",
      "\n",
      "                Current testing loss:  1.738444447517395 \n",
      " \n",
      "...completed  39169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7742276910172166, \n",
      "\n",
      "                Current testing accuracy:  0.695 \n",
      "\n",
      "                Current testing loss:  0.9741286635398865 \n",
      " \n",
      "...completed  39297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.826140365490275, \n",
      "\n",
      "                Current testing accuracy:  0.661 \n",
      "\n",
      "                Current testing loss:  1.034469485282898 \n",
      " \n",
      "...completed  39425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.250663926938584, \n",
      "\n",
      "                Current testing accuracy:  0.694 \n",
      "\n",
      "                Current testing loss:  0.9626694321632385 \n",
      " \n",
      "...completed  39553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2021333594988448, \n",
      "\n",
      "                Current testing accuracy:  0.732 \n",
      "\n",
      "                Current testing loss:  0.8016287088394165 \n",
      " \n",
      "...completed  39681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0754704149489456, \n",
      "\n",
      "                Current testing accuracy:  0.711 \n",
      "\n",
      "                Current testing loss:  0.9691113829612732 \n",
      " \n",
      "...completed  39809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9800136660557488, \n",
      "\n",
      "                Current testing accuracy:  0.711 \n",
      "\n",
      "                Current testing loss:  0.9493952989578247 \n",
      " \n",
      "...completed  39937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9412817297476579, \n",
      "\n",
      "                Current testing accuracy:  0.684 \n",
      "\n",
      "                Current testing loss:  1.0251367092132568 \n",
      " \n",
      "...completed  40065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3092362997064555, \n",
      "\n",
      "                Current testing accuracy:  0.727 \n",
      "\n",
      "                Current testing loss:  1.0144728422164917 \n",
      " \n",
      "...completed  40193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0213394273453131, \n",
      "\n",
      "                Current testing accuracy:  0.765 \n",
      "\n",
      "                Current testing loss:  0.7474063634872437 \n",
      " \n",
      "...completed  40321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1183443484783169, \n",
      "\n",
      "                Current testing accuracy:  0.635 \n",
      "\n",
      "                Current testing loss:  1.314656376838684 \n",
      " \n",
      "...completed  40449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.690455701709439, \n",
      "\n",
      "                Current testing accuracy:  0.706 \n",
      "\n",
      "                Current testing loss:  0.9305192828178406 \n",
      " \n",
      "...completed  40577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.105465699629093, \n",
      "\n",
      "                Current testing accuracy:  0.641 \n",
      "\n",
      "                Current testing loss:  1.241657018661499 \n",
      " \n",
      "...completed  40705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1013056503817893, \n",
      "\n",
      "                Current testing accuracy:  0.617 \n",
      "\n",
      "                Current testing loss:  1.2463676929473877 \n",
      " \n",
      "...completed  40833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.5092336771460282, \n",
      "\n",
      "                Current testing accuracy:  0.631 \n",
      "\n",
      "                Current testing loss:  1.4371042251586914 \n",
      " \n",
      "...completed  40961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0420225952026385, \n",
      "\n",
      "                Current testing accuracy:  0.7 \n",
      "\n",
      "                Current testing loss:  1.032456636428833 \n",
      " \n",
      "...completed  41089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.15960535433571, \n",
      "\n",
      "                Current testing accuracy:  0.788 \n",
      "\n",
      "                Current testing loss:  0.7691518068313599 \n",
      " \n",
      "...completed  41217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9406355018374626, \n",
      "\n",
      "                Current testing accuracy:  0.69 \n",
      "\n",
      "                Current testing loss:  1.1104521751403809 \n",
      " \n",
      "...completed  41345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0223319825161843, \n",
      "\n",
      "                Current testing accuracy:  0.716 \n",
      "\n",
      "                Current testing loss:  1.013997197151184 \n",
      " \n",
      "...completed  41473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0488879371006448, \n",
      "\n",
      "                Current testing accuracy:  0.637 \n",
      "\n",
      "                Current testing loss:  1.4445089101791382 \n",
      " \n",
      "...completed  41601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.241592562758683, \n",
      "\n",
      "                Current testing accuracy:  0.736 \n",
      "\n",
      "                Current testing loss:  0.8625162839889526 \n",
      " \n",
      "...completed  41729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2709877104248548, \n",
      "\n",
      "                Current testing accuracy:  0.58 \n",
      "\n",
      "                Current testing loss:  1.4156216382980347 \n",
      " \n",
      "...completed  41857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3053151750143321, \n",
      "\n",
      "                Current testing accuracy:  0.759 \n",
      "\n",
      "                Current testing loss:  0.8087244629859924 \n",
      " \n",
      "...completed  41985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8597650122837077, \n",
      "\n",
      "                Current testing accuracy:  0.65 \n",
      "\n",
      "                Current testing loss:  1.2249207496643066 \n",
      " \n",
      "...completed  42113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2253333617925364, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.8580569624900818 \n",
      " \n",
      "...completed  42241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3036388706081536, \n",
      "\n",
      "                Current testing accuracy:  0.668 \n",
      "\n",
      "                Current testing loss:  1.227953314781189 \n",
      " \n",
      "...completed  42369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8607165049205034, \n",
      "\n",
      "                Current testing accuracy:  0.723 \n",
      "\n",
      "                Current testing loss:  0.9383342266082764 \n",
      " \n",
      "...completed  42497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5844088637909977, \n",
      "\n",
      "                Current testing accuracy:  0.726 \n",
      "\n",
      "                Current testing loss:  0.9538847804069519 \n",
      " \n",
      "...completed  42625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2817968444364851, \n",
      "\n",
      "                Current testing accuracy:  0.622 \n",
      "\n",
      "                Current testing loss:  1.2518479824066162 \n",
      " \n",
      "...completed  42753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0776097556126985, \n",
      "\n",
      "                Current testing accuracy:  0.714 \n",
      "\n",
      "                Current testing loss:  0.8953083753585815 \n",
      " \n",
      "...completed  42881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0572640723051006, \n",
      "\n",
      "                Current testing accuracy:  0.733 \n",
      "\n",
      "                Current testing loss:  1.02243173122406 \n",
      " \n",
      "...completed  43009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.801536924289401, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.727310061454773 \n",
      " \n",
      "...completed  43137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8467569997910545, \n",
      "\n",
      "                Current testing accuracy:  0.775 \n",
      "\n",
      "                Current testing loss:  0.8027193546295166 \n",
      " \n",
      "...completed  43265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.019726415715457, \n",
      "\n",
      "                Current testing accuracy:  0.767 \n",
      "\n",
      "                Current testing loss:  0.8046261668205261 \n",
      " \n",
      "...completed  43393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3339122787880555, \n",
      "\n",
      "                Current testing accuracy:  0.713 \n",
      "\n",
      "                Current testing loss:  0.9714052081108093 \n",
      " \n",
      "...completed  43521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2209320906492849, \n",
      "\n",
      "                Current testing accuracy:  0.623 \n",
      "\n",
      "                Current testing loss:  1.3866503238677979 \n",
      " \n",
      "...completed  43649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1103341400984306, \n",
      "\n",
      "                Current testing accuracy:  0.707 \n",
      "\n",
      "                Current testing loss:  0.98091721534729 \n",
      " \n",
      "...completed  43777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.987257147963021, \n",
      "\n",
      "                Current testing accuracy:  0.726 \n",
      "\n",
      "                Current testing loss:  0.9501310586929321 \n",
      " \n",
      "...completed  43905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3813498243946256, \n",
      "\n",
      "                Current testing accuracy:  0.694 \n",
      "\n",
      "                Current testing loss:  1.1264770030975342 \n",
      " \n",
      "...completed  44033  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2928271887345488, \n",
      "\n",
      "                Current testing accuracy:  0.599 \n",
      "\n",
      "                Current testing loss:  1.2526568174362183 \n",
      " \n",
      "...completed  44161  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2608730025041375, \n",
      "\n",
      "                Current testing accuracy:  0.684 \n",
      "\n",
      "                Current testing loss:  1.0896300077438354 \n",
      " \n",
      "...completed  44289  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9800179991906219, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  0.8846126794815063 \n",
      " \n",
      "...completed  44417  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8745431829103154, \n",
      "\n",
      "                Current testing accuracy:  0.764 \n",
      "\n",
      "                Current testing loss:  0.8021618723869324 \n",
      " \n",
      "...completed  44545  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9691991536798739, \n",
      "\n",
      "                Current testing accuracy:  0.767 \n",
      "\n",
      "                Current testing loss:  0.8096385598182678 \n",
      " \n",
      "...completed  44673  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1317206059296439, \n",
      "\n",
      "                Current testing accuracy:  0.654 \n",
      "\n",
      "                Current testing loss:  1.0860724449157715 \n",
      " \n",
      "...completed  44801  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.925792999818519, \n",
      "\n",
      "                Current testing accuracy:  0.69 \n",
      "\n",
      "                Current testing loss:  1.0682131052017212 \n",
      " \n",
      "...completed  44929  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8033682984308683, \n",
      "\n",
      "                Current testing accuracy:  0.699 \n",
      "\n",
      "                Current testing loss:  1.0352237224578857 \n",
      " \n",
      "...completed  45057  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3122910934498009, \n",
      "\n",
      "                Current testing accuracy:  0.816 \n",
      "\n",
      "                Current testing loss:  0.5845133066177368 \n",
      " \n",
      "...completed  45185  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0809553778186638, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.7626925706863403 \n",
      " \n",
      "...completed  45313  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8835853397134947, \n",
      "\n",
      "                Current testing accuracy:  0.699 \n",
      "\n",
      "                Current testing loss:  0.9297450184822083 \n",
      " \n",
      "...completed  45441  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8382834237385168, \n",
      "\n",
      "                Current testing accuracy:  0.668 \n",
      "\n",
      "                Current testing loss:  0.9995123147964478 \n",
      " \n",
      "...completed  45569  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.240336623014187, \n",
      "\n",
      "                Current testing accuracy:  0.725 \n",
      "\n",
      "                Current testing loss:  0.9353002309799194 \n",
      " \n",
      "...completed  45697  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0544902139674832, \n",
      "\n",
      "                Current testing accuracy:  0.718 \n",
      "\n",
      "                Current testing loss:  0.8858683109283447 \n",
      " \n",
      "...completed  45825  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0391856635978187, \n",
      "\n",
      "                Current testing accuracy:  0.796 \n",
      "\n",
      "                Current testing loss:  0.7024438977241516 \n",
      " \n",
      "...completed  45953  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7477688644755212, \n",
      "\n",
      "                Current testing accuracy:  0.751 \n",
      "\n",
      "                Current testing loss:  0.8021082878112793 \n",
      " \n",
      "...completed  46081  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8766547502348931, \n",
      "\n",
      "                Current testing accuracy:  0.721 \n",
      "\n",
      "                Current testing loss:  0.8871396780014038 \n",
      " \n",
      "...completed  46209  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1028124752511062, \n",
      "\n",
      "                Current testing accuracy:  0.71 \n",
      "\n",
      "                Current testing loss:  1.0740752220153809 \n",
      " \n",
      "...completed  46337  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8672088227639989, \n",
      "\n",
      "                Current testing accuracy:  0.689 \n",
      "\n",
      "                Current testing loss:  0.9546248912811279 \n",
      " \n",
      "...completed  46465  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0513929699577673, \n",
      "\n",
      "                Current testing accuracy:  0.724 \n",
      "\n",
      "                Current testing loss:  0.983969509601593 \n",
      " \n",
      "...completed  46593  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1798445021758428, \n",
      "\n",
      "                Current testing accuracy:  0.785 \n",
      "\n",
      "                Current testing loss:  0.6928489208221436 \n",
      " \n",
      "...completed  46721  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9103397606476022, \n",
      "\n",
      "                Current testing accuracy:  0.774 \n",
      "\n",
      "                Current testing loss:  0.7442940473556519 \n",
      " \n",
      "...completed  46849  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7220225701080167, \n",
      "\n",
      "                Current testing accuracy:  0.774 \n",
      "\n",
      "                Current testing loss:  0.7457816004753113 \n",
      " \n",
      "...completed  46977  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1713838843689928, \n",
      "\n",
      "                Current testing accuracy:  0.741 \n",
      "\n",
      "                Current testing loss:  0.8279982805252075 \n",
      " \n",
      "...completed  47105  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9059080890511488, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.8126627206802368 \n",
      " \n",
      "...completed  47233  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.161110477712807, \n",
      "\n",
      "                Current testing accuracy:  0.688 \n",
      "\n",
      "                Current testing loss:  1.0668971538543701 \n",
      " \n",
      "...completed  47361  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9610852095562663, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.8764929175376892 \n",
      " \n",
      "...completed  47489  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1568236795324474, \n",
      "\n",
      "                Current testing accuracy:  0.738 \n",
      "\n",
      "                Current testing loss:  0.8559229969978333 \n",
      " \n",
      "...completed  47617  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8182284445163646, \n",
      "\n",
      "                Current testing accuracy:  0.756 \n",
      "\n",
      "                Current testing loss:  0.7911362648010254 \n",
      " \n",
      "...completed  47745  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1352232767408523, \n",
      "\n",
      "                Current testing accuracy:  0.779 \n",
      "\n",
      "                Current testing loss:  0.6802865862846375 \n",
      " \n",
      "...completed  47873  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9435149227514898, \n",
      "\n",
      "                Current testing accuracy:  0.718 \n",
      "\n",
      "                Current testing loss:  0.9620407819747925 \n",
      " \n",
      "...completed  48001  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.187501064730391, \n",
      "\n",
      "                Current testing accuracy:  0.684 \n",
      "\n",
      "                Current testing loss:  1.3066887855529785 \n",
      " \n",
      "...completed  48129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9866149150966379, \n",
      "\n",
      "                Current testing accuracy:  0.802 \n",
      "\n",
      "                Current testing loss:  0.6661202907562256 \n",
      " \n",
      "...completed  48257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8845903109963134, \n",
      "\n",
      "                Current testing accuracy:  0.779 \n",
      "\n",
      "                Current testing loss:  0.748672366142273 \n",
      " \n",
      "...completed  48385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.113628505550821, \n",
      "\n",
      "                Current testing accuracy:  0.626 \n",
      "\n",
      "                Current testing loss:  1.3126513957977295 \n",
      " \n",
      "...completed  48513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9960049343896666, \n",
      "\n",
      "                Current testing accuracy:  0.676 \n",
      "\n",
      "                Current testing loss:  1.3203288316726685 \n",
      " \n",
      "...completed  48641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2869664913895917, \n",
      "\n",
      "                Current testing accuracy:  0.744 \n",
      "\n",
      "                Current testing loss:  0.8896468877792358 \n",
      " \n",
      "...completed  48769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8910081517149209, \n",
      "\n",
      "                Current testing accuracy:  0.769 \n",
      "\n",
      "                Current testing loss:  0.7320122122764587 \n",
      " \n",
      "...completed  48897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.269977900556114, \n",
      "\n",
      "                Current testing accuracy:  0.791 \n",
      "\n",
      "                Current testing loss:  0.7275816798210144 \n",
      " \n",
      "...completed  49025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.4561171838350284, \n",
      "\n",
      "                Current testing accuracy:  0.785 \n",
      "\n",
      "                Current testing loss:  0.7407651543617249 \n",
      " \n",
      "...completed  49153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0429630181433394, \n",
      "\n",
      "                Current testing accuracy:  0.685 \n",
      "\n",
      "                Current testing loss:  1.0138400793075562 \n",
      " \n",
      "...completed  49281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0487062476547067, \n",
      "\n",
      "                Current testing accuracy:  0.815 \n",
      "\n",
      "                Current testing loss:  0.6126987338066101 \n",
      " \n",
      "...completed  49409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1285897442235324, \n",
      "\n",
      "                Current testing accuracy:  0.772 \n",
      "\n",
      "                Current testing loss:  0.7848514318466187 \n",
      " \n",
      "...completed  49537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1518491366191626, \n",
      "\n",
      "                Current testing accuracy:  0.688 \n",
      "\n",
      "                Current testing loss:  1.181931972503662 \n",
      " \n",
      "...completed  49665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8918636655469072, \n",
      "\n",
      "                Current testing accuracy:  0.762 \n",
      "\n",
      "                Current testing loss:  0.8017090559005737 \n",
      " \n",
      "...completed  49793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9770672823611619, \n",
      "\n",
      "                Current testing accuracy:  0.762 \n",
      "\n",
      "                Current testing loss:  0.8391170501708984 \n",
      " \n",
      "...completed  49921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9373545942398422, \n",
      "\n",
      "                Current testing accuracy:  0.705 \n",
      "\n",
      "                Current testing loss:  0.9734053015708923 \n",
      " \n",
      "...completed  50049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8660966185142183, \n",
      "\n",
      "                Current testing accuracy:  0.754 \n",
      "\n",
      "                Current testing loss:  0.8506090044975281 \n",
      " \n",
      "...completed  50177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3913790905954495, \n",
      "\n",
      "                Current testing accuracy:  0.754 \n",
      "\n",
      "                Current testing loss:  0.8090481758117676 \n",
      " \n",
      "...completed  50305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0167989344858341, \n",
      "\n",
      "                Current testing accuracy:  0.715 \n",
      "\n",
      "                Current testing loss:  0.9949955940246582 \n",
      " \n",
      "...completed  50433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2805788914602916, \n",
      "\n",
      "                Current testing accuracy:  0.599 \n",
      "\n",
      "                Current testing loss:  1.4716246128082275 \n",
      " \n",
      "...completed  50561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1958204749839698, \n",
      "\n",
      "                Current testing accuracy:  0.677 \n",
      "\n",
      "                Current testing loss:  1.0937875509262085 \n",
      " \n",
      "...completed  50689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9355088441122814, \n",
      "\n",
      "                Current testing accuracy:  0.658 \n",
      "\n",
      "                Current testing loss:  1.3175867795944214 \n",
      " \n",
      "...completed  50817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2130612407347452, \n",
      "\n",
      "                Current testing accuracy:  0.689 \n",
      "\n",
      "                Current testing loss:  1.1019130945205688 \n",
      " \n",
      "...completed  50945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0277075308209476, \n",
      "\n",
      "                Current testing accuracy:  0.464 \n",
      "\n",
      "                Current testing loss:  2.0106041431427 \n",
      " \n",
      "...completed  51073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0053064005609826, \n",
      "\n",
      "                Current testing accuracy:  0.748 \n",
      "\n",
      "                Current testing loss:  0.923778772354126 \n",
      " \n",
      "...completed  51201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9243328722168371, \n",
      "\n",
      "                Current testing accuracy:  0.762 \n",
      "\n",
      "                Current testing loss:  0.8467655181884766 \n",
      " \n",
      "...completed  51329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0953898798064274, \n",
      "\n",
      "                Current testing accuracy:  0.827 \n",
      "\n",
      "                Current testing loss:  0.5635995268821716 \n",
      " \n",
      "...completed  51457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8210875100214481, \n",
      "\n",
      "                Current testing accuracy:  0.736 \n",
      "\n",
      "                Current testing loss:  0.9312089681625366 \n",
      " \n",
      "...completed  51585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8592606530983815, \n",
      "\n",
      "                Current testing accuracy:  0.474 \n",
      "\n",
      "                Current testing loss:  2.1791892051696777 \n",
      " \n",
      "...completed  51713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8742310671251765, \n",
      "\n",
      "                Current testing accuracy:  0.735 \n",
      "\n",
      "                Current testing loss:  0.8943145871162415 \n",
      " \n",
      "...completed  51841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7426516323055594, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.8403341174125671 \n",
      " \n",
      "...completed  51969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6323931014550439, \n",
      "\n",
      "                Current testing accuracy:  0.783 \n",
      "\n",
      "                Current testing loss:  0.7147373557090759 \n",
      " \n",
      "...completed  52097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1329434842733406, \n",
      "\n",
      "                Current testing accuracy:  0.718 \n",
      "\n",
      "                Current testing loss:  1.046581745147705 \n",
      " \n",
      "...completed  52225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.081828903753017, \n",
      "\n",
      "                Current testing accuracy:  0.76 \n",
      "\n",
      "                Current testing loss:  0.8162050843238831 \n",
      " \n",
      "...completed  52353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8203016449816403, \n",
      "\n",
      "                Current testing accuracy:  0.754 \n",
      "\n",
      "                Current testing loss:  0.8018979430198669 \n",
      " \n",
      "...completed  52481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9282934541121985, \n",
      "\n",
      "                Current testing accuracy:  0.746 \n",
      "\n",
      "                Current testing loss:  0.9098772406578064 \n",
      " \n",
      "...completed  52609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0107149727920728, \n",
      "\n",
      "                Current testing accuracy:  0.755 \n",
      "\n",
      "                Current testing loss:  0.7964701652526855 \n",
      " \n",
      "...completed  52737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.15963884320783, \n",
      "\n",
      "                Current testing accuracy:  0.725 \n",
      "\n",
      "                Current testing loss:  0.982187032699585 \n",
      " \n",
      "...completed  52865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7822679727963191, \n",
      "\n",
      "                Current testing accuracy:  0.688 \n",
      "\n",
      "                Current testing loss:  1.0931518077850342 \n",
      " \n",
      "...completed  52993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1568126045335276, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.8173073530197144 \n",
      " \n",
      "...completed  53121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.116479965310333, \n",
      "\n",
      "                Current testing accuracy:  0.726 \n",
      "\n",
      "                Current testing loss:  0.9961944222450256 \n",
      " \n",
      "...completed  53249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8824335656657709, \n",
      "\n",
      "                Current testing accuracy:  0.757 \n",
      "\n",
      "                Current testing loss:  0.8142272233963013 \n",
      " \n",
      "...completed  53377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8769106109014224, \n",
      "\n",
      "                Current testing accuracy:  0.689 \n",
      "\n",
      "                Current testing loss:  1.148382544517517 \n",
      " \n",
      "...completed  53505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.002888912633182, \n",
      "\n",
      "                Current testing accuracy:  0.775 \n",
      "\n",
      "                Current testing loss:  0.75949627161026 \n",
      " \n",
      "...completed  53633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8458370614188482, \n",
      "\n",
      "                Current testing accuracy:  0.804 \n",
      "\n",
      "                Current testing loss:  0.7002682089805603 \n",
      " \n",
      "...completed  53761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.788711420554371, \n",
      "\n",
      "                Current testing accuracy:  0.734 \n",
      "\n",
      "                Current testing loss:  0.9755586981773376 \n",
      " \n",
      "...completed  53889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9941743805756111, \n",
      "\n",
      "                Current testing accuracy:  0.8 \n",
      "\n",
      "                Current testing loss:  0.6732536554336548 \n",
      " \n",
      "...completed  54017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8703799504477843, \n",
      "\n",
      "                Current testing accuracy:  0.746 \n",
      "\n",
      "                Current testing loss:  0.8516390323638916 \n",
      " \n",
      "...completed  54145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7671162388109991, \n",
      "\n",
      "                Current testing accuracy:  0.7 \n",
      "\n",
      "                Current testing loss:  1.050014615058899 \n",
      " \n",
      "...completed  54273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1699354619860003, \n",
      "\n",
      "                Current testing accuracy:  0.795 \n",
      "\n",
      "                Current testing loss:  0.7068294286727905 \n",
      " \n",
      "...completed  54401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7874627933687106, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.8305509090423584 \n",
      " \n",
      "...completed  54529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6615169117548021, \n",
      "\n",
      "                Current testing accuracy:  0.725 \n",
      "\n",
      "                Current testing loss:  0.9732840061187744 \n",
      " \n",
      "...completed  54657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8487219795333161, \n",
      "\n",
      "                Current testing accuracy:  0.794 \n",
      "\n",
      "                Current testing loss:  0.7864652276039124 \n",
      " \n",
      "...completed  54785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1136561463409755, \n",
      "\n",
      "                Current testing accuracy:  0.756 \n",
      "\n",
      "                Current testing loss:  0.8343742489814758 \n",
      " \n",
      "...completed  54913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8227640466019608, \n",
      "\n",
      "                Current testing accuracy:  0.695 \n",
      "\n",
      "                Current testing loss:  1.1033806800842285 \n",
      " \n",
      "...completed  55041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.068128562473575, \n",
      "\n",
      "                Current testing accuracy:  0.798 \n",
      "\n",
      "                Current testing loss:  0.6466104984283447 \n",
      " \n",
      "...completed  55169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1867389731914155, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.7900967597961426 \n",
      " \n",
      "...completed  55297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9247819350252939, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.8915719389915466 \n",
      " \n",
      "...completed  55425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7751849943674074, \n",
      "\n",
      "                Current testing accuracy:  0.819 \n",
      "\n",
      "                Current testing loss:  0.5971309542655945 \n",
      " \n",
      "...completed  55553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9384365802283519, \n",
      "\n",
      "                Current testing accuracy:  0.76 \n",
      "\n",
      "                Current testing loss:  0.8251571655273438 \n",
      " \n",
      "...completed  55681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0582128135289715, \n",
      "\n",
      "                Current testing accuracy:  0.747 \n",
      "\n",
      "                Current testing loss:  0.8677630424499512 \n",
      " \n",
      "...completed  55809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1692278849569107, \n",
      "\n",
      "                Current testing accuracy:  0.811 \n",
      "\n",
      "                Current testing loss:  0.6523266434669495 \n",
      " \n",
      "...completed  55937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1168476936139058, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.8698708415031433 \n",
      " \n",
      "...completed  56065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0245641895942912, \n",
      "\n",
      "                Current testing accuracy:  0.81 \n",
      "\n",
      "                Current testing loss:  0.6060857176780701 \n",
      " \n",
      "...completed  56193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0161745062702887, \n",
      "\n",
      "                Current testing accuracy:  0.772 \n",
      "\n",
      "                Current testing loss:  0.7477411031723022 \n",
      " \n",
      "...completed  56321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1139104078632087, \n",
      "\n",
      "                Current testing accuracy:  0.754 \n",
      "\n",
      "                Current testing loss:  0.7802406549453735 \n",
      " \n",
      "...completed  56449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9109465403928212, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.7279035449028015 \n",
      " \n",
      "...completed  56577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.917732962284628, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.8728782534599304 \n",
      " \n",
      "...completed  56705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7198424000409673, \n",
      "\n",
      "                Current testing accuracy:  0.646 \n",
      "\n",
      "                Current testing loss:  1.210986614227295 \n",
      " \n",
      "...completed  56833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3418488659068046, \n",
      "\n",
      "                Current testing accuracy:  0.719 \n",
      "\n",
      "                Current testing loss:  0.9851939678192139 \n",
      " \n",
      "...completed  56961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8559439764685521, \n",
      "\n",
      "                Current testing accuracy:  0.728 \n",
      "\n",
      "                Current testing loss:  0.9180278182029724 \n",
      " \n",
      "...completed  57089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1664643594504387, \n",
      "\n",
      "                Current testing accuracy:  0.832 \n",
      "\n",
      "                Current testing loss:  0.5926369428634644 \n",
      " \n",
      "...completed  57217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.914633246508922, \n",
      "\n",
      "                Current testing accuracy:  0.828 \n",
      "\n",
      "                Current testing loss:  0.593554675579071 \n",
      " \n",
      "...completed  57345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.882867974253827, \n",
      "\n",
      "                Current testing accuracy:  0.671 \n",
      "\n",
      "                Current testing loss:  1.1277676820755005 \n",
      " \n",
      "...completed  57473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.138063725699503, \n",
      "\n",
      "                Current testing accuracy:  0.715 \n",
      "\n",
      "                Current testing loss:  1.0939396619796753 \n",
      " \n",
      "...completed  57601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3268230269898282, \n",
      "\n",
      "                Current testing accuracy:  0.707 \n",
      "\n",
      "                Current testing loss:  0.9916451573371887 \n",
      " \n",
      "...completed  57729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6974596536499149, \n",
      "\n",
      "                Current testing accuracy:  0.609 \n",
      "\n",
      "                Current testing loss:  1.558039903640747 \n",
      " \n",
      "...completed  57857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9861792287869378, \n",
      "\n",
      "                Current testing accuracy:  0.793 \n",
      "\n",
      "                Current testing loss:  0.6874174475669861 \n",
      " \n",
      "...completed  57985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7551083564037224, \n",
      "\n",
      "                Current testing accuracy:  0.808 \n",
      "\n",
      "                Current testing loss:  0.6325914263725281 \n",
      " \n",
      "...completed  58113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7279539422874359, \n",
      "\n",
      "                Current testing accuracy:  0.695 \n",
      "\n",
      "                Current testing loss:  0.9976333379745483 \n",
      " \n",
      "...completed  58241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7145494623109698, \n",
      "\n",
      "                Current testing accuracy:  0.726 \n",
      "\n",
      "                Current testing loss:  0.9593989253044128 \n",
      " \n",
      "...completed  58369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0306381584425637, \n",
      "\n",
      "                Current testing accuracy:  0.617 \n",
      "\n",
      "                Current testing loss:  1.3645867109298706 \n",
      " \n",
      "...completed  58497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8583825827067812, \n",
      "\n",
      "                Current testing accuracy:  0.741 \n",
      "\n",
      "                Current testing loss:  0.9035946726799011 \n",
      " \n",
      "...completed  58625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1443162061752545, \n",
      "\n",
      "                Current testing accuracy:  0.724 \n",
      "\n",
      "                Current testing loss:  1.0455613136291504 \n",
      " \n",
      "...completed  58753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.022854945408767, \n",
      "\n",
      "                Current testing accuracy:  0.791 \n",
      "\n",
      "                Current testing loss:  0.7091443538665771 \n",
      " \n",
      "...completed  58881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9581445001508722, \n",
      "\n",
      "                Current testing accuracy:  0.722 \n",
      "\n",
      "                Current testing loss:  0.9179469347000122 \n",
      " \n",
      "...completed  59009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.00648918929042, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.8241239786148071 \n",
      " \n",
      "...completed  59137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.038949318526683, \n",
      "\n",
      "                Current testing accuracy:  0.736 \n",
      "\n",
      "                Current testing loss:  0.8422315716743469 \n",
      " \n",
      "...completed  59265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2462302410735226, \n",
      "\n",
      "                Current testing accuracy:  0.712 \n",
      "\n",
      "                Current testing loss:  0.9581857919692993 \n",
      " \n",
      "...completed  59393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0531642154767695, \n",
      "\n",
      "                Current testing accuracy:  0.632 \n",
      "\n",
      "                Current testing loss:  1.3395963907241821 \n",
      " \n",
      "...completed  59521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.842353542607924, \n",
      "\n",
      "                Current testing accuracy:  0.775 \n",
      "\n",
      "                Current testing loss:  0.8357599973678589 \n",
      " \n",
      "...completed  59649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0476541438463052, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.9210953712463379 \n",
      " \n",
      "...completed  59777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0271926005875862, \n",
      "\n",
      "                Current testing accuracy:  0.765 \n",
      "\n",
      "                Current testing loss:  0.8927894830703735 \n",
      " \n",
      "...completed  59905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.037648226377673, \n",
      "\n",
      "                Current testing accuracy:  0.797 \n",
      "\n",
      "                Current testing loss:  0.6960724592208862 \n",
      " \n",
      "...completed  60033  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9533316409723582, \n",
      "\n",
      "                Current testing accuracy:  0.718 \n",
      "\n",
      "                Current testing loss:  1.0814950466156006 \n",
      " \n",
      "...completed  60161  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.918592569207739, \n",
      "\n",
      "                Current testing accuracy:  0.707 \n",
      "\n",
      "                Current testing loss:  0.9842125773429871 \n",
      " \n",
      "...completed  60289  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9156591136015724, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.7586045265197754 \n",
      " \n",
      "...completed  60417  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.158198616949214, \n",
      "\n",
      "                Current testing accuracy:  0.802 \n",
      "\n",
      "                Current testing loss:  0.6517930030822754 \n",
      " \n",
      "...completed  60545  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0364082113328124, \n",
      "\n",
      "                Current testing accuracy:  0.781 \n",
      "\n",
      "                Current testing loss:  0.8029415011405945 \n",
      " \n",
      "...completed  60673  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3997794236451977, \n",
      "\n",
      "                Current testing accuracy:  0.738 \n",
      "\n",
      "                Current testing loss:  0.8999881744384766 \n",
      " \n",
      "...completed  60801  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7709827522294779, \n",
      "\n",
      "                Current testing accuracy:  0.784 \n",
      "\n",
      "                Current testing loss:  0.8202998638153076 \n",
      " \n",
      "...completed  60929  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0297822409034723, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.6547696590423584 \n",
      " \n",
      "...completed  61057  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8082266801625009, \n",
      "\n",
      "                Current testing accuracy:  0.81 \n",
      "\n",
      "                Current testing loss:  0.6527553200721741 \n",
      " \n",
      "...completed  61185  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7730159782852295, \n",
      "\n",
      "                Current testing accuracy:  0.72 \n",
      "\n",
      "                Current testing loss:  1.05906081199646 \n",
      " \n",
      "...completed  61313  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1299019054876758, \n",
      "\n",
      "                Current testing accuracy:  0.72 \n",
      "\n",
      "                Current testing loss:  1.1139512062072754 \n",
      " \n",
      "...completed  61441  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6158050330118705, \n",
      "\n",
      "                Current testing accuracy:  0.685 \n",
      "\n",
      "                Current testing loss:  1.118011713027954 \n",
      " \n",
      "...completed  61569  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8249712134366041, \n",
      "\n",
      "                Current testing accuracy:  0.753 \n",
      "\n",
      "                Current testing loss:  0.9287630319595337 \n",
      " \n",
      "...completed  61697  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8058067824939137, \n",
      "\n",
      "                Current testing accuracy:  0.795 \n",
      "\n",
      "                Current testing loss:  0.7281274199485779 \n",
      " \n",
      "...completed  61825  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6966414304701356, \n",
      "\n",
      "                Current testing accuracy:  0.77 \n",
      "\n",
      "                Current testing loss:  0.8457027673721313 \n",
      " \n",
      "...completed  61953  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9691929745948027, \n",
      "\n",
      "                Current testing accuracy:  0.723 \n",
      "\n",
      "                Current testing loss:  0.9775000214576721 \n",
      " \n",
      "...completed  62081  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8570493242424959, \n",
      "\n",
      "                Current testing accuracy:  0.841 \n",
      "\n",
      "                Current testing loss:  0.5415975451469421 \n",
      " \n",
      "...completed  62209  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8344049800414082, \n",
      "\n",
      "                Current testing accuracy:  0.728 \n",
      "\n",
      "                Current testing loss:  0.9132315516471863 \n",
      " \n",
      "...completed  62337  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9804909377341744, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.9769561886787415 \n",
      " \n",
      "...completed  62465  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.125931736740533, \n",
      "\n",
      "                Current testing accuracy:  0.76 \n",
      "\n",
      "                Current testing loss:  0.8934475183486938 \n",
      " \n",
      "...completed  62593  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7704199603595638, \n",
      "\n",
      "                Current testing accuracy:  0.716 \n",
      "\n",
      "                Current testing loss:  1.0115914344787598 \n",
      " \n",
      "...completed  62721  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6869914571575038, \n",
      "\n",
      "                Current testing accuracy:  0.787 \n",
      "\n",
      "                Current testing loss:  0.7748335599899292 \n",
      " \n",
      "...completed  62849  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.111459026796803, \n",
      "\n",
      "                Current testing accuracy:  0.674 \n",
      "\n",
      "                Current testing loss:  1.0129233598709106 \n",
      " \n",
      "...completed  62977  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9622062771093818, \n",
      "\n",
      "                Current testing accuracy:  0.779 \n",
      "\n",
      "                Current testing loss:  0.7610952258110046 \n",
      " \n",
      "...completed  63105  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8955968722494845, \n",
      "\n",
      "                Current testing accuracy:  0.514 \n",
      "\n",
      "                Current testing loss:  1.8747512102127075 \n",
      " \n",
      "...completed  63233  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0025615300007829, \n",
      "\n",
      "                Current testing accuracy:  0.77 \n",
      "\n",
      "                Current testing loss:  0.8150576949119568 \n",
      " \n",
      "...completed  63361  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.083091389659467, \n",
      "\n",
      "                Current testing accuracy:  0.64 \n",
      "\n",
      "                Current testing loss:  1.1504600048065186 \n",
      " \n",
      "...completed  63489  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1002579541388968, \n",
      "\n",
      "                Current testing accuracy:  0.715 \n",
      "\n",
      "                Current testing loss:  1.072887659072876 \n",
      " \n",
      "...completed  63617  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9453505282736212, \n",
      "\n",
      "                Current testing accuracy:  0.774 \n",
      "\n",
      "                Current testing loss:  0.7560566663742065 \n",
      " \n",
      "...completed  63745  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0195729933419795, \n",
      "\n",
      "                Current testing accuracy:  0.747 \n",
      "\n",
      "                Current testing loss:  1.0189650058746338 \n",
      " \n",
      "...completed  63873  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0960150264350386, \n",
      "\n",
      "                Current testing accuracy:  0.7 \n",
      "\n",
      "                Current testing loss:  1.0810832977294922 \n",
      " \n",
      "...completed  64001  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0529779523879625, \n",
      "\n",
      "                Current testing accuracy:  0.673 \n",
      "\n",
      "                Current testing loss:  1.2545976638793945 \n",
      " \n",
      "...completed  64129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.014450892267643, \n",
      "\n",
      "                Current testing accuracy:  0.716 \n",
      "\n",
      "                Current testing loss:  1.0735963582992554 \n",
      " \n",
      "...completed  64257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9809521066077398, \n",
      "\n",
      "                Current testing accuracy:  0.791 \n",
      "\n",
      "                Current testing loss:  0.6834434866905212 \n",
      " \n",
      "...completed  64385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7503646754283722, \n",
      "\n",
      "                Current testing accuracy:  0.788 \n",
      "\n",
      "                Current testing loss:  0.681827187538147 \n",
      " \n",
      "...completed  64513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8336518891647806, \n",
      "\n",
      "                Current testing accuracy:  0.757 \n",
      "\n",
      "                Current testing loss:  0.8740782737731934 \n",
      " \n",
      "...completed  64641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7601060430992304, \n",
      "\n",
      "                Current testing accuracy:  0.741 \n",
      "\n",
      "                Current testing loss:  1.1417455673217773 \n",
      " \n",
      "...completed  64769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0849051392905729, \n",
      "\n",
      "                Current testing accuracy:  0.811 \n",
      "\n",
      "                Current testing loss:  0.6253092885017395 \n",
      " \n",
      "...completed  64897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6860675148854938, \n",
      "\n",
      "                Current testing accuracy:  0.834 \n",
      "\n",
      "                Current testing loss:  0.5702813863754272 \n",
      " \n",
      "...completed  65025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.173988335148088, \n",
      "\n",
      "                Current testing accuracy:  0.614 \n",
      "\n",
      "                Current testing loss:  1.5908753871917725 \n",
      " \n",
      "...completed  65153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3649759076925383, \n",
      "\n",
      "                Current testing accuracy:  0.87 \n",
      "\n",
      "                Current testing loss:  0.5046035051345825 \n",
      " \n",
      "...completed  65281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.813269956884362, \n",
      "\n",
      "                Current testing accuracy:  0.81 \n",
      "\n",
      "                Current testing loss:  0.6426299214363098 \n",
      " \n",
      "...completed  65409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8920300407638884, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.9363458156585693 \n",
      " \n",
      "...completed  65537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8851094386751175, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.7224831581115723 \n",
      " \n",
      "...completed  65665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5071584401167186, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.7960643172264099 \n",
      " \n",
      "...completed  65793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9852911729619647, \n",
      "\n",
      "                Current testing accuracy:  0.792 \n",
      "\n",
      "                Current testing loss:  0.7207394242286682 \n",
      " \n",
      "...completed  65921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0351959443578878, \n",
      "\n",
      "                Current testing accuracy:  0.789 \n",
      "\n",
      "                Current testing loss:  0.7274124622344971 \n",
      " \n",
      "...completed  66049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8983914095668268, \n",
      "\n",
      "                Current testing accuracy:  0.737 \n",
      "\n",
      "                Current testing loss:  0.907889723777771 \n",
      " \n",
      "...completed  66177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7477786374272313, \n",
      "\n",
      "                Current testing accuracy:  0.618 \n",
      "\n",
      "                Current testing loss:  1.2448889017105103 \n",
      " \n",
      "...completed  66305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.730215759163583, \n",
      "\n",
      "                Current testing accuracy:  0.641 \n",
      "\n",
      "                Current testing loss:  1.2221542596817017 \n",
      " \n",
      "...completed  66433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8790577437375822, \n",
      "\n",
      "                Current testing accuracy:  0.792 \n",
      "\n",
      "                Current testing loss:  0.8129646182060242 \n",
      " \n",
      "...completed  66561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7953003927822522, \n",
      "\n",
      "                Current testing accuracy:  0.629 \n",
      "\n",
      "                Current testing loss:  1.3474040031433105 \n",
      " \n",
      "...completed  66689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9209149526268994, \n",
      "\n",
      "                Current testing accuracy:  0.752 \n",
      "\n",
      "                Current testing loss:  0.8653932809829712 \n",
      " \n",
      "...completed  66817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.09142768255564, \n",
      "\n",
      "                Current testing accuracy:  0.779 \n",
      "\n",
      "                Current testing loss:  0.7820357084274292 \n",
      " \n",
      "...completed  66945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0691755722253475, \n",
      "\n",
      "                Current testing accuracy:  0.806 \n",
      "\n",
      "                Current testing loss:  0.6612570881843567 \n",
      " \n",
      "...completed  67073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2986202566244103, \n",
      "\n",
      "                Current testing accuracy:  0.714 \n",
      "\n",
      "                Current testing loss:  1.0587196350097656 \n",
      " \n",
      "...completed  67201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1313429602737628, \n",
      "\n",
      "                Current testing accuracy:  0.765 \n",
      "\n",
      "                Current testing loss:  0.8981034755706787 \n",
      " \n",
      "...completed  67329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.080317325389295, \n",
      "\n",
      "                Current testing accuracy:  0.827 \n",
      "\n",
      "                Current testing loss:  0.5755190253257751 \n",
      " \n",
      "...completed  67457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1643068016805813, \n",
      "\n",
      "                Current testing accuracy:  0.806 \n",
      "\n",
      "                Current testing loss:  0.6686309576034546 \n",
      " \n",
      "...completed  67585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9487256697081499, \n",
      "\n",
      "                Current testing accuracy:  0.698 \n",
      "\n",
      "                Current testing loss:  1.1355823278427124 \n",
      " \n",
      "...completed  67713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9311225220786206, \n",
      "\n",
      "                Current testing accuracy:  0.82 \n",
      "\n",
      "                Current testing loss:  0.6024040579795837 \n",
      " \n",
      "...completed  67841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8505877803654407, \n",
      "\n",
      "                Current testing accuracy:  0.721 \n",
      "\n",
      "                Current testing loss:  0.9918959140777588 \n",
      " \n",
      "...completed  67969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7023278115812195, \n",
      "\n",
      "                Current testing accuracy:  0.735 \n",
      "\n",
      "                Current testing loss:  0.9910269975662231 \n",
      " \n",
      "...completed  68097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7472798075104734, \n",
      "\n",
      "                Current testing accuracy:  0.801 \n",
      "\n",
      "                Current testing loss:  0.6784690022468567 \n",
      " \n",
      "...completed  68225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9001475828245873, \n",
      "\n",
      "                Current testing accuracy:  0.817 \n",
      "\n",
      "                Current testing loss:  0.6169494390487671 \n",
      " \n",
      "...completed  68353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0829770511156198, \n",
      "\n",
      "                Current testing accuracy:  0.846 \n",
      "\n",
      "                Current testing loss:  0.5358055233955383 \n",
      " \n",
      "...completed  68481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8080536527813464, \n",
      "\n",
      "                Current testing accuracy:  0.813 \n",
      "\n",
      "                Current testing loss:  0.6501474976539612 \n",
      " \n",
      "...completed  68609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.942001891376762, \n",
      "\n",
      "                Current testing accuracy:  0.681 \n",
      "\n",
      "                Current testing loss:  1.291391134262085 \n",
      " \n",
      "...completed  68737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1292514084188237, \n",
      "\n",
      "                Current testing accuracy:  0.795 \n",
      "\n",
      "                Current testing loss:  0.745720624923706 \n",
      " \n",
      "...completed  68865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6429334077440974, \n",
      "\n",
      "                Current testing accuracy:  0.783 \n",
      "\n",
      "                Current testing loss:  0.7570988535881042 \n",
      " \n",
      "...completed  68993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7091656611454038, \n",
      "\n",
      "                Current testing accuracy:  0.799 \n",
      "\n",
      "                Current testing loss:  0.7135258316993713 \n",
      " \n",
      "...completed  69121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8829098823822044, \n",
      "\n",
      "                Current testing accuracy:  0.806 \n",
      "\n",
      "                Current testing loss:  0.6448990106582642 \n",
      " \n",
      "...completed  69249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7865218390393949, \n",
      "\n",
      "                Current testing accuracy:  0.785 \n",
      "\n",
      "                Current testing loss:  0.7368694543838501 \n",
      " \n",
      "...completed  69377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2357786070245549, \n",
      "\n",
      "                Current testing accuracy:  0.757 \n",
      "\n",
      "                Current testing loss:  0.9851381182670593 \n",
      " \n",
      "...completed  69505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0111642575458326, \n",
      "\n",
      "                Current testing accuracy:  0.86 \n",
      "\n",
      "                Current testing loss:  0.5108258724212646 \n",
      " \n",
      "...completed  69633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9476745344343627, \n",
      "\n",
      "                Current testing accuracy:  0.825 \n",
      "\n",
      "                Current testing loss:  0.6107935309410095 \n",
      " \n",
      "...completed  69761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0771999294096481, \n",
      "\n",
      "                Current testing accuracy:  0.755 \n",
      "\n",
      "                Current testing loss:  0.8475995063781738 \n",
      " \n",
      "...completed  69889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8588633637968641, \n",
      "\n",
      "                Current testing accuracy:  0.829 \n",
      "\n",
      "                Current testing loss:  0.5346583127975464 \n",
      " \n",
      "...completed  70017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9210657916507277, \n",
      "\n",
      "                Current testing accuracy:  0.719 \n",
      "\n",
      "                Current testing loss:  0.9646368622779846 \n",
      " \n",
      "...completed  70145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8705751357850975, \n",
      "\n",
      "                Current testing accuracy:  0.76 \n",
      "\n",
      "                Current testing loss:  0.871725857257843 \n",
      " \n",
      "...completed  70273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.001702087815026, \n",
      "\n",
      "                Current testing accuracy:  0.765 \n",
      "\n",
      "                Current testing loss:  0.7974156737327576 \n",
      " \n",
      "...completed  70401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5936587345861337, \n",
      "\n",
      "                Current testing accuracy:  0.779 \n",
      "\n",
      "                Current testing loss:  0.7705150842666626 \n",
      " \n",
      "...completed  70529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7985010514264808, \n",
      "\n",
      "                Current testing accuracy:  0.778 \n",
      "\n",
      "                Current testing loss:  0.8434632420539856 \n",
      " \n",
      "...completed  70657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0056687975484238, \n",
      "\n",
      "                Current testing accuracy:  0.689 \n",
      "\n",
      "                Current testing loss:  1.0508228540420532 \n",
      " \n",
      "...completed  70785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7989888308177626, \n",
      "\n",
      "                Current testing accuracy:  0.772 \n",
      "\n",
      "                Current testing loss:  0.8308576345443726 \n",
      " \n",
      "...completed  70913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8528725306902238, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.901470422744751 \n",
      " \n",
      "...completed  71041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9374411613224858, \n",
      "\n",
      "                Current testing accuracy:  0.709 \n",
      "\n",
      "                Current testing loss:  0.9571473598480225 \n",
      " \n",
      "...completed  71169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8407762396775063, \n",
      "\n",
      "                Current testing accuracy:  0.82 \n",
      "\n",
      "                Current testing loss:  0.5904150605201721 \n",
      " \n",
      "...completed  71297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8349931945707052, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.9823869466781616 \n",
      " \n",
      "...completed  71425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0844618766009368, \n",
      "\n",
      "                Current testing accuracy:  0.554 \n",
      "\n",
      "                Current testing loss:  1.6457772254943848 \n",
      " \n",
      "...completed  71553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8872350491308434, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  0.9905014038085938 \n",
      " \n",
      "...completed  71681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2649600327704853, \n",
      "\n",
      "                Current testing accuracy:  0.755 \n",
      "\n",
      "                Current testing loss:  0.9689087867736816 \n",
      " \n",
      "...completed  71809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9124348906385267, \n",
      "\n",
      "                Current testing accuracy:  0.481 \n",
      "\n",
      "                Current testing loss:  2.229734182357788 \n",
      " \n",
      "...completed  71937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3014131063498127, \n",
      "\n",
      "                Current testing accuracy:  0.753 \n",
      "\n",
      "                Current testing loss:  0.830302357673645 \n",
      " \n",
      "...completed  72065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9144413437178569, \n",
      "\n",
      "                Current testing accuracy:  0.783 \n",
      "\n",
      "                Current testing loss:  0.7948765754699707 \n",
      " \n",
      "...completed  72193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1177682161663647, \n",
      "\n",
      "                Current testing accuracy:  0.829 \n",
      "\n",
      "                Current testing loss:  0.5857832431793213 \n",
      " \n",
      "...completed  72321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1137932947625087, \n",
      "\n",
      "                Current testing accuracy:  0.695 \n",
      "\n",
      "                Current testing loss:  1.2249044179916382 \n",
      " \n",
      "...completed  72449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2293192150834202, \n",
      "\n",
      "                Current testing accuracy:  0.73 \n",
      "\n",
      "                Current testing loss:  1.0877774953842163 \n",
      " \n",
      "...completed  72577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7853941541439724, \n",
      "\n",
      "                Current testing accuracy:  0.641 \n",
      "\n",
      "                Current testing loss:  1.2985786199569702 \n",
      " \n",
      "...completed  72705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1207967215872534, \n",
      "\n",
      "                Current testing accuracy:  0.739 \n",
      "\n",
      "                Current testing loss:  0.9524351954460144 \n",
      " \n",
      "...completed  72833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0075614340921915, \n",
      "\n",
      "                Current testing accuracy:  0.769 \n",
      "\n",
      "                Current testing loss:  0.7530921697616577 \n",
      " \n",
      "...completed  72961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1008426344606521, \n",
      "\n",
      "                Current testing accuracy:  0.715 \n",
      "\n",
      "                Current testing loss:  1.072062373161316 \n",
      " \n",
      "...completed  73089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8523837502808591, \n",
      "\n",
      "                Current testing accuracy:  0.685 \n",
      "\n",
      "                Current testing loss:  1.1694834232330322 \n",
      " \n",
      "...completed  73217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6754752438774076, \n",
      "\n",
      "                Current testing accuracy:  0.76 \n",
      "\n",
      "                Current testing loss:  0.8351196050643921 \n",
      " \n",
      "...completed  73345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9348101883899371, \n",
      "\n",
      "                Current testing accuracy:  0.815 \n",
      "\n",
      "                Current testing loss:  0.6249337196350098 \n",
      " \n",
      "...completed  73473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7034993751724166, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.7229191660881042 \n",
      " \n",
      "...completed  73601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8031326643731322, \n",
      "\n",
      "                Current testing accuracy:  0.753 \n",
      "\n",
      "                Current testing loss:  0.8727226853370667 \n",
      " \n",
      "...completed  73729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8585205107993694, \n",
      "\n",
      "                Current testing accuracy:  0.724 \n",
      "\n",
      "                Current testing loss:  0.9640925526618958 \n",
      " \n",
      "...completed  73857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7969623031849928, \n",
      "\n",
      "                Current testing accuracy:  0.764 \n",
      "\n",
      "                Current testing loss:  0.7720213532447815 \n",
      " \n",
      "...completed  73985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.763045195882496, \n",
      "\n",
      "                Current testing accuracy:  0.789 \n",
      "\n",
      "                Current testing loss:  0.7276800274848938 \n",
      " \n",
      "...completed  74113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9307748372020512, \n",
      "\n",
      "                Current testing accuracy:  0.847 \n",
      "\n",
      "                Current testing loss:  0.528460681438446 \n",
      " \n",
      "...completed  74241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1755634513929145, \n",
      "\n",
      "                Current testing accuracy:  0.784 \n",
      "\n",
      "                Current testing loss:  0.7167219519615173 \n",
      " \n",
      "...completed  74369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7903550790511922, \n",
      "\n",
      "                Current testing accuracy:  0.714 \n",
      "\n",
      "                Current testing loss:  0.9453917145729065 \n",
      " \n",
      "...completed  74497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7952780285438621, \n",
      "\n",
      "                Current testing accuracy:  0.762 \n",
      "\n",
      "                Current testing loss:  0.8271337151527405 \n",
      " \n",
      "...completed  74625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.018999896952053, \n",
      "\n",
      "                Current testing accuracy:  0.83 \n",
      "\n",
      "                Current testing loss:  0.5933466553688049 \n",
      " \n",
      "...completed  74753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9588200634980026, \n",
      "\n",
      "                Current testing accuracy:  0.745 \n",
      "\n",
      "                Current testing loss:  0.8702905178070068 \n",
      " \n",
      "...completed  74881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9143158746681896, \n",
      "\n",
      "                Current testing accuracy:  0.697 \n",
      "\n",
      "                Current testing loss:  1.1671606302261353 \n",
      " \n",
      "...completed  75009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9157483956740009, \n",
      "\n",
      "                Current testing accuracy:  0.768 \n",
      "\n",
      "                Current testing loss:  0.7478788495063782 \n",
      " \n",
      "...completed  75137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8534305051087099, \n",
      "\n",
      "                Current testing accuracy:  0.839 \n",
      "\n",
      "                Current testing loss:  0.539107620716095 \n",
      " \n",
      "...completed  75265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1107539924538763, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.864585816860199 \n",
      " \n",
      "...completed  75393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1276798426097994, \n",
      "\n",
      "                Current testing accuracy:  0.734 \n",
      "\n",
      "                Current testing loss:  0.9341070055961609 \n",
      " \n",
      "...completed  75521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0145212008958637, \n",
      "\n",
      "                Current testing accuracy:  0.757 \n",
      "\n",
      "                Current testing loss:  0.8538548350334167 \n",
      " \n",
      "...completed  75649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7226230856808895, \n",
      "\n",
      "                Current testing accuracy:  0.831 \n",
      "\n",
      "                Current testing loss:  0.5858534574508667 \n",
      " \n",
      "...completed  75777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8537964600386552, \n",
      "\n",
      "                Current testing accuracy:  0.747 \n",
      "\n",
      "                Current testing loss:  0.8940877914428711 \n",
      " \n",
      "...completed  75905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7396304814559747, \n",
      "\n",
      "                Current testing accuracy:  0.724 \n",
      "\n",
      "                Current testing loss:  0.9017438888549805 \n",
      " \n",
      "...completed  76033  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0533924091265776, \n",
      "\n",
      "                Current testing accuracy:  0.724 \n",
      "\n",
      "                Current testing loss:  1.1297820806503296 \n",
      " \n",
      "...completed  76161  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8122406395400317, \n",
      "\n",
      "                Current testing accuracy:  0.787 \n",
      "\n",
      "                Current testing loss:  0.7138978242874146 \n",
      " \n",
      "...completed  76289  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8537099507004449, \n",
      "\n",
      "                Current testing accuracy:  0.816 \n",
      "\n",
      "                Current testing loss:  0.6696931719779968 \n",
      " \n",
      "...completed  76417  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8303307872585708, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.8213326334953308 \n",
      " \n",
      "...completed  76545  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6433974606531185, \n",
      "\n",
      "                Current testing accuracy:  0.793 \n",
      "\n",
      "                Current testing loss:  0.7410157322883606 \n",
      " \n",
      "...completed  76673  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9657872008473625, \n",
      "\n",
      "                Current testing accuracy:  0.622 \n",
      "\n",
      "                Current testing loss:  1.4339078664779663 \n",
      " \n",
      "...completed  76801  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8238868433345079, \n",
      "\n",
      "                Current testing accuracy:  0.789 \n",
      "\n",
      "                Current testing loss:  0.8111767172813416 \n",
      " \n",
      "...completed  76929  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3320163951330808, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.7262373566627502 \n",
      " \n",
      "...completed  77057  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1459228033692739, \n",
      "\n",
      "                Current testing accuracy:  0.794 \n",
      "\n",
      "                Current testing loss:  0.742674708366394 \n",
      " \n",
      "...completed  77185  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2306345496664333, \n",
      "\n",
      "                Current testing accuracy:  0.758 \n",
      "\n",
      "                Current testing loss:  0.8100016117095947 \n",
      " \n",
      "...completed  77313  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9647294995792226, \n",
      "\n",
      "                Current testing accuracy:  0.768 \n",
      "\n",
      "                Current testing loss:  0.7776243090629578 \n",
      " \n",
      "...completed  77441  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.757964061107856, \n",
      "\n",
      "                Current testing accuracy:  0.747 \n",
      "\n",
      "                Current testing loss:  0.8887828588485718 \n",
      " \n",
      "...completed  77569  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9038585533246364, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.6685259938240051 \n",
      " \n",
      "...completed  77697  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7541937691520069, \n",
      "\n",
      "                Current testing accuracy:  0.745 \n",
      "\n",
      "                Current testing loss:  0.8727393746376038 \n",
      " \n",
      "...completed  77825  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8785014724214524, \n",
      "\n",
      "                Current testing accuracy:  0.744 \n",
      "\n",
      "                Current testing loss:  0.9189335703849792 \n",
      " \n",
      "...completed  77953  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7783538653698476, \n",
      "\n",
      "                Current testing accuracy:  0.673 \n",
      "\n",
      "                Current testing loss:  1.2223923206329346 \n",
      " \n",
      "...completed  78081  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0138965894364969, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.7705640196800232 \n",
      " \n",
      "...completed  78209  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.894069515024337, \n",
      "\n",
      "                Current testing accuracy:  0.581 \n",
      "\n",
      "                Current testing loss:  1.7861038446426392 \n",
      " \n",
      "...completed  78337  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0636188097600021, \n",
      "\n",
      "                Current testing accuracy:  0.76 \n",
      "\n",
      "                Current testing loss:  0.8984381556510925 \n",
      " \n",
      "...completed  78465  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0316642640827234, \n",
      "\n",
      "                Current testing accuracy:  0.748 \n",
      "\n",
      "                Current testing loss:  0.8891847729682922 \n",
      " \n",
      "...completed  78593  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1742998905628497, \n",
      "\n",
      "                Current testing accuracy:  0.853 \n",
      "\n",
      "                Current testing loss:  0.527514636516571 \n",
      " \n",
      "...completed  78721  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9018691664591487, \n",
      "\n",
      "                Current testing accuracy:  0.81 \n",
      "\n",
      "                Current testing loss:  0.6544946432113647 \n",
      " \n",
      "...completed  78849  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1193166552985083, \n",
      "\n",
      "                Current testing accuracy:  0.674 \n",
      "\n",
      "                Current testing loss:  1.2827401161193848 \n",
      " \n",
      "...completed  78977  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9348562554239317, \n",
      "\n",
      "                Current testing accuracy:  0.713 \n",
      "\n",
      "                Current testing loss:  0.9611141085624695 \n",
      " \n",
      "...completed  79105  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6143211136753752, \n",
      "\n",
      "                Current testing accuracy:  0.836 \n",
      "\n",
      "                Current testing loss:  0.571865439414978 \n",
      " \n",
      "...completed  79233  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9568649942327285, \n",
      "\n",
      "                Current testing accuracy:  0.81 \n",
      "\n",
      "                Current testing loss:  0.634117066860199 \n",
      " \n",
      "...completed  79361  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8083942954841845, \n",
      "\n",
      "                Current testing accuracy:  0.795 \n",
      "\n",
      "                Current testing loss:  0.7362484931945801 \n",
      " \n",
      "...completed  79489  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7816844230580138, \n",
      "\n",
      "                Current testing accuracy:  0.812 \n",
      "\n",
      "                Current testing loss:  0.6674983501434326 \n",
      " \n",
      "...completed  79617  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7208676650326176, \n",
      "\n",
      "                Current testing accuracy:  0.809 \n",
      "\n",
      "                Current testing loss:  0.6429346203804016 \n",
      " \n",
      "...completed  79745  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7359322330410176, \n",
      "\n",
      "                Current testing accuracy:  0.809 \n",
      "\n",
      "                Current testing loss:  0.6879523396492004 \n",
      " \n",
      "...completed  79873  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0862329110471585, \n",
      "\n",
      "                Current testing accuracy:  0.807 \n",
      "\n",
      "                Current testing loss:  0.8748161196708679 \n",
      " \n",
      "...completed  80001  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7842158350249497, \n",
      "\n",
      "                Current testing accuracy:  0.679 \n",
      "\n",
      "                Current testing loss:  1.2661001682281494 \n",
      " \n",
      "...completed  80129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9347714470307125, \n",
      "\n",
      "                Current testing accuracy:  0.828 \n",
      "\n",
      "                Current testing loss:  0.6204622983932495 \n",
      " \n",
      "...completed  80257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8576576416507735, \n",
      "\n",
      "                Current testing accuracy:  0.811 \n",
      "\n",
      "                Current testing loss:  0.6937097311019897 \n",
      " \n",
      "...completed  80385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6670910146506637, \n",
      "\n",
      "                Current testing accuracy:  0.784 \n",
      "\n",
      "                Current testing loss:  0.7661820650100708 \n",
      " \n",
      "...completed  80513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9710264848630743, \n",
      "\n",
      "                Current testing accuracy:  0.692 \n",
      "\n",
      "                Current testing loss:  1.0610045194625854 \n",
      " \n",
      "...completed  80641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.010835883249644, \n",
      "\n",
      "                Current testing accuracy:  0.756 \n",
      "\n",
      "                Current testing loss:  0.9508906006813049 \n",
      " \n",
      "...completed  80769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.032844439185574, \n",
      "\n",
      "                Current testing accuracy:  0.835 \n",
      "\n",
      "                Current testing loss:  0.5757347345352173 \n",
      " \n",
      "...completed  80897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.84540964072478, \n",
      "\n",
      "                Current testing accuracy:  0.748 \n",
      "\n",
      "                Current testing loss:  0.9385790228843689 \n",
      " \n",
      "...completed  81025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9314643895960639, \n",
      "\n",
      "                Current testing accuracy:  0.85 \n",
      "\n",
      "                Current testing loss:  0.5550858378410339 \n",
      " \n",
      "...completed  81153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6859289181181865, \n",
      "\n",
      "                Current testing accuracy:  0.77 \n",
      "\n",
      "                Current testing loss:  0.8597941994667053 \n",
      " \n",
      "...completed  81281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7303138582434086, \n",
      "\n",
      "                Current testing accuracy:  0.783 \n",
      "\n",
      "                Current testing loss:  0.734775185585022 \n",
      " \n",
      "...completed  81409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0400144771310416, \n",
      "\n",
      "                Current testing accuracy:  0.864 \n",
      "\n",
      "                Current testing loss:  0.49503880739212036 \n",
      " \n",
      "...completed  81537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8311150668707352, \n",
      "\n",
      "                Current testing accuracy:  0.823 \n",
      "\n",
      "                Current testing loss:  0.6483028531074524 \n",
      " \n",
      "...completed  81665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7626842535001799, \n",
      "\n",
      "                Current testing accuracy:  0.732 \n",
      "\n",
      "                Current testing loss:  0.9744799733161926 \n",
      " \n",
      "...completed  81793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.20800989832934, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.7875213623046875 \n",
      " \n",
      "...completed  81921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7177088308965693, \n",
      "\n",
      "                Current testing accuracy:  0.748 \n",
      "\n",
      "                Current testing loss:  0.9306982159614563 \n",
      " \n",
      "...completed  82049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7879564974556619, \n",
      "\n",
      "                Current testing accuracy:  0.778 \n",
      "\n",
      "                Current testing loss:  0.7384822964668274 \n",
      " \n",
      "...completed  82177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.812347909547652, \n",
      "\n",
      "                Current testing accuracy:  0.737 \n",
      "\n",
      "                Current testing loss:  0.9280111193656921 \n",
      " \n",
      "...completed  82305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9269895502219683, \n",
      "\n",
      "                Current testing accuracy:  0.822 \n",
      "\n",
      "                Current testing loss:  0.6180033087730408 \n",
      " \n",
      "...completed  82433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7390866810843875, \n",
      "\n",
      "                Current testing accuracy:  0.727 \n",
      "\n",
      "                Current testing loss:  1.1917661428451538 \n",
      " \n",
      "...completed  82561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7770546377466374, \n",
      "\n",
      "                Current testing accuracy:  0.857 \n",
      "\n",
      "                Current testing loss:  0.5204423666000366 \n",
      " \n",
      "...completed  82689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8085966287337527, \n",
      "\n",
      "                Current testing accuracy:  0.823 \n",
      "\n",
      "                Current testing loss:  0.6394118666648865 \n",
      " \n",
      "...completed  82817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8375973935595029, \n",
      "\n",
      "                Current testing accuracy:  0.801 \n",
      "\n",
      "                Current testing loss:  0.6887353658676147 \n",
      " \n",
      "...completed  82945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8402781593525024, \n",
      "\n",
      "                Current testing accuracy:  0.771 \n",
      "\n",
      "                Current testing loss:  0.8194502592086792 \n",
      " \n",
      "...completed  83073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8849420913726185, \n",
      "\n",
      "                Current testing accuracy:  0.833 \n",
      "\n",
      "                Current testing loss:  0.5772004127502441 \n",
      " \n",
      "...completed  83201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.686814721811138, \n",
      "\n",
      "                Current testing accuracy:  0.703 \n",
      "\n",
      "                Current testing loss:  1.0710116624832153 \n",
      " \n",
      "...completed  83329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.746695734248707, \n",
      "\n",
      "                Current testing accuracy:  0.616 \n",
      "\n",
      "                Current testing loss:  1.4801868200302124 \n",
      " \n",
      "...completed  83457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7795363238053632, \n",
      "\n",
      "                Current testing accuracy:  0.754 \n",
      "\n",
      "                Current testing loss:  0.92681485414505 \n",
      " \n",
      "...completed  83585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8380952637310042, \n",
      "\n",
      "                Current testing accuracy:  0.755 \n",
      "\n",
      "                Current testing loss:  0.9791271686553955 \n",
      " \n",
      "...completed  83713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9942814880805457, \n",
      "\n",
      "                Current testing accuracy:  0.637 \n",
      "\n",
      "                Current testing loss:  1.4025657176971436 \n",
      " \n",
      "...completed  83841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8245827882786951, \n",
      "\n",
      "                Current testing accuracy:  0.789 \n",
      "\n",
      "                Current testing loss:  0.7035980820655823 \n",
      " \n",
      "...completed  83969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.774425584899701, \n",
      "\n",
      "                Current testing accuracy:  0.794 \n",
      "\n",
      "                Current testing loss:  0.7477392554283142 \n",
      " \n",
      "...completed  84097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6517564266829723, \n",
      "\n",
      "                Current testing accuracy:  0.784 \n",
      "\n",
      "                Current testing loss:  0.787837564945221 \n",
      " \n",
      "...completed  84225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8857003274236699, \n",
      "\n",
      "                Current testing accuracy:  0.728 \n",
      "\n",
      "                Current testing loss:  1.0101619958877563 \n",
      " \n",
      "...completed  84353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9490667965968522, \n",
      "\n",
      "                Current testing accuracy:  0.731 \n",
      "\n",
      "                Current testing loss:  0.9169742465019226 \n",
      " \n",
      "...completed  84481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8823668271641072, \n",
      "\n",
      "                Current testing accuracy:  0.74 \n",
      "\n",
      "                Current testing loss:  0.88286954164505 \n",
      " \n",
      "...completed  84609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0092281888029948, \n",
      "\n",
      "                Current testing accuracy:  0.725 \n",
      "\n",
      "                Current testing loss:  1.134232997894287 \n",
      " \n",
      "...completed  84737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2277669373714684, \n",
      "\n",
      "                Current testing accuracy:  0.708 \n",
      "\n",
      "                Current testing loss:  1.3236743211746216 \n",
      " \n",
      "...completed  84865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0368914366431472, \n",
      "\n",
      "                Current testing accuracy:  0.709 \n",
      "\n",
      "                Current testing loss:  0.9570392370223999 \n",
      " \n",
      "...completed  84993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7419836064594536, \n",
      "\n",
      "                Current testing accuracy:  0.809 \n",
      "\n",
      "                Current testing loss:  0.6854341626167297 \n",
      " \n",
      "...completed  85121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0801915223982093, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.9328420758247375 \n",
      " \n",
      "...completed  85249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9395878617302147, \n",
      "\n",
      "                Current testing accuracy:  0.785 \n",
      "\n",
      "                Current testing loss:  0.7521151304244995 \n",
      " \n",
      "...completed  85377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.727779348731044, \n",
      "\n",
      "                Current testing accuracy:  0.744 \n",
      "\n",
      "                Current testing loss:  0.9010125398635864 \n",
      " \n",
      "...completed  85505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7404333455284071, \n",
      "\n",
      "                Current testing accuracy:  0.741 \n",
      "\n",
      "                Current testing loss:  0.8969497680664062 \n",
      " \n",
      "...completed  85633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5630332115107408, \n",
      "\n",
      "                Current testing accuracy:  0.827 \n",
      "\n",
      "                Current testing loss:  0.5948296189308167 \n",
      " \n",
      "...completed  85761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1864198783668891, \n",
      "\n",
      "                Current testing accuracy:  0.652 \n",
      "\n",
      "                Current testing loss:  1.319413661956787 \n",
      " \n",
      "...completed  85889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9040375476354665, \n",
      "\n",
      "                Current testing accuracy:  0.762 \n",
      "\n",
      "                Current testing loss:  0.9556131958961487 \n",
      " \n",
      "...completed  86017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0353152355010522, \n",
      "\n",
      "                Current testing accuracy:  0.812 \n",
      "\n",
      "                Current testing loss:  0.6662687659263611 \n",
      " \n",
      "...completed  86145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.802138381613952, \n",
      "\n",
      "                Current testing accuracy:  0.821 \n",
      "\n",
      "                Current testing loss:  0.5824490785598755 \n",
      " \n",
      "...completed  86273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6507336885760573, \n",
      "\n",
      "                Current testing accuracy:  0.728 \n",
      "\n",
      "                Current testing loss:  0.9499982595443726 \n",
      " \n",
      "...completed  86401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5690883184452389, \n",
      "\n",
      "                Current testing accuracy:  0.84 \n",
      "\n",
      "                Current testing loss:  0.5859072804450989 \n",
      " \n",
      "...completed  86529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7833970048566457, \n",
      "\n",
      "                Current testing accuracy:  0.783 \n",
      "\n",
      "                Current testing loss:  0.8012308478355408 \n",
      " \n",
      "...completed  86657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6134820100689877, \n",
      "\n",
      "                Current testing accuracy:  0.711 \n",
      "\n",
      "                Current testing loss:  1.0862654447555542 \n",
      " \n",
      "...completed  86785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7904215898482918, \n",
      "\n",
      "                Current testing accuracy:  0.808 \n",
      "\n",
      "                Current testing loss:  0.6858255863189697 \n",
      " \n",
      "...completed  86913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7424312714837265, \n",
      "\n",
      "                Current testing accuracy:  0.762 \n",
      "\n",
      "                Current testing loss:  0.8573140501976013 \n",
      " \n",
      "...completed  87041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0224923907687753, \n",
      "\n",
      "                Current testing accuracy:  0.842 \n",
      "\n",
      "                Current testing loss:  0.5496655106544495 \n",
      " \n",
      "...completed  87169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1215130489962806, \n",
      "\n",
      "                Current testing accuracy:  0.689 \n",
      "\n",
      "                Current testing loss:  1.1909677982330322 \n",
      " \n",
      "...completed  87297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9568567568735844, \n",
      "\n",
      "                Current testing accuracy:  0.784 \n",
      "\n",
      "                Current testing loss:  0.8733054399490356 \n",
      " \n",
      "...completed  87425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9026286303141102, \n",
      "\n",
      "                Current testing accuracy:  0.712 \n",
      "\n",
      "                Current testing loss:  1.1612061262130737 \n",
      " \n",
      "...completed  87553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7114898676086356, \n",
      "\n",
      "                Current testing accuracy:  0.829 \n",
      "\n",
      "                Current testing loss:  0.6241054534912109 \n",
      " \n",
      "...completed  87681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5319087922391983, \n",
      "\n",
      "                Current testing accuracy:  0.808 \n",
      "\n",
      "                Current testing loss:  0.6416363716125488 \n",
      " \n",
      "...completed  87809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0782473126869405, \n",
      "\n",
      "                Current testing accuracy:  0.709 \n",
      "\n",
      "                Current testing loss:  1.1001126766204834 \n",
      " \n",
      "...completed  87937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9894813080611549, \n",
      "\n",
      "                Current testing accuracy:  0.863 \n",
      "\n",
      "                Current testing loss:  0.4984540045261383 \n",
      " \n",
      "...completed  88065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9749151949468575, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  1.0652384757995605 \n",
      " \n",
      "...completed  88193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9821774749900527, \n",
      "\n",
      "                Current testing accuracy:  0.71 \n",
      "\n",
      "                Current testing loss:  1.116505742073059 \n",
      " \n",
      "...completed  88321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6294509475489107, \n",
      "\n",
      "                Current testing accuracy:  0.858 \n",
      "\n",
      "                Current testing loss:  0.5462164282798767 \n",
      " \n",
      "...completed  88449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8913766825121243, \n",
      "\n",
      "                Current testing accuracy:  0.826 \n",
      "\n",
      "                Current testing loss:  0.6369022727012634 \n",
      " \n",
      "...completed  88577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.844233464196676, \n",
      "\n",
      "                Current testing accuracy:  0.834 \n",
      "\n",
      "                Current testing loss:  0.6113761067390442 \n",
      " \n",
      "...completed  88705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8629449685415924, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.713947594165802 \n",
      " \n",
      "...completed  88833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8878185057902002, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.9004521369934082 \n",
      " \n",
      "...completed  88961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8143700548419304, \n",
      "\n",
      "                Current testing accuracy:  0.79 \n",
      "\n",
      "                Current testing loss:  0.7442865967750549 \n",
      " \n",
      "...completed  89089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6921856125554786, \n",
      "\n",
      "                Current testing accuracy:  0.829 \n",
      "\n",
      "                Current testing loss:  0.5878925919532776 \n",
      " \n",
      "...completed  89217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.709618893078634, \n",
      "\n",
      "                Current testing accuracy:  0.847 \n",
      "\n",
      "                Current testing loss:  0.5525490045547485 \n",
      " \n",
      "...completed  89345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.957568992912829, \n",
      "\n",
      "                Current testing accuracy:  0.731 \n",
      "\n",
      "                Current testing loss:  0.9024259448051453 \n",
      " \n",
      "...completed  89473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8539697575294731, \n",
      "\n",
      "                Current testing accuracy:  0.736 \n",
      "\n",
      "                Current testing loss:  0.9735482931137085 \n",
      " \n",
      "...completed  89601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0465890326014602, \n",
      "\n",
      "                Current testing accuracy:  0.796 \n",
      "\n",
      "                Current testing loss:  0.8325133323669434 \n",
      " \n",
      "...completed  89729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7774940335700213, \n",
      "\n",
      "                Current testing accuracy:  0.865 \n",
      "\n",
      "                Current testing loss:  0.5014492869377136 \n",
      " \n",
      "...completed  89857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7971823931470468, \n",
      "\n",
      "                Current testing accuracy:  0.829 \n",
      "\n",
      "                Current testing loss:  0.6138694882392883 \n",
      " \n",
      "...completed  89985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8290402796809317, \n",
      "\n",
      "                Current testing accuracy:  0.8 \n",
      "\n",
      "                Current testing loss:  0.7072893381118774 \n",
      " \n",
      "...completed  90113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9997380315939854, \n",
      "\n",
      "                Current testing accuracy:  0.784 \n",
      "\n",
      "                Current testing loss:  0.7790924310684204 \n",
      " \n",
      "...completed  90241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6864435712009573, \n",
      "\n",
      "                Current testing accuracy:  0.702 \n",
      "\n",
      "                Current testing loss:  1.2826496362686157 \n",
      " \n",
      "...completed  90369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6523896994010272, \n",
      "\n",
      "                Current testing accuracy:  0.787 \n",
      "\n",
      "                Current testing loss:  0.7359509468078613 \n",
      " \n",
      "...completed  90497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2194290376766224, \n",
      "\n",
      "                Current testing accuracy:  0.822 \n",
      "\n",
      "                Current testing loss:  0.6285455226898193 \n",
      " \n",
      "...completed  90625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9019262148475384, \n",
      "\n",
      "                Current testing accuracy:  0.799 \n",
      "\n",
      "                Current testing loss:  0.7266650795936584 \n",
      " \n",
      "...completed  90753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8613200749524008, \n",
      "\n",
      "                Current testing accuracy:  0.82 \n",
      "\n",
      "                Current testing loss:  0.6900481581687927 \n",
      " \n",
      "...completed  90881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9802308782558793, \n",
      "\n",
      "                Current testing accuracy:  0.823 \n",
      "\n",
      "                Current testing loss:  0.5985664129257202 \n",
      " \n",
      "...completed  91009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0521302410280242, \n",
      "\n",
      "                Current testing accuracy:  0.761 \n",
      "\n",
      "                Current testing loss:  0.8837382197380066 \n",
      " \n",
      "...completed  91137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0123975782767012, \n",
      "\n",
      "                Current testing accuracy:  0.737 \n",
      "\n",
      "                Current testing loss:  1.2801166772842407 \n",
      " \n",
      "...completed  91265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.744504010520675, \n",
      "\n",
      "                Current testing accuracy:  0.796 \n",
      "\n",
      "                Current testing loss:  0.6532282829284668 \n",
      " \n",
      "...completed  91393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6936996094802543, \n",
      "\n",
      "                Current testing accuracy:  0.835 \n",
      "\n",
      "                Current testing loss:  0.5378590226173401 \n",
      " \n",
      "...completed  91521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0502620667435998, \n",
      "\n",
      "                Current testing accuracy:  0.732 \n",
      "\n",
      "                Current testing loss:  0.9317607283592224 \n",
      " \n",
      "...completed  91649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6553864066536361, \n",
      "\n",
      "                Current testing accuracy:  0.779 \n",
      "\n",
      "                Current testing loss:  0.7626562714576721 \n",
      " \n",
      "...completed  91777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6904204309734199, \n",
      "\n",
      "                Current testing accuracy:  0.788 \n",
      "\n",
      "                Current testing loss:  0.8320889472961426 \n",
      " \n",
      "...completed  91905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8087213640377158, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.7349851131439209 \n",
      " \n",
      "...completed  92033  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8348670710527486, \n",
      "\n",
      "                Current testing accuracy:  0.843 \n",
      "\n",
      "                Current testing loss:  0.5523156523704529 \n",
      " \n",
      "...completed  92161  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7688627508453703, \n",
      "\n",
      "                Current testing accuracy:  0.783 \n",
      "\n",
      "                Current testing loss:  0.8406018614768982 \n",
      " \n",
      "...completed  92289  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7167241513484823, \n",
      "\n",
      "                Current testing accuracy:  0.759 \n",
      "\n",
      "                Current testing loss:  0.926277220249176 \n",
      " \n",
      "...completed  92417  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.717940838908163, \n",
      "\n",
      "                Current testing accuracy:  0.717 \n",
      "\n",
      "                Current testing loss:  1.0181734561920166 \n",
      " \n",
      "...completed  92545  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7671623845710727, \n",
      "\n",
      "                Current testing accuracy:  0.713 \n",
      "\n",
      "                Current testing loss:  1.1293591260910034 \n",
      " \n",
      "...completed  92673  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9383527040350828, \n",
      "\n",
      "                Current testing accuracy:  0.788 \n",
      "\n",
      "                Current testing loss:  0.7443831562995911 \n",
      " \n",
      "...completed  92801  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7490717587842433, \n",
      "\n",
      "                Current testing accuracy:  0.791 \n",
      "\n",
      "                Current testing loss:  0.7302169799804688 \n",
      " \n",
      "...completed  92929  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0012528865381771, \n",
      "\n",
      "                Current testing accuracy:  0.769 \n",
      "\n",
      "                Current testing loss:  0.8037063479423523 \n",
      " \n",
      "...completed  93057  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5057582837824195, \n",
      "\n",
      "                Current testing accuracy:  0.805 \n",
      "\n",
      "                Current testing loss:  0.6868056058883667 \n",
      " \n",
      "...completed  93185  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0470324107289244, \n",
      "\n",
      "                Current testing accuracy:  0.744 \n",
      "\n",
      "                Current testing loss:  0.8938314318656921 \n",
      " \n",
      "...completed  93313  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3399795556752003, \n",
      "\n",
      "                Current testing accuracy:  0.839 \n",
      "\n",
      "                Current testing loss:  0.5389965176582336 \n",
      " \n",
      "...completed  93441  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.537323237486651, \n",
      "\n",
      "                Current testing accuracy:  0.67 \n",
      "\n",
      "                Current testing loss:  1.2564878463745117 \n",
      " \n",
      "...completed  93569  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6668355124393175, \n",
      "\n",
      "                Current testing accuracy:  0.822 \n",
      "\n",
      "                Current testing loss:  0.5972696542739868 \n",
      " \n",
      "...completed  93697  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.289415424659957, \n",
      "\n",
      "                Current testing accuracy:  0.8 \n",
      "\n",
      "                Current testing loss:  0.7630860209465027 \n",
      " \n",
      "...completed  93825  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1269348752345998, \n",
      "\n",
      "                Current testing accuracy:  0.764 \n",
      "\n",
      "                Current testing loss:  0.9466816186904907 \n",
      " \n",
      "...completed  93953  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7019954095789012, \n",
      "\n",
      "                Current testing accuracy:  0.841 \n",
      "\n",
      "                Current testing loss:  0.5666266083717346 \n",
      " \n",
      "...completed  94081  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.08600247305624, \n",
      "\n",
      "                Current testing accuracy:  0.746 \n",
      "\n",
      "                Current testing loss:  0.9569931030273438 \n",
      " \n",
      "...completed  94209  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9563550984759885, \n",
      "\n",
      "                Current testing accuracy:  0.834 \n",
      "\n",
      "                Current testing loss:  0.5901427865028381 \n",
      " \n",
      "...completed  94337  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5349582492117761, \n",
      "\n",
      "                Current testing accuracy:  0.755 \n",
      "\n",
      "                Current testing loss:  0.8349343538284302 \n",
      " \n",
      "...completed  94465  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8766619563036586, \n",
      "\n",
      "                Current testing accuracy:  0.808 \n",
      "\n",
      "                Current testing loss:  0.654521107673645 \n",
      " \n",
      "...completed  94593  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8036688678267749, \n",
      "\n",
      "                Current testing accuracy:  0.744 \n",
      "\n",
      "                Current testing loss:  1.002732515335083 \n",
      " \n",
      "...completed  94721  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.823784484726998, \n",
      "\n",
      "                Current testing accuracy:  0.866 \n",
      "\n",
      "                Current testing loss:  0.5015421509742737 \n",
      " \n",
      "...completed  94849  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9731731406314355, \n",
      "\n",
      "                Current testing accuracy:  0.819 \n",
      "\n",
      "                Current testing loss:  0.6543247103691101 \n",
      " \n",
      "...completed  94977  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9227742447485454, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  0.9477071762084961 \n",
      " \n",
      "...completed  95105  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8209523564947387, \n",
      "\n",
      "                Current testing accuracy:  0.721 \n",
      "\n",
      "                Current testing loss:  1.0403072834014893 \n",
      " \n",
      "...completed  95233  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6920323140468518, \n",
      "\n",
      "                Current testing accuracy:  0.78 \n",
      "\n",
      "                Current testing loss:  0.8033467531204224 \n",
      " \n",
      "...completed  95361  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8759142159684943, \n",
      "\n",
      "                Current testing accuracy:  0.777 \n",
      "\n",
      "                Current testing loss:  0.8269397616386414 \n",
      " \n",
      "...completed  95489  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8647635740062185, \n",
      "\n",
      "                Current testing accuracy:  0.852 \n",
      "\n",
      "                Current testing loss:  0.5321516394615173 \n",
      " \n",
      "...completed  95617  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6989862563867675, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  0.8766629695892334 \n",
      " \n",
      "...completed  95745  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1537929041265915, \n",
      "\n",
      "                Current testing accuracy:  0.84 \n",
      "\n",
      "                Current testing loss:  0.5917333960533142 \n",
      " \n",
      "...completed  95873  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7548071468782735, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  1.0928186178207397 \n",
      " \n",
      "...completed  96001  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2498494663788584, \n",
      "\n",
      "                Current testing accuracy:  0.8 \n",
      "\n",
      "                Current testing loss:  0.6815310716629028 \n",
      " \n",
      "...completed  96129  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7354100719980661, \n",
      "\n",
      "                Current testing accuracy:  0.854 \n",
      "\n",
      "                Current testing loss:  0.5352759957313538 \n",
      " \n",
      "...completed  96257  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8494755918757844, \n",
      "\n",
      "                Current testing accuracy:  0.815 \n",
      "\n",
      "                Current testing loss:  0.755562961101532 \n",
      " \n",
      "...completed  96385  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8480337656530144, \n",
      "\n",
      "                Current testing accuracy:  0.785 \n",
      "\n",
      "                Current testing loss:  0.8212080597877502 \n",
      " \n",
      "...completed  96513  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5309495889853224, \n",
      "\n",
      "                Current testing accuracy:  0.833 \n",
      "\n",
      "                Current testing loss:  0.5977705121040344 \n",
      " \n",
      "...completed  96641  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0710673460612092, \n",
      "\n",
      "                Current testing accuracy:  0.799 \n",
      "\n",
      "                Current testing loss:  0.7088912725448608 \n",
      " \n",
      "...completed  96769  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7715380886491373, \n",
      "\n",
      "                Current testing accuracy:  0.698 \n",
      "\n",
      "                Current testing loss:  1.1027685403823853 \n",
      " \n",
      "...completed  96897  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8417646684864799, \n",
      "\n",
      "                Current testing accuracy:  0.851 \n",
      "\n",
      "                Current testing loss:  0.518566906452179 \n",
      " \n",
      "...completed  97025  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1553584897176084, \n",
      "\n",
      "                Current testing accuracy:  0.712 \n",
      "\n",
      "                Current testing loss:  1.1075881719589233 \n",
      " \n",
      "...completed  97153  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9755138326723554, \n",
      "\n",
      "                Current testing accuracy:  0.804 \n",
      "\n",
      "                Current testing loss:  0.6997723579406738 \n",
      " \n",
      "...completed  97281  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8606614610791752, \n",
      "\n",
      "                Current testing accuracy:  0.743 \n",
      "\n",
      "                Current testing loss:  1.0009735822677612 \n",
      " \n",
      "...completed  97409  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9185889974636394, \n",
      "\n",
      "                Current testing accuracy:  0.839 \n",
      "\n",
      "                Current testing loss:  0.5986316204071045 \n",
      " \n",
      "...completed  97537  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5381492424853969, \n",
      "\n",
      "                Current testing accuracy:  0.808 \n",
      "\n",
      "                Current testing loss:  0.6886195540428162 \n",
      " \n",
      "...completed  97665  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.928557077719482, \n",
      "\n",
      "                Current testing accuracy:  0.778 \n",
      "\n",
      "                Current testing loss:  0.7777791023254395 \n",
      " \n",
      "...completed  97793  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7759419730404709, \n",
      "\n",
      "                Current testing accuracy:  0.742 \n",
      "\n",
      "                Current testing loss:  1.0033262968063354 \n",
      " \n",
      "...completed  97921  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1120432931201671, \n",
      "\n",
      "                Current testing accuracy:  0.836 \n",
      "\n",
      "                Current testing loss:  0.573331892490387 \n",
      " \n",
      "...completed  98049  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7537586832681975, \n",
      "\n",
      "                Current testing accuracy:  0.767 \n",
      "\n",
      "                Current testing loss:  0.8613120913505554 \n",
      " \n",
      "...completed  98177  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7629212313023133, \n",
      "\n",
      "                Current testing accuracy:  0.769 \n",
      "\n",
      "                Current testing loss:  0.8698651790618896 \n",
      " \n",
      "...completed  98305  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5161461876399487, \n",
      "\n",
      "                Current testing accuracy:  0.84 \n",
      "\n",
      "                Current testing loss:  0.6073365211486816 \n",
      " \n",
      "...completed  98433  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8272642451488676, \n",
      "\n",
      "                Current testing accuracy:  0.668 \n",
      "\n",
      "                Current testing loss:  1.4168747663497925 \n",
      " \n",
      "...completed  98561  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6457138563814624, \n",
      "\n",
      "                Current testing accuracy:  0.681 \n",
      "\n",
      "                Current testing loss:  1.3663043975830078 \n",
      " \n",
      "...completed  98689  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1541563214148851, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.8466248512268066 \n",
      " \n",
      "...completed  98817  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9503556677637808, \n",
      "\n",
      "                Current testing accuracy:  0.837 \n",
      "\n",
      "                Current testing loss:  0.5843237042427063 \n",
      " \n",
      "...completed  98945  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8272274638569995, \n",
      "\n",
      "                Current testing accuracy:  0.833 \n",
      "\n",
      "                Current testing loss:  0.6481008529663086 \n",
      " \n",
      "...completed  99073  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8043988632654866, \n",
      "\n",
      "                Current testing accuracy:  0.768 \n",
      "\n",
      "                Current testing loss:  0.9140938520431519 \n",
      " \n",
      "...completed  99201  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7798911812316511, \n",
      "\n",
      "                Current testing accuracy:  0.797 \n",
      "\n",
      "                Current testing loss:  0.8005274534225464 \n",
      " \n",
      "...completed  99329  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9207094784035306, \n",
      "\n",
      "                Current testing accuracy:  0.823 \n",
      "\n",
      "                Current testing loss:  0.6172895431518555 \n",
      " \n",
      "...completed  99457  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8512383486656594, \n",
      "\n",
      "                Current testing accuracy:  0.817 \n",
      "\n",
      "                Current testing loss:  0.6363763213157654 \n",
      " \n",
      "...completed  99585  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6408971230031246, \n",
      "\n",
      "                Current testing accuracy:  0.729 \n",
      "\n",
      "                Current testing loss:  1.0450260639190674 \n",
      " \n",
      "...completed  99713  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7386697773187176, \n",
      "\n",
      "                Current testing accuracy:  0.822 \n",
      "\n",
      "                Current testing loss:  0.6036171317100525 \n",
      " \n",
      "...completed  99841  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7703039258470774, \n",
      "\n",
      "                Current testing accuracy:  0.817 \n",
      "\n",
      "                Current testing loss:  0.6282764077186584 \n",
      " \n",
      "...completed  99969  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0110290235118242, \n",
      "\n",
      "                Current testing accuracy:  0.737 \n",
      "\n",
      "                Current testing loss:  1.0864999294281006 \n",
      " \n",
      "...completed  100097  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7991505661608027, \n",
      "\n",
      "                Current testing accuracy:  0.833 \n",
      "\n",
      "                Current testing loss:  0.644790768623352 \n",
      " \n",
      "...completed  100225  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7125883996511178, \n",
      "\n",
      "                Current testing accuracy:  0.707 \n",
      "\n",
      "                Current testing loss:  1.1211810111999512 \n",
      " \n",
      "...completed  100353  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8055025277451073, \n",
      "\n",
      "                Current testing accuracy:  0.765 \n",
      "\n",
      "                Current testing loss:  0.8410617113113403 \n",
      " \n",
      "...completed  100481  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6374955157379381, \n",
      "\n",
      "                Current testing accuracy:  0.845 \n",
      "\n",
      "                Current testing loss:  0.5685759782791138 \n",
      " \n",
      "...completed  100609  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7852967373283288, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.840076208114624 \n",
      " \n",
      "...completed  100737  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1415032202930355, \n",
      "\n",
      "                Current testing accuracy:  0.757 \n",
      "\n",
      "                Current testing loss:  0.9034379124641418 \n",
      " \n",
      "...completed  100865  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.880197062299878, \n",
      "\n",
      "                Current testing accuracy:  0.665 \n",
      "\n",
      "                Current testing loss:  1.400475025177002 \n",
      " \n",
      "...completed  100993  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0008812156504374, \n",
      "\n",
      "                Current testing accuracy:  0.846 \n",
      "\n",
      "                Current testing loss:  0.5530051589012146 \n",
      " \n",
      "...completed  101121  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7261038796404318, \n",
      "\n",
      "                Current testing accuracy:  0.843 \n",
      "\n",
      "                Current testing loss:  0.5558565855026245 \n",
      " \n",
      "...completed  101249  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.2044080046367585, \n",
      "\n",
      "                Current testing accuracy:  0.78 \n",
      "\n",
      "                Current testing loss:  0.8261319398880005 \n",
      " \n",
      "...completed  101377  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7987979619594627, \n",
      "\n",
      "                Current testing accuracy:  0.822 \n",
      "\n",
      "                Current testing loss:  0.5879433751106262 \n",
      " \n",
      "...completed  101505  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6640395218442863, \n",
      "\n",
      "                Current testing accuracy:  0.833 \n",
      "\n",
      "                Current testing loss:  0.6324344873428345 \n",
      " \n",
      "...completed  101633  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7330619427657954, \n",
      "\n",
      "                Current testing accuracy:  0.83 \n",
      "\n",
      "                Current testing loss:  0.5731832385063171 \n",
      " \n",
      "...completed  101761  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8406053599983707, \n",
      "\n",
      "                Current testing accuracy:  0.732 \n",
      "\n",
      "                Current testing loss:  1.0763051509857178 \n",
      " \n",
      "...completed  101889  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8664016861204125, \n",
      "\n",
      "                Current testing accuracy:  0.817 \n",
      "\n",
      "                Current testing loss:  0.7044426202774048 \n",
      " \n",
      "...completed  102017  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8599049984909966, \n",
      "\n",
      "                Current testing accuracy:  0.81 \n",
      "\n",
      "                Current testing loss:  0.6898457407951355 \n",
      " \n",
      "...completed  102145  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6250779139821692, \n",
      "\n",
      "                Current testing accuracy:  0.748 \n",
      "\n",
      "                Current testing loss:  0.9330876469612122 \n",
      " \n",
      "...completed  102273  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8158156150926175, \n",
      "\n",
      "                Current testing accuracy:  0.73 \n",
      "\n",
      "                Current testing loss:  1.0427076816558838 \n",
      " \n",
      "...completed  102401  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7110486126798232, \n",
      "\n",
      "                Current testing accuracy:  0.844 \n",
      "\n",
      "                Current testing loss:  0.5761827826499939 \n",
      " \n",
      "...completed  102529  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7376647371742402, \n",
      "\n",
      "                Current testing accuracy:  0.828 \n",
      "\n",
      "                Current testing loss:  0.6510889530181885 \n",
      " \n",
      "...completed  102657  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7236722234771875, \n",
      "\n",
      "                Current testing accuracy:  0.83 \n",
      "\n",
      "                Current testing loss:  0.5976999998092651 \n",
      " \n",
      "...completed  102785  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8601093913462545, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.8377633094787598 \n",
      " \n",
      "...completed  102913  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7278098425833193, \n",
      "\n",
      "                Current testing accuracy:  0.766 \n",
      "\n",
      "                Current testing loss:  0.9529487490653992 \n",
      " \n",
      "...completed  103041  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7451609755805173, \n",
      "\n",
      "                Current testing accuracy:  0.848 \n",
      "\n",
      "                Current testing loss:  0.5331307649612427 \n",
      " \n",
      "...completed  103169  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6354546777978114, \n",
      "\n",
      "                Current testing accuracy:  0.836 \n",
      "\n",
      "                Current testing loss:  0.6245230436325073 \n",
      " \n",
      "...completed  103297  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.896918874640656, \n",
      "\n",
      "                Current testing accuracy:  0.701 \n",
      "\n",
      "                Current testing loss:  1.1278254985809326 \n",
      " \n",
      "...completed  103425  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.214163853195363, \n",
      "\n",
      "                Current testing accuracy:  0.679 \n",
      "\n",
      "                Current testing loss:  0.990287721157074 \n",
      " \n",
      "...completed  103553  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6980100231279125, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.6883306503295898 \n",
      " \n",
      "...completed  103681  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7740784444712645, \n",
      "\n",
      "                Current testing accuracy:  0.734 \n",
      "\n",
      "                Current testing loss:  1.0296626091003418 \n",
      " \n",
      "...completed  103809  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0387461023937643, \n",
      "\n",
      "                Current testing accuracy:  0.775 \n",
      "\n",
      "                Current testing loss:  0.8707095980644226 \n",
      " \n",
      "...completed  103937  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8520022934171525, \n",
      "\n",
      "                Current testing accuracy:  0.793 \n",
      "\n",
      "                Current testing loss:  0.7691105008125305 \n",
      " \n",
      "...completed  104065  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9762626822137381, \n",
      "\n",
      "                Current testing accuracy:  0.748 \n",
      "\n",
      "                Current testing loss:  0.9577921032905579 \n",
      " \n",
      "...completed  104193  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6577135173646451, \n",
      "\n",
      "                Current testing accuracy:  0.745 \n",
      "\n",
      "                Current testing loss:  0.8924260139465332 \n",
      " \n",
      "...completed  104321  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.878415753350831, \n",
      "\n",
      "                Current testing accuracy:  0.804 \n",
      "\n",
      "                Current testing loss:  0.7507867217063904 \n",
      " \n",
      "...completed  104449  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8098103225728615, \n",
      "\n",
      "                Current testing accuracy:  0.826 \n",
      "\n",
      "                Current testing loss:  0.5830265879631042 \n",
      " \n",
      "...completed  104577  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6479618666416829, \n",
      "\n",
      "                Current testing accuracy:  0.755 \n",
      "\n",
      "                Current testing loss:  0.8443983197212219 \n",
      " \n",
      "...completed  104705  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8776083706884534, \n",
      "\n",
      "                Current testing accuracy:  0.722 \n",
      "\n",
      "                Current testing loss:  0.9442381262779236 \n",
      " \n",
      "...completed  104833  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.5922287989766204, \n",
      "\n",
      "                Current testing accuracy:  0.673 \n",
      "\n",
      "                Current testing loss:  1.1283156871795654 \n",
      " \n",
      "...completed  104961  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8067008286577861, \n",
      "\n",
      "                Current testing accuracy:  0.79 \n",
      "\n",
      "                Current testing loss:  0.7352458238601685 \n",
      " \n",
      "...completed  105089  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0513451086846821, \n",
      "\n",
      "                Current testing accuracy:  0.82 \n",
      "\n",
      "                Current testing loss:  0.6419162750244141 \n",
      " \n",
      "...completed  105217  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7167704416315814, \n",
      "\n",
      "                Current testing accuracy:  0.772 \n",
      "\n",
      "                Current testing loss:  0.840684711933136 \n",
      " \n",
      "...completed  105345  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8632784547751642, \n",
      "\n",
      "                Current testing accuracy:  0.741 \n",
      "\n",
      "                Current testing loss:  0.9302315711975098 \n",
      " \n",
      "...completed  105473  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8711006260848637, \n",
      "\n",
      "                Current testing accuracy:  0.812 \n",
      "\n",
      "                Current testing loss:  0.6834733486175537 \n",
      " \n",
      "...completed  105601  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9379400115239542, \n",
      "\n",
      "                Current testing accuracy:  0.764 \n",
      "\n",
      "                Current testing loss:  0.8443506956100464 \n",
      " \n",
      "...completed  105729  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1472578280365637, \n",
      "\n",
      "                Current testing accuracy:  0.7 \n",
      "\n",
      "                Current testing loss:  1.3685002326965332 \n",
      " \n",
      "...completed  105857  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8107630639999357, \n",
      "\n",
      "                Current testing accuracy:  0.72 \n",
      "\n",
      "                Current testing loss:  1.164734125137329 \n",
      " \n",
      "...completed  105985  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8680575711265328, \n",
      "\n",
      "                Current testing accuracy:  0.791 \n",
      "\n",
      "                Current testing loss:  0.7212981581687927 \n",
      " \n",
      "...completed  106113  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9613359141302595, \n",
      "\n",
      "                Current testing accuracy:  0.789 \n",
      "\n",
      "                Current testing loss:  0.8420328497886658 \n",
      " \n",
      "...completed  106241  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8799797725828036, \n",
      "\n",
      "                Current testing accuracy:  0.786 \n",
      "\n",
      "                Current testing loss:  0.8523220419883728 \n",
      " \n",
      "...completed  106369  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0052714271764847, \n",
      "\n",
      "                Current testing accuracy:  0.776 \n",
      "\n",
      "                Current testing loss:  0.8003868460655212 \n",
      " \n",
      "...completed  106497  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.1373855988661834, \n",
      "\n",
      "                Current testing accuracy:  0.802 \n",
      "\n",
      "                Current testing loss:  0.6760338544845581 \n",
      " \n",
      "...completed  106625  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.3296235633674742, \n",
      "\n",
      "                Current testing accuracy:  0.802 \n",
      "\n",
      "                Current testing loss:  0.7074371576309204 \n",
      " \n",
      "...completed  106753  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8830447024229808, \n",
      "\n",
      "                Current testing accuracy:  0.778 \n",
      "\n",
      "                Current testing loss:  0.8092398047447205 \n",
      " \n",
      "...completed  106881  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7611300895915818, \n",
      "\n",
      "                Current testing accuracy:  0.824 \n",
      "\n",
      "                Current testing loss:  0.6180089116096497 \n",
      " \n",
      "...completed  107009  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7515347566931996, \n",
      "\n",
      "                Current testing accuracy:  0.803 \n",
      "\n",
      "                Current testing loss:  0.7164696455001831 \n",
      " \n",
      "...completed  107137  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.7682884682536422, \n",
      "\n",
      "                Current testing accuracy:  0.658 \n",
      "\n",
      "                Current testing loss:  1.7896870374679565 \n",
      " \n",
      "...completed  107265  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.9336781004190442, \n",
      "\n",
      "                Current testing accuracy:  0.841 \n",
      "\n",
      "                Current testing loss:  0.5904838442802429 \n",
      " \n",
      "...completed  107393  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.6940826540357348, \n",
      "\n",
      "                Current testing accuracy:  0.806 \n",
      "\n",
      "                Current testing loss:  0.8417122960090637 \n",
      " \n",
      "...completed  107521  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  1.0733066243027944, \n",
      "\n",
      "                Current testing accuracy:  0.7 \n",
      "\n",
      "                Current testing loss:  1.0828579664230347 \n",
      " \n",
      "...completed  107649  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.8688241976600417, \n",
      "\n",
      "                Current testing accuracy:  0.79 \n",
      "\n",
      "                Current testing loss:  0.7020519375801086 \n",
      " \n",
      "...completed  107777  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.735490909220637, \n",
      "\n",
      "                Current testing accuracy:  0.827 \n",
      "\n",
      "                Current testing loss:  0.6127563714981079 \n",
      " \n",
      "...completed  107905  iterations of training. \n",
      "\n",
      "                Current training cross_entropy loss:  0.45164653984762104, \n",
      "\n",
      "                Current testing accuracy:  0.778 \n",
      "\n",
      "                Current testing loss:  0.8099610805511475 \n",
      " \n",
      "...Converge at 107905 iterations of training.\n"
     ]
    }
   ],
   "source": [
    "# Normal Learning\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "netffa_online = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_online_ce, test_loss_ffa_online_ce, \n",
    " losses_ffa_online_mse, test_loss_ffa_online_mse,\n",
    " accuracy_ffa_online) = netffa_online.train_online(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    max_it=150000, conv_loss=0.50, \n",
    "    report_rate=batchsize, lr=0.01,\n",
    "    model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844 844 844 844 844\n"
     ]
    }
   ],
   "source": [
    "print(len(losses_ffa_online_ce), \n",
    "      len(test_loss_ffa_online_ce),\n",
    "      len(losses_ffa_online_mse),\n",
    "      len(test_loss_ffa_online_mse),\n",
    "      len(accuracy_ffa_online)\n",
    "      )\n",
    "np.save('results/netffa/losses_ffa_online_ce.npy', np.asarray(losses_ffa_online_ce))\n",
    "np.save('results/netffa/test_loss_ffa_online_ce.npy', np.asarray(test_loss_ffa_online_ce))\n",
    "np.save('results/netffa/losses_ffa_online_mse.npy', np.asarray(losses_ffa_online_mse))\n",
    "np.save('results/netffa/test_loss_ffa_online_mse.npy', np.asarray(test_loss_ffa_online_mse))\n",
    "np.save('results/netffa/accuracy_ffa_online.npy', np.asarray(accuracy_ffa_online))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cpu\n",
      "input:\n",
      "  path: datasets\n",
      "  batch_size: 128\n",
      "model:\n",
      "  peer_normalization: 0.03\n",
      "  momentum: 0.9\n",
      "  hidden_dim: 500\n",
      "  num_layers: 2\n",
      "training:\n",
      "  epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0\n",
      "  momentum: 0\n",
      "  downstream_learning_rate: 0.001\n",
      "  downstream_weight_decay: 0\n",
      "  val_idx: 0\n",
      "  final_test: true\n",
      "\n",
      "FF_model(\n",
      "  (model): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      "  (ff_loss): BCEWithLogitsLoss()\n",
      "  (linear_classifier): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=10, bias=False)\n",
      "  )\n",
      "  (classification_loss): CrossEntropyLoss()\n",
      ") \n",
      "\n",
      "...completed  0  epochs of training. \n",
      "\n",
      "            Current innate loss:  54.250599748599605, \n",
      "\n",
      "            Current training cross_entropy loss:  1.1063182309547566, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.121723175048828, \n",
      "   \n",
      "            Current testing accuracy:  0.384 \n",
      "         \n",
      "            \n",
      "...completed  1  epochs of training. \n",
      "\n",
      "            Current innate loss:  38.19058132467803, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7683097876562095, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.2204933166503906, \n",
      "   \n",
      "            Current testing accuracy:  0.396 \n",
      "         \n",
      "            \n",
      "...completed  2  epochs of training. \n",
      "\n",
      "            Current innate loss:  29.883563592567207, \n",
      "\n",
      "            Current training cross_entropy loss:  0.605814536073193, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.4040873050689697, \n",
      "   \n",
      "            Current testing accuracy:  0.397 \n",
      "         \n",
      "            \n",
      "...completed  3  epochs of training. \n",
      "\n",
      "            Current innate loss:  24.439759585057725, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5103917722561344, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.4496915340423584, \n",
      "   \n",
      "            Current testing accuracy:  0.398 \n",
      "         \n",
      "            \n",
      "...completed  4  epochs of training. \n",
      "\n",
      "            Current innate loss:  20.75711267038902, \n",
      "\n",
      "            Current training cross_entropy loss:  0.44739979212143405, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.5698604583740234, \n",
      "   \n",
      "            Current testing accuracy:  0.398 \n",
      "         \n",
      "            \n",
      "...completed  5  epochs of training. \n",
      "\n",
      "            Current innate loss:  18.07232374107122, \n",
      "\n",
      "            Current training cross_entropy loss:  0.40209265108033243, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.6686747074127197, \n",
      "   \n",
      "            Current testing accuracy:  0.4 \n",
      "         \n",
      "            \n",
      "...completed  6  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.919218144008514, \n",
      "\n",
      "            Current training cross_entropy loss:  0.36797631704500533, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.7232718467712402, \n",
      "   \n",
      "            Current testing accuracy:  0.4 \n",
      "         \n",
      "            \n",
      "...completed  7  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.29416673584199, \n",
      "\n",
      "            Current training cross_entropy loss:  0.341077593785728, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.792142629623413, \n",
      "   \n",
      "            Current testing accuracy:  0.402 \n",
      "         \n",
      "            \n",
      "...completed  8  epochs of training. \n",
      "\n",
      "            Current innate loss:  12.909620828880286, \n",
      "\n",
      "            Current training cross_entropy loss:  0.3194835749605839, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.840909719467163, \n",
      "   \n",
      "            Current testing accuracy:  0.402 \n",
      "         \n",
      "            \n",
      "...completed  9  epochs of training. \n",
      "\n",
      "            Current innate loss:  11.765612033324212, \n",
      "\n",
      "            Current training cross_entropy loss:  0.3016494975558349, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.8214688301086426, \n",
      "   \n",
      "            Current testing accuracy:  0.402 \n",
      "         \n",
      "            \n",
      "...completed  10  epochs of training. \n",
      "\n",
      "            Current innate loss:  10.931047843059263, \n",
      "\n",
      "            Current training cross_entropy loss:  0.28626595335717525, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.928504467010498, \n",
      "   \n",
      "            Current testing accuracy:  0.403 \n",
      "         \n",
      "            \n",
      "...completed  11  epochs of training. \n",
      "\n",
      "            Current innate loss:  10.129487243452058, \n",
      "\n",
      "            Current training cross_entropy loss:  0.2732500908028635, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.9024829864501953, \n",
      "   \n",
      "            Current testing accuracy:  0.403 \n",
      "         \n",
      "            \n",
      "...completed  12  epochs of training. \n",
      "\n",
      "            Current innate loss:  9.51345326384459, \n",
      "\n",
      "            Current training cross_entropy loss:  0.2616977377127673, \n",
      "\n",
      "            Current testing cross_entropy loss:  3.9906492233276367, \n",
      "   \n",
      "            Current testing accuracy:  0.403 \n",
      "         \n",
      "            \n",
      "...completed  13  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.09211480946093, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4153754736451652, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.632002830505371, \n",
      "   \n",
      "            Current testing accuracy:  0.553 \n",
      "         \n",
      "            \n",
      "...completed  14  epochs of training. \n",
      "\n",
      "            Current innate loss:  16.61105221956209, \n",
      "\n",
      "            Current training cross_entropy loss:  0.44871279127787767, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.6583468914031982, \n",
      "   \n",
      "            Current testing accuracy:  0.506 \n",
      "         \n",
      "            \n",
      "...completed  15  epochs of training. \n",
      "\n",
      "            Current innate loss:  16.54435052091269, \n",
      "\n",
      "            Current training cross_entropy loss:  0.45464996512336053, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.7938874959945679, \n",
      "   \n",
      "            Current testing accuracy:  0.517 \n",
      "         \n",
      "            \n",
      "...completed  16  epochs of training. \n",
      "\n",
      "            Current innate loss:  16.110868415708914, \n",
      "\n",
      "            Current training cross_entropy loss:  0.45286825285520654, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.8353869915008545, \n",
      "   \n",
      "            Current testing accuracy:  0.519 \n",
      "         \n",
      "            \n",
      "...completed  17  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.500798697527578, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4480556417325623, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.9023358821868896, \n",
      "   \n",
      "            Current testing accuracy:  0.519 \n",
      "         \n",
      "            \n",
      "...completed  18  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.898352728819757, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4418423967556837, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.9444282054901123, \n",
      "   \n",
      "            Current testing accuracy:  0.529 \n",
      "         \n",
      "            \n",
      "...completed  19  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.336558305726939, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4348940533390309, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.9761000871658325, \n",
      "   \n",
      "            Current testing accuracy:  0.532 \n",
      "         \n",
      "            \n",
      "...completed  20  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.75663834751032, \n",
      "\n",
      "            Current training cross_entropy loss:  0.42799715860729004, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.999545693397522, \n",
      "   \n",
      "            Current testing accuracy:  0.537 \n",
      "         \n",
      "            \n",
      "...completed  21  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.19223086002875, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4210480215878988, \n",
      "\n",
      "            Current testing cross_entropy loss:  2.0323550701141357, \n",
      "   \n",
      "            Current testing accuracy:  0.538 \n",
      "         \n",
      "            \n",
      "...completed  22  epochs of training. \n",
      "\n",
      "            Current innate loss:  12.68994063608129, \n",
      "\n",
      "            Current training cross_entropy loss:  0.41423835832402095, \n",
      "\n",
      "            Current testing cross_entropy loss:  2.06111478805542, \n",
      "   \n",
      "            Current testing accuracy:  0.537 \n",
      "         \n",
      "            \n",
      "...completed  23  epochs of training. \n",
      "\n",
      "            Current innate loss:  12.210954088378449, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4075793293890352, \n",
      "\n",
      "            Current testing cross_entropy loss:  2.1114015579223633, \n",
      "   \n",
      "            Current testing accuracy:  0.536 \n",
      "         \n",
      "            \n",
      "...completed  24  epochs of training. \n",
      "\n",
      "            Current innate loss:  11.76981039461925, \n",
      "\n",
      "            Current training cross_entropy loss:  0.40105334778855567, \n",
      "\n",
      "            Current testing cross_entropy loss:  2.120497226715088, \n",
      "   \n",
      "            Current testing accuracy:  0.538 \n",
      "         \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "net_ffa_nonstat = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_nonstat_ce, test_loss_ffa_nonstat_ce, \n",
    " losses_ffa_nonstat_mse, test_loss_ffa_nonstat_mse,\n",
    " accuracy_ffa_nonstat) = net_ffa_nonstat.train_nonstationary(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4841 25 4841 25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(losses_ffa_nonstat_ce), \n",
    "      len(test_loss_ffa_nonstat_ce),\n",
    "      len(losses_ffa_nonstat_mse),\n",
    "      len(test_loss_ffa_nonstat_mse),\n",
    "      len(accuracy_ffa_nonstat)\n",
    "      )\n",
    "np.save('results/netffa/losses_ffa_nonstat_ce.npy', np.asarray(losses_ffa_nonstat_ce))\n",
    "np.save('results/netffa/test_loss_ffa_nonstat_ce.npy', np.asarray(test_loss_ffa_nonstat_ce))\n",
    "np.save('results/netffa/losses_ffa_nonstat_mse.npy', np.asarray(losses_ffa_nonstat_mse))\n",
    "np.save('results/netffa/test_loss_ffa_nonstat_mse.npy', np.asarray(test_loss_ffa_nonstat_mse))\n",
    "np.save('results/netffa/accuracy_ffa_nonstat.npy', np.asarray(accuracy_ffa_nonstat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cpu\n",
      "input:\n",
      "  path: datasets\n",
      "  batch_size: 128\n",
      "model:\n",
      "  peer_normalization: 0.03\n",
      "  momentum: 0.9\n",
      "  hidden_dim: 500\n",
      "  num_layers: 2\n",
      "training:\n",
      "  epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0\n",
      "  momentum: 0\n",
      "  downstream_learning_rate: 0.001\n",
      "  downstream_weight_decay: 0\n",
      "  val_idx: 0\n",
      "  final_test: true\n",
      "\n",
      "FF_model(\n",
      "  (model): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      "  (ff_loss): BCEWithLogitsLoss()\n",
      "  (linear_classifier): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=10, bias=False)\n",
      "  )\n",
      "  (classification_loss): CrossEntropyLoss()\n",
      ") \n",
      "\n",
      "...completed  0  epochs of training. \n",
      "\n",
      "            Current innate loss:  47.83585045154278, \n",
      "\n",
      "            Current training cross_entropy loss:  2.082044208049774, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.601974368095398, \n",
      "   \n",
      "            Current testing accuracy:  0.706 \n",
      "         \n",
      "            \n",
      "...completed  1  epochs of training. \n",
      "\n",
      "            Current innate loss:  43.026067212911755, \n",
      "\n",
      "            Current training cross_entropy loss:  1.8250050297150244, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.0120466947555542, \n",
      "   \n",
      "            Current testing accuracy:  0.756 \n",
      "         \n",
      "            \n",
      "...completed  2  epochs of training. \n",
      "\n",
      "            Current innate loss:  38.39896525684585, \n",
      "\n",
      "            Current training cross_entropy loss:  1.5996860942270001, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.7651218771934509, \n",
      "   \n",
      "            Current testing accuracy:  0.793 \n",
      "         \n",
      "            \n",
      "...completed  3  epochs of training. \n",
      "\n",
      "            Current innate loss:  35.197245308069085, \n",
      "\n",
      "            Current training cross_entropy loss:  1.431257771834349, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.6279633045196533, \n",
      "   \n",
      "            Current testing accuracy:  0.826 \n",
      "         \n",
      "            \n",
      "...completed  4  epochs of training. \n",
      "\n",
      "            Current innate loss:  32.331414269667405, \n",
      "\n",
      "            Current training cross_entropy loss:  1.3045746279068482, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.5567597150802612, \n",
      "   \n",
      "            Current testing accuracy:  0.841 \n",
      "         \n",
      "            \n",
      "...completed  5  epochs of training. \n",
      "\n",
      "            Current innate loss:  29.826282320674668, \n",
      "\n",
      "            Current training cross_entropy loss:  1.2072035091809736, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.5015517473220825, \n",
      "   \n",
      "            Current testing accuracy:  0.854 \n",
      "         \n",
      "            \n",
      "...completed  6  epochs of training. \n",
      "\n",
      "            Current innate loss:  27.675512602041056, \n",
      "\n",
      "            Current training cross_entropy loss:  1.1298233476641415, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.4660837650299072, \n",
      "   \n",
      "            Current testing accuracy:  0.863 \n",
      "         \n",
      "            \n",
      "...completed  7  epochs of training. \n",
      "\n",
      "            Current innate loss:  25.893254466393056, \n",
      "\n",
      "            Current training cross_entropy loss:  1.0667121291447144, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.44494205713272095, \n",
      "   \n",
      "            Current testing accuracy:  0.871 \n",
      "         \n",
      "            \n",
      "...completed  8  epochs of training. \n",
      "\n",
      "            Current innate loss:  24.34041057733389, \n",
      "\n",
      "            Current training cross_entropy loss:  1.0141427609485778, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.42658939957618713, \n",
      "   \n",
      "            Current testing accuracy:  0.873 \n",
      "         \n",
      "            \n",
      "...completed  9  epochs of training. \n",
      "\n",
      "            Current innate loss:  22.985449744126736, \n",
      "\n",
      "            Current training cross_entropy loss:  0.9696412145556548, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.4110877215862274, \n",
      "   \n",
      "            Current testing accuracy:  0.876 \n",
      "         \n",
      "            \n",
      "...completed  10  epochs of training. \n",
      "\n",
      "            Current innate loss:  21.78733462698254, \n",
      "\n",
      "            Current training cross_entropy loss:  0.9315475093248563, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3993780314922333, \n",
      "   \n",
      "            Current testing accuracy:  0.879 \n",
      "         \n",
      "            \n",
      "...completed  11  epochs of training. \n",
      "\n",
      "            Current innate loss:  20.722088516981174, \n",
      "\n",
      "            Current training cross_entropy loss:  0.8983522961728084, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.38786250352859497, \n",
      "   \n",
      "            Current testing accuracy:  0.882 \n",
      "         \n",
      "            \n",
      "...completed  12  epochs of training. \n",
      "\n",
      "            Current innate loss:  19.78800420413121, \n",
      "\n",
      "            Current training cross_entropy loss:  0.8691781898515935, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3804551959037781, \n",
      "   \n",
      "            Current testing accuracy:  0.885 \n",
      "         \n",
      "            \n",
      "...completed  13  epochs of training. \n",
      "\n",
      "            Current innate loss:  18.943191408368694, \n",
      "\n",
      "            Current training cross_entropy loss:  0.8432661240845373, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3748491406440735, \n",
      "   \n",
      "            Current testing accuracy:  0.889 \n",
      "         \n",
      "            \n",
      "...completed  14  epochs of training. \n",
      "\n",
      "            Current innate loss:  18.16662462625748, \n",
      "\n",
      "            Current training cross_entropy loss:  0.820084464718134, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3714127838611603, \n",
      "   \n",
      "            Current testing accuracy:  0.888 \n",
      "         \n",
      "            \n",
      "...completed  15  epochs of training. \n",
      "\n",
      "            Current innate loss:  17.464312970867525, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7992526859045028, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3643338978290558, \n",
      "   \n",
      "            Current testing accuracy:  0.887 \n",
      "         \n",
      "            \n",
      "...completed  16  epochs of training. \n",
      "\n",
      "            Current innate loss:  16.81666714260481, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7803800278434566, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3630030155181885, \n",
      "   \n",
      "            Current testing accuracy:  0.89 \n",
      "         \n",
      "            \n",
      "...completed  17  epochs of training. \n",
      "\n",
      "            Current innate loss:  16.213235322600415, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7632058865409291, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.35846978425979614, \n",
      "   \n",
      "            Current testing accuracy:  0.886 \n",
      "         \n",
      "            \n",
      "...completed  18  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.678244680335165, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7474212700900761, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.35456979274749756, \n",
      "   \n",
      "            Current testing accuracy:  0.892 \n",
      "         \n",
      "            \n",
      "...completed  19  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.181424534137433, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7328190839405243, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3510766625404358, \n",
      "   \n",
      "            Current testing accuracy:  0.891 \n",
      "         \n",
      "            \n",
      "...completed  20  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.720349944030845, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7193363414579259, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.34417617321014404, \n",
      "   \n",
      "            Current testing accuracy:  0.897 \n",
      "         \n",
      "            \n",
      "...completed  21  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.275276849069796, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7068869544423265, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.342999666929245, \n",
      "   \n",
      "            Current testing accuracy:  0.893 \n",
      "         \n",
      "            \n",
      "...completed  22  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.86029407976729, \n",
      "\n",
      "            Current training cross_entropy loss:  0.6952743929531529, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.33870282769203186, \n",
      "   \n",
      "            Current testing accuracy:  0.895 \n",
      "         \n",
      "            \n",
      "...completed  23  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.490610339664496, \n",
      "\n",
      "            Current training cross_entropy loss:  0.684340869810464, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.33776533603668213, \n",
      "   \n",
      "            Current testing accuracy:  0.9 \n",
      "         \n",
      "            \n",
      "...completed  24  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.127237348177495, \n",
      "\n",
      "            Current training cross_entropy loss:  0.6741762727315609, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3330245316028595, \n",
      "   \n",
      "            Current testing accuracy:  0.897 \n",
      "         \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "net_ffa_noisy = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_noisy_ce, test_loss_ffa_noisy_ce, \n",
    " losses_ffa_noisy_mse, test_loss_ffa_noisy_mse,\n",
    " accuracy_ffa_noisy) = net_ffa_noisy.train_noisydata(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9750 25 9750 25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(losses_ffa_noisy_ce), \n",
    "      len(test_loss_ffa_noisy_ce),\n",
    "      len(losses_ffa_noisy_mse),\n",
    "      len(test_loss_ffa_noisy_mse),\n",
    "      len(accuracy_ffa_noisy)\n",
    "      )\n",
    "np.save('results/netffa/losses_ffa_noisy_ce.npy', np.asarray(losses_ffa_noisy_ce))\n",
    "np.save('results/netffa/test_loss_ffa_noisy_ce.npy', np.asarray(test_loss_ffa_noisy_ce))\n",
    "np.save('results/netffa/losses_ffa_noisy_mse.npy', np.asarray(losses_ffa_noisy_mse))\n",
    "np.save('results/netffa/test_loss_ffa_noisy_mse.npy', np.asarray(test_loss_ffa_noisy_mse))\n",
    "np.save('results/netffa/accuracy_ffa_noisy.npy', np.asarray(accuracy_ffa_noisy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/test/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_test = data[0]\n",
    "accuracy_node_perturb_test = data[1]\n",
    "test_losses_node_perturb_test = data[2]\n",
    "snr_node_perturb_test = data[3]\n",
    "cosine_similarity_node_perturb_test = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/normal/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_normal = data[0]\n",
    "accuracy_node_perturb_normal = data[1]\n",
    "test_losses_node_perturb_normal = data[2]\n",
    "snr_node_perturb_normal = data[3]\n",
    "cosine_similarity_node_perturb_normal = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/online/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_online = data[0]\n",
    "accuracy_node_perturb_online = data[1]\n",
    "test_losses_node_perturb_online = data[2]\n",
    "snr_node_perturb_online = data[3]\n",
    "cosine_similarity_node_perturb_online = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/noisy/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_noisy = data[0]\n",
    "accuracy_node_perturb_noisy = data[1]\n",
    "test_losses_node_perturb_noisy = data[2]\n",
    "snr_node_perturb_noisy = data[3]\n",
    "cosine_similarity_node_perturb_noisy = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/non-stat/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_non_stat = data[0]\n",
    "accuracy_node_perturb_non_stat = data[1]\n",
    "test_losses_node_perturb_non_stat = data[2]\n",
    "snr_node_perturb_non_stat = data[3]\n",
    "cosine_similarity_node_perturb_non_stat = data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolen-Pollack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current loss:  61.69\n",
      "...completed  2.0  epochs of training. Current loss:  60.47\n",
      "...completed  3.0  epochs of training. Current loss:  58.96\n",
      "...completed  4.0  epochs of training. Current loss:  57.89\n",
      "...completed  5.0  epochs of training. Current loss:  57.2\n",
      "...completed  6.0  epochs of training. Current loss:  56.72\n",
      "...completed  7.0  epochs of training. Current loss:  56.38\n",
      "...completed  8.0  epochs of training. Current loss:  56.12\n",
      "...completed  9.0  epochs of training. Current loss:  55.92\n",
      "...completed  10.0  epochs of training. Current loss:  55.76\n",
      "...completed  11.0  epochs of training. Current loss:  55.64\n",
      "...completed  12.0  epochs of training. Current loss:  55.53\n",
      "...completed  13.0  epochs of training. Current loss:  55.45\n",
      "...completed  14.0  epochs of training. Current loss:  55.38\n",
      "...completed  15.0  epochs of training. Current loss:  55.33\n",
      "...completed  16.0  epochs of training. Current loss:  55.28\n",
      "...completed  17.0  epochs of training. Current loss:  55.24\n",
      "...completed  18.0  epochs of training. Current loss:  55.21\n",
      "...completed  19.0  epochs of training. Current loss:  55.18\n",
      "...completed  20.0  epochs of training. Current loss:  55.16\n",
      "...completed  21.0  epochs of training. Current loss:  55.14\n",
      "...completed  22.0  epochs of training. Current loss:  55.12\n",
      "...completed  23.0  epochs of training. Current loss:  55.1\n",
      "...completed  24.0  epochs of training. Current loss:  55.09\n",
      "...completed  25.0  epochs of training. Current loss:  55.08\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Normal Learning\n",
    "netkolepoll = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_normal, accuracy_kp_normal, test_loss_kp_normal, snr_kp_normal, cosine_similarity_kp_normal) = \\\n",
    "    netkolepoll.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "net_kp_online = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_online, accuracy_kp_online, test_loss_kp_online, snr_kp_online, cosine_similarity_kp_online) = \\\n",
    "    net_kp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current loss:  61.84\n",
      "...completed  2.0  epochs of training. Current loss:  60.92\n",
      "...completed  3.0  epochs of training. Current loss:  59.47\n",
      "...completed  4.0  epochs of training. Current loss:  58.24\n",
      "...completed  5.0  epochs of training. Current loss:  57.43\n",
      "...completed  6.0  epochs of training. Current loss:  56.87\n",
      "...completed  7.0  epochs of training. Current loss:  56.47\n",
      "...completed  8.0  epochs of training. Current loss:  56.18\n",
      "...completed  9.0  epochs of training. Current loss:  55.97\n",
      "...completed  10.0  epochs of training. Current loss:  55.8\n",
      "...completed  11.0  epochs of training. Current loss:  55.66\n",
      "...completed  12.0  epochs of training. Current loss:  55.55\n",
      "...completed  13.0  epochs of training. Current loss:  55.45\n",
      "...completed  14.0  epochs of training. Current loss:  55.38\n",
      "...completed  15.0  epochs of training. Current loss:  55.31\n",
      "...completed  16.0  epochs of training. Current loss:  55.26\n",
      "...completed  17.0  epochs of training. Current loss:  55.21\n",
      "...completed  18.0  epochs of training. Current loss:  55.17\n",
      "...completed  19.0  epochs of training. Current loss:  55.14\n",
      "...completed  20.0  epochs of training. Current loss:  55.11\n",
      "...completed  21.0  epochs of training. Current loss:  55.09\n",
      "...completed  22.0  epochs of training. Current loss:  55.07\n",
      "...completed  23.0  epochs of training. Current loss:  55.06\n",
      "...completed  24.0  epochs of training. Current loss:  55.04\n",
      "...completed  25.0  epochs of training. Current loss:  55.03\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Noisy Input\n",
    "net_kp_noisy = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_noisy, accuracy_kp_noisy, test_loss_kp_noisy, snr_kp_noisy, cosine_similarity_kp_noisy) = \\\n",
    "    net_kp_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_kp_nonstat = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_nonstat, accuracy_kp_nonstat, test_loss_kp_nonstat, snr_kp_nonstat, cosine_similarity_kp_nonstat) = \\\n",
    "    net_kp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run without node perturb and ffa\n",
    "# FFA\n",
    "snr_ffa_normal, cosine_similarity_ffa_normal = [0], [0]\n",
    "snr_ffa_online, cosine_similarity_ffa_online = [0], [0]\n",
    "snr_ffa_noisy, cosine_similarity_ffa_noisy = [0], [0]\n",
    "snr_ffa_nonstat, cosine_similarity_ffa_nonstat = [0], [0]\n",
    "\n",
    "# Node Perturb\n",
    "losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr_np_normal, cosine_similarity_np_normal = [0], [0], [0], [0], [0]\n",
    "losses_np_online, accuracy_np_online, test_loss_np_online, snr_np_online, cosine_similarity_np_online = [0], [0], [0], [0], [0]\n",
    "losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr_np_noisy, cosine_similarity_np_noisy = [0], [0], [0], [0], [0]\n",
    "losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr_np_nonstat, cosine_similarity_np_nonstat = [0], [0], [0], [0], [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays\n",
    "# normal \n",
    "tr_loss_normal = [losses_bp_normal, losses_ffa_normal_ce, losses_np_normal, losses_kp_normal]\n",
    "te_acc_normal = [accuracy_bp_normal, accuracy_ffa_normal, accuracy_np_normal, accuracy_kp_normal]\n",
    "te_loss_normal = [test_loss_bp_normal, test_loss_ffa_normal_ce, test_loss_np_normal, test_loss_kp_normal]\n",
    "snr_normal = [snr_bp_normal, snr_ffa_normal, snr_np_normal, snr_kp_normal]\n",
    "cosine_similarity_normal = [cosine_similarity_bp_normal, cosine_similarity_ffa_normal, cosine_similarity_np_normal, cosine_similarity_kp_normal]\n",
    "\n",
    "# online\n",
    "# Calculate the moving average\n",
    "window_size = 100\n",
    "losses_bp_online_mean = uniform_filter1d(losses_bp_online, size=window_size)\n",
    "losses_ffa_online_mean = uniform_filter1d(losses_ffa_online_ce, size=window_size)\n",
    "losses_np_online_mean = uniform_filter1d(losses_np_online, size=window_size)\n",
    "losses_kp_online_mean = uniform_filter1d(losses_kp_online, size=window_size)\n",
    "tr_loss_online_mean = [losses_bp_online_mean, losses_ffa_online_mean, losses_np_online_mean, losses_kp_online_mean]\n",
    "\n",
    "tr_loss_online = [losses_bp_online, losses_ffa_online_ce, losses_np_online, losses_kp_online]\n",
    "te_acc_online = [accuracy_bp_online, accuracy_ffa_online, accuracy_np_online, accuracy_kp_online]\n",
    "te_loss_online = [test_loss_bp_online, test_loss_ffa_online_ce, test_loss_np_online, test_loss_kp_online]\n",
    "snr_online = [snr_bp_online, snr_ffa_online, snr_np_online, snr_kp_online]\n",
    "cosine_similarity_online = [cosine_similarity_bp_online, cosine_similarity_ffa_online, cosine_similarity_np_online, cosine_similarity_kp_online]\n",
    "\n",
    "# noisy\n",
    "tr_loss_noisy = [losses_bp_noisy, losses_ffa_noisy_ce, losses_np_noisy, losses_kp_noisy]\n",
    "te_acc_noisy = [accuracy_bp_noisy, accuracy_ffa_noisy, accuracy_np_noisy, accuracy_kp_noisy]\n",
    "te_loss_noisy = [test_loss_bp_noisy, test_loss_ffa_noisy_ce, test_loss_np_noisy, test_loss_kp_noisy]\n",
    "snr_noisy = [snr_bp_noisy, snr_ffa_noisy, snr_np_noisy, snr_kp_noisy]\n",
    "cosine_similarity_noisy = [cosine_similarity_bp_noisy, cosine_similarity_ffa_noisy, cosine_similarity_np_noisy, cosine_similarity_kp_noisy]\n",
    "\n",
    "# nonstat\n",
    "tr_loss_nonstat = [losses_bp_nonstat, losses_ffa_nonstat, losses_np_nonstat, losses_kp_nonstat]\n",
    "te_acc_nonstat = [accuracy_bp_nonstat, accuracy_ffa_nonstat, accuracy_np_nonstat, accuracy_kp_nonstat]\n",
    "te_loss_nonstat = [test_loss_bp_nonstat, test_loss_ffa_nonstat_ce, test_loss_np_nonstat, test_loss_kp_nonstat]\n",
    "snr_nonstat = [snr_bp_nonstat, snr_ffa_nonstat, snr_np_nonstat, snr_kp_nonstat]\n",
    "cosine_similarity_nonstat = [cosine_similarity_bp_nonstat, cosine_similarity_ffa_nonstat, cosine_similarity_np_nonstat, cosine_similarity_kp_nonstat]\n",
    "\n",
    "# algorithms\n",
    "# algos = ['normal', 'online', 'noisy', 'nonstat']\n",
    "algos = ['normal', 'noisy', 'nonstat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "for i, algo in enumerate(algos):\n",
    "    if algo == 'normal':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_normal, te_acc_normal, te_loss_normal, snr_normal, cosine_similarity_normal\n",
    "    # elif algo == 'online':\n",
    "    #     tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_online, te_acc_online, te_loss_online, snr_online, cosine_similarity_online\n",
    "    elif algo == 'noisy':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_noisy, te_acc_noisy, te_loss_noisy, snr_noisy, cosine_similarity_noisy\n",
    "    elif algo == 'nonstat':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_nonstat, te_acc_nonstat, te_loss_nonstat, snr_nonstat, cosine_similarity_nonstat\n",
    "        \n",
    "    # plot\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(151)\n",
    "\n",
    "    plt.plot(tr_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(tr_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(tr_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(tr_loss[3], label=\"Kolen-Pollack\", color='k')\n",
    "    \n",
    "    plt.xlabel(\"Updates (every batch)\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training loss\")\n",
    "\n",
    "    plt.subplot(152)\n",
    "    plt.plot(te_acc[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_acc[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_acc[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_acc[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Performance over time\")\n",
    "\n",
    "    plt.subplot(153)\n",
    "    plt.plot(te_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_loss[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Test loss\")\n",
    "\n",
    "    plt.subplot(154)\n",
    "    with plt.xkcd():\n",
    "            x = [0, 1, 2, 3]\n",
    "            snr1_vals = [snr[0][:, 1], snr[0][:, 2], snr[0][:, 3], snr[0][:, 0]]\n",
    "            snr2_vals = [snr[1][:, 1], snr[1][:, 2], snr[1][:, 3], snr[1][:, 0]]\n",
    "            colors = ['gold', 'c', 'k', 'r']\n",
    "            labels = ['FFA', 'Node Perturbation', 'Kolen Pollack', 'Backprop']\n",
    "            for i in range(len(snr1_vals)):\n",
    "                plt.bar(x, snr1_vals[i], color=colors, tick_label=labels)\n",
    "                plt.bar(x, snr2_vals[i], color=colors, tick_label=labels, alpha=0.5)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.ylabel('SNR')\n",
    "            plt.xlabel('Algorithm')\n",
    "            plt.title('Gradient SNR')\n",
    "\n",
    "    plt.subplot(155)\n",
    "    with plt.xkcd():\n",
    "        epochs = np.arange(0, numepochs, 1)\n",
    "        plt.plot(epochs, cos_sim[0][:, 0], label=\"Backprop, 1st layer\", color='r', linestyle=':')\n",
    "        plt.plot(epochs, cos_sim[0][:, 1], label=\"Backprop, 2nd layer\", color='r', linestyle='-.')\n",
    "        # plt.plot(cos_sim[1][:, 0], label=\"FFA, 1st layer\", color='gold', linestyle=':')\n",
    "        # plt.plot(cos_sim[1][:, 1], label=\"FFA, 2nd layer\", color='gold', linestyle='-.')\n",
    "        # plt.plot(cos_sim[2][:, 0], label=\"Node Perturbation, 1st layer\", color='c', linestyle=':')\n",
    "        # plt.plot(cos_sim[2][:, 1], label=\"Node Perturbation, 2nd layer\", color='c', linestyle='-.')\n",
    "        plt.plot(epochs, cos_sim[3][:, 0], label=\"Kollen-Pollack, 1st layer\", color='k', linestyle=':')\n",
    "        plt.plot(epochs, cos_sim[3][:, 1], label=\"Kollen-Pollack, 2nd layer\", color='k', linestyle='-.')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cosine Sim\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Cosine Similarity to Backprop\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning separately becuase of different lenghts and additional smoothing operation\n",
    "# plot\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(151)\n",
    "\n",
    "plt.plot(tr_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(tr_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(tr_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(tr_loss_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "\n",
    "plt.xlabel(\"Updates (every batch)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Training loss\")\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(te_acc_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_acc_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_acc_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_acc_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(te_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_loss_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Test loss\")\n",
    "\n",
    "plt.subplot(154)\n",
    "with plt.xkcd():\n",
    "        x = [0, 1, 2, 3]\n",
    "        snr1_vals = [snr_online[0][:, 1], snr_online[0][:, 2], snr_online[0][:, 3], snr_online[0][:, 0]]\n",
    "        snr2_vals = [snr_online[1][:, 1], snr_online[1][:, 2], snr_online[1][:, 3], snr_online[1][:, 0]]\n",
    "        colors = ['gold', 'c', 'k', 'r']\n",
    "        labels = ['FFA', 'Node Perturbation', 'Kolen Pollack', 'Backprop']\n",
    "        for i in range(len(snr1_vals)):\n",
    "            plt.bar(x, snr1_vals[i], color=colors, tick_label=labels)\n",
    "            plt.bar(x, snr2_vals[i], color=colors, tick_label=labels, alpha=0.5)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('SNR')\n",
    "        plt.xlabel('Algorithm')\n",
    "        plt.title('Gradient SNR')\n",
    "\n",
    "plt.subplot(155)\n",
    "with plt.xkcd():\n",
    "    epochs = np.arange(0, numepochs, 1)\n",
    "    plt.plot(epochs, cosine_similarity_online[0][:, 0], label=\"Backprop, 1st layer\", color='r', linestyle=':')\n",
    "    plt.plot(epochs, cosine_similarity_online[0][:, 1], label=\"Backprop, 2nd layer\", color='r', linestyle='-.')\n",
    "    # plt.plot(cos_sim[1][:, 0], label=\"FFA, 1st layer\", color='gold', linestyle=':')\n",
    "    # plt.plot(cos_sim[1][:, 1], label=\"FFA, 2nd layer\", color='gold', linestyle='-.')\n",
    "    # plt.plot(cos_sim[2][:, 0], label=\"Node Perturbation, 1st layer\", color='c', linestyle=':')\n",
    "    # plt.plot(cos_sim[2][:, 1], label=\"Node Perturbation, 2nd layer\", color='c', linestyle='-.')\n",
    "    plt.plot(epochs, cosine_similarity_online[3][:, 0], label=\"Kollen-Pollack, 1st layer\", color='k', linestyle=':')\n",
    "    plt.plot(epochs, cosine_similarity_online[3][:, 1], label=\"Kollen-Pollack, 2nd layer\", color='k', linestyle='-.')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cosine Sim\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cosine Similarity to Backprop\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
