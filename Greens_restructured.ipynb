{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Much code taken from Neuromatch NeuroAI 2024 Microlearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from IPython.display import Image, SVG, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "import io\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "## Plotting and metrics imports\n",
    "from metrics import get_plotting_color, plot_examples, plot_class_distribution, plot_results, plot_scores_per_class, plot_weights\n",
    "\n",
    "## Other functions imports\n",
    "from helpers import sigmoid, ReLU, add_bias, create_batches, calculate_accuracy, calculate_cosine_similarity, calculate_grad_snr\n",
    "\n",
    "## MLP imports\n",
    "from MLP import MLP, NodePerturbMLP, KolenPollackMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST function\n",
    "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
    "\n",
    "  valid_prop = 1 - train_prop\n",
    "\n",
    "  discard_prop = 1 - keep_prop\n",
    "\n",
    "  transform = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.Normalize((0.1307,), (0.3081,))]\n",
    "      )\n",
    "\n",
    "\n",
    "  with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    \n",
    "    rng_data = np.random.default_rng(seed=42)\n",
    "    train_num = 50000\n",
    "    shuffled_train_idx = rng_data.permutation(train_num)\n",
    "\n",
    "    full_train_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=True, download=True, transform=transform)\n",
    "    full_test_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=False, download=True, transform=transform)\n",
    "    \n",
    "    full_train_images = full_train_set.data.numpy().astype(float) / 255\n",
    "    train_images = full_train_images[shuffled_train_idx[:train_num]].reshape((-1, 784)).T.copy()\n",
    "    valid_images = full_train_images[shuffled_train_idx[train_num:]].reshape((-1, 784)).T.copy()\n",
    "    test_images = (full_test_set.data.numpy().astype(float) / 255).reshape((-1, 784)).T\n",
    "\n",
    "    full_train_labels = torch.nn.functional.one_hot(full_train_set.targets, num_classes=10).numpy()\n",
    "    train_labels = full_train_labels[shuffled_train_idx[:train_num]].T.copy()\n",
    "    valid_labels = full_train_labels[shuffled_train_idx[train_num:]].T.copy()\n",
    "    test_labels = torch.nn.functional.one_hot(full_test_set.targets, num_classes=10).numpy().T\n",
    "\n",
    "    train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "      full_train_set, [train_prop * keep_prop, valid_prop * keep_prop, discard_prop])\n",
    "    test_set, _ = torch.utils.data.random_split(\n",
    "      full_test_set, [keep_prop, discard_prop])\n",
    "\n",
    "  print(\"Number of examples retained:\")\n",
    "  print(f\"  {len(train_set)} (training)\")\n",
    "  print(f\"  {len(valid_set)} (validation)\")\n",
    "  print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "  return train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples retained:\n",
      "  24001 (training)\n",
      "  5999 (validation)\n",
      "  5000 (test)\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels = download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "NUM_INPUTS = 784\n",
    "NUM_OUTPUTS = 10\n",
    "numhidden = 500\n",
    "batchsize = 128\n",
    "initweight = 0.1\n",
    "learnrate = 0.001\n",
    "noise = 0.1\n",
    "numepochs = 25\n",
    "numrepeats = 1\n",
    "numbatches = int(train_images.shape[1] / batchsize)\n",
    "numupdates = numepochs * numbatches\n",
    "activation = 'sigmoid'\n",
    "report = True\n",
    "rep_rate = 1\n",
    "seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "indices = np.random.choice(test_images.shape[1], size=1000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current loss:  59.83\n",
      "...completed  2.0  epochs of training. Current loss:  57.47\n",
      "...completed  3.0  epochs of training. Current loss:  56.28\n",
      "...completed  4.0  epochs of training. Current loss:  55.61\n",
      "...completed  5.0  epochs of training. Current loss:  55.17\n",
      "...completed  6.0  epochs of training. Current loss:  54.85\n",
      "...completed  7.0  epochs of training. Current loss:  54.61\n",
      "...completed  8.0  epochs of training. Current loss:  54.43\n",
      "...completed  9.0  epochs of training. Current loss:  54.27\n",
      "...completed  10.0  epochs of training. Current loss:  54.15\n",
      "...completed  11.0  epochs of training. Current loss:  54.04\n",
      "...completed  12.0  epochs of training. Current loss:  53.95\n",
      "...completed  13.0  epochs of training. Current loss:  53.87\n",
      "...completed  14.0  epochs of training. Current loss:  53.8\n",
      "...completed  15.0  epochs of training. Current loss:  53.73\n",
      "...completed  16.0  epochs of training. Current loss:  53.68\n",
      "...completed  17.0  epochs of training. Current loss:  53.63\n",
      "...completed  18.0  epochs of training. Current loss:  53.58\n",
      "...completed  19.0  epochs of training. Current loss:  53.54\n",
      "...completed  20.0  epochs of training. Current loss:  53.5\n",
      "...completed  21.0  epochs of training. Current loss:  53.46\n",
      "...completed  22.0  epochs of training. Current loss:  53.43\n",
      "...completed  23.0  epochs of training. Current loss:  53.4\n",
      "...completed  24.0  epochs of training. Current loss:  53.37\n",
      "...completed  25.0  epochs of training. Current loss:  53.35\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Normal learning\n",
    "netbackprop = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_normal, accuracy_bp_normal, test_loss_bp_normal, snr_bp_normal, cosine_similarity_bp_normal) = \\\n",
    "    netbackprop.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_online = accuracy_online[::batchsize]\n",
    "#plt.plot(losses_feedback, label=\"Feedback Alignment\", color='g')\n",
    "plt.plot(accuracy_backprop, label=\"Backprop\", color='r')\n",
    "plt.plot(accuracy_kolepoll, label=\"Kolen-Pollack\", color='k')\n",
    "# plt.plot(accuracy_node_perturb, label=\"Node Perturbation\", color='c')\n",
    "plt.plot(accuracy_online, label=\"Online Learning\", color='gold')\n",
    "plt.plot(accuracy_nonstat, label=\"Non-stationary Data\", color='forestgreen')\n",
    "# plt.plot(accuracy_noisy_input, label=\"Noisy Input Data\", color='dodgerblue')\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 715. TiB for an array with shape (1000, 500, 785, 500, 501) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Online learning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m net_bp_online \u001b[38;5;241m=\u001b[39m MLP(rng, numhidden, sigma\u001b[38;5;241m=\u001b[39minitweight, activation\u001b[38;5;241m=\u001b[39mactivation)\n\u001b[0;32m      3\u001b[0m (losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr_bp_online, cosine_similarity_bp_online) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mnet_bp_online\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_online\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_it\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumupdates\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackprop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bonni\\Bonnie\\Studium\\Master\\Master - Computational Neuroscience\\NeuroAI\\NMA-Microlearning-Project-1\\MLP.py:551\u001b[0m, in \u001b[0;36mtrain_online\u001b[1;34m(self, rng, images, labels, test_images, test_labels, learning_rate, max_it, conv_loss, algorithm, noise, report, report_rate)\u001b[0m\n\u001b[0;32m    548\u001b[0m test_loss = []\n\u001b[0;32m    549\u001b[0m cosine_similarity = []\n\u001b[1;32m--> 551\u001b[0m # estimate the gradient SNR on the test set\n\u001b[0;32m    552\u001b[0m grad = np.zeros((test_images.shape[1], *self.W_h_1.shape))\n\u001b[0;32m    553\u001b[0m for t in range(test_images.shape[1]):\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 715. TiB for an array with shape (1000, 500, 785, 500, 501) and data type float64"
     ]
    }
   ],
   "source": [
    "# Online learning\n",
    "net_bp_online = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr_bp_online, cosine_similarity_bp_online) = \\\n",
    "    net_bp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "# create a network and train it using backprop\n",
    "netbackprop_noisy = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr_bp_noisy, cosine_similarity_bp_noisy) = \\\n",
    "    netbackprop_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_bp_nonstat = MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_nonstat, accuracy_bp_nonstat, test_loss_bp_nonstat, snr_bp_nonstat, cosine_similarity_bp_nonstat) = \\\n",
    "    net_bp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal learning\n",
    "# create a network and train it using ffa\n",
    "# netffa = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal, snr_ffa_normal, cosine_similarity_ffa_normal) = \\\n",
    "#     netffa.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "# net_ffa_online = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_online, accuracy_ffa_online, test_loss_ffa_online, snr_ffa_online, cosine_similarity_ffa_online) = \\\n",
    "#     net_ffa_online.train_online(rng_bp2, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-4, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network and train it using ffa\n",
    "# net_ffa_noisy = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy, snr_ffa_noisy, cosine_similarity_ffa_noisy) = \\\n",
    "#     net_ffa_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# net_ffa_nonstat = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat, snr_ffa_nonstat, cosine_similarity_ffa_nonstat) = \\\n",
    "#     net_ffa_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/test/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_test = data[0]\n",
    "accuracy_node_perturb_test = data[1]\n",
    "test_losses_node_perturb_test = data[2]\n",
    "snr_node_perturb_test = data[3]\n",
    "cosine_similarity_node_perturb_test = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "# create a network and train it using node perturbation\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     netnodeperturb = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr_np_normal, cosine_similarity_np_normal) = \\\n",
    "#         netnodeperturb.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                              learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, report=report, report_rate=rep_rate)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/normal/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_normal = data[0]\n",
    "accuracy_node_perturb_normal = data[1]\n",
    "test_losses_node_perturb_normal = data[2]\n",
    "snr_node_perturb_normal = data[3]\n",
    "cosine_similarity_node_perturb_normal = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     netnodeperturb_online = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_online, accuracy_np_online, test_loss_np_online, snr_np_online, cosine_similarity_np_online) = \\\n",
    "#         netnodeperturb_online.train_online(rng_bp2, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-4, algorithm='node_perturb', noise=noise, \\\n",
    "#                       report=report, report_rate=batchsize)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/online/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_online = data[0]\n",
    "accuracy_node_perturb_online = data[1]\n",
    "test_losses_node_perturb_online = data[2]\n",
    "snr_node_perturb_online = data[3]\n",
    "cosine_similarity_node_perturb_online = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     nodeperturb_noisy = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr_np_noisy, cosine_similarity_np_noisy) = \\\n",
    "#         nodeperturb_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                         learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, \\\n",
    "#                         noise_type='gauss',report=report, report_rate=rep_rate)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/noisy/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_noisy = data[0]\n",
    "accuracy_node_perturb_noisy = data[1]\n",
    "test_losses_node_perturb_noisy = data[2]\n",
    "snr_node_perturb_noisy = data[3]\n",
    "cosine_similarity_node_perturb_noisy = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     nodeperturb_nonstat = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr_np_nonstat, cosine_similarity_np_nonstat) = \\\n",
    "#         nodeperturb_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                         learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, \\\n",
    "#                         report=report, report_rate=1)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/non-stat/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_non_stat = data[0]\n",
    "accuracy_node_perturb_non_stat = data[1]\n",
    "test_losses_node_perturb_non_stat = data[2]\n",
    "snr_node_perturb_non_stat = data[3]\n",
    "cosine_similarity_node_perturb_non_stat = data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolen-Pollack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "netkolepoll = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_normal, accuracy_kp_normal, test_loss_kp_normal, snr_kp_normal, cosine_similarity_kp_normal) = \\\n",
    "    netkolepoll.train(rng_kp, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "net_kp_online = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_online, accuracy_kp_online, test_loss_kp_online, snr_kp_online, cosine_similarity_kp_online) = \\\n",
    "    net_kp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "net_kp_noisy = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_noisy, accuracy_kp_noisy, test_loss_kp_noisy, snr_kp_noisy, cosine_similarity_kp_noisy) = \\\n",
    "    net_kp_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_kp_nonstat = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_nonstat, accuracy_kp_nonstat, test_loss_kp_nonstat, snr_kp_nonstat, cosine_similarity_kp_nonstat) = \\\n",
    "    net_kp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run without node perturb and ffa\n",
    "# FFA\n",
    "losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal, snr_ffa_normal, cosine_similarity_ffa_normal = [0], [0], [0], [0], [0]\n",
    "losses_ffa_online, accuracy_ffa_online, test_loss_ffa_online, snr_ffa_online, cosine_similarity_ffa_online = [0], [0], [0], [0], [0]\n",
    "losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy, snr_ffa_noisy, cosine_similarity_ffa_noisy = [0], [0], [0], [0], [0]\n",
    "losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat, snr_ffa_nonstat, cosine_similarity_ffa_nonstat = [0], [0], [0], [0], [0]\n",
    "\n",
    "# Node Perturb\n",
    "losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr_np_normal, cosine_similarity_np_normal = [0], [0], [0], [0], [0]\n",
    "losses_np_online, accuracy_np_online, test_loss_np_online, snr_np_online, cosine_similarity_np_online = [0], [0], [0], [0], [0]\n",
    "losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr_np_noisy, cosine_similarity_np_noisy = [0], [0], [0], [0], [0]\n",
    "losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr_np_nonstat, cosine_similarity_np_nonstat = [0], [0], [0], [0], [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays\n",
    "# normal \n",
    "tr_loss_normal = [losses_bp_normal, losses_ffa_normal, losses_np_normal, losses_kp_normal]\n",
    "te_acc_normal = [accuracy_bp_normal, accuracy_ffa_normal, accuracy_np_normal, accuracy_kp_normal]\n",
    "te_loss_normal = [test_loss_bp_normal, test_loss_ffa_normal, test_loss_np_normal, test_loss_kp_normal]\n",
    "snr_normal = [snr_bp_normal, snr_ffa_normal, snr_np_normal, snr_kp_normal]\n",
    "cosine_similarity_normal = [cosine_similarity_bp_normal, cosine_similarity_ffa_normal, cosine_similarity_np_normal, cosine_similarity_kp_normal]\n",
    "\n",
    "# online\n",
    "# Calculate the moving average\n",
    "window_size = 100\n",
    "losses_bp_online_mean = uniform_filter1d(losses_bp_online, size=window_size)\n",
    "losses_ffa_online_mean = uniform_filter1d(losses_ffa_online, size=window_size)\n",
    "losses_np_online_mean = uniform_filter1d(losses_np_online, size=window_size)\n",
    "losses_kp_online_mean = uniform_filter1d(losses_kp_online, size=window_size)\n",
    "tr_loss_online_mean = [losses_bp_online_mean, losses_ffa_online_mean, losses_np_online_mean, losses_kp_online_mean]\n",
    "\n",
    "tr_loss_online = [losses_bp_online, losses_ffa_online, losses_np_online, losses_kp_online]\n",
    "te_acc_online = [accuracy_bp_online, accuracy_ffa_online, accuracy_np_online, accuracy_kp_online]\n",
    "te_loss_online = [test_loss_bp_online, test_loss_ffa_online, test_loss_np_online, test_loss_kp_online]\n",
    "snr_online = [snr_bp_online, snr_ffa_online, snr_np_online, snr_kp_online]\n",
    "cosine_similarity_online = [cosine_similarity_bp_online, cosine_similarity_ffa_online, cosine_similarity_np_online, cosine_similarity_kp_online]\n",
    "\n",
    "# noisy\n",
    "tr_loss_noisy = [losses_bp_noisy, losses_ffa_noisy, losses_np_noisy, losses_kp_noisy]\n",
    "te_acc_noisy = [accuracy_bp_noisy, accuracy_ffa_noisy, accuracy_np_noisy, accuracy_kp_noisy]\n",
    "te_loss_noisy = [test_loss_bp_noisy, test_loss_ffa_noisy, test_loss_np_noisy, test_loss_kp_noisy]\n",
    "snr_noisy = [snr_bp_noisy, snr_ffa_noisy, snr_np_noisy, snr_kp_noisy]\n",
    "cosine_similarity_noisy = [cosine_similarity_bp_noisy, cosine_similarity_ffa_noisy, cosine_similarity_np_noisy, cosine_similarity_kp_noisy]\n",
    "\n",
    "# nonstat\n",
    "tr_loss_nonstat = [losses_bp_nonstat, losses_ffa_nonstat, losses_np_nonstat, losses_kp_nonstat]\n",
    "te_acc_nonstat = [accuracy_bp_nonstat, accuracy_ffa_nonstat, accuracy_np_nonstat, accuracy_kp_nonstat]\n",
    "te_loss_nonstat = [test_loss_bp_nonstat, test_loss_ffa_nonstat, test_loss_np_nonstat, test_loss_kp_nonstat]\n",
    "snr_nonstat = [snr_bp_nonstat, snr_ffa_nonstat, snr_np_nonstat, snr_kp_nonstat]\n",
    "cosine_similarity_nonstat = [cosine_similarity_bp_nonstat, cosine_similarity_ffa_nonstat, cosine_similarity_np_nonstat, cosine_similarity_kp_nonstat]\n",
    "\n",
    "# algorithms\n",
    "# algos = ['normal', 'online', 'noisy', 'nonstat']\n",
    "algos = ['normal', 'noisy', 'nonstat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "for i, algo in enumerate(algos):\n",
    "    if algo == 'normal':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_normal, te_acc_normal, te_loss_normal, snr_normal, cosine_similarity_normal\n",
    "    # elif algo == 'online':\n",
    "    #     tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_online, te_acc_online, te_loss_online, snr_online, cosine_similarity_online\n",
    "    elif algo == 'noisy':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_noisy, te_acc_noisy, te_loss_noisy, snr_noisy, cosine_similarity_noisy\n",
    "    elif algo == 'nonstat':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_nonstat, te_acc_nonstat, te_loss_nonstat, snr_nonstat, cosine_similarity_nonstat\n",
    "        \n",
    "    # plot\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(151)\n",
    "\n",
    "    plt.plot(tr_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(tr_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(tr_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(tr_loss[3], label=\"Kolen-Pollack\", color='k')\n",
    "    \n",
    "    plt.xlabel(\"Updates (every batch)\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training loss\")\n",
    "\n",
    "    plt.subplot(152)\n",
    "    plt.plot(te_acc[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_acc[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_acc[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_acc[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Performance over time\")\n",
    "\n",
    "    plt.subplot(153)\n",
    "    plt.plot(te_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_loss[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Test loss\")\n",
    "\n",
    "    plt.subplot(154)\n",
    "    with plt.xkcd():\n",
    "            x = [0, 1, 2, 3]\n",
    "            snr_vals = [snr[1], snr[2], snr[3], snr[0]]\n",
    "            colors = ['gold', 'c', 'k', 'r']\n",
    "            labels = ['FFA', 'Node Perturbation', 'Kolen Pollack', 'Backprop']\n",
    "            plt.bar(x, snr_vals, color=colors, tick_label=labels)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.ylabel('SNR')\n",
    "            plt.xlabel('Algorithm')\n",
    "            plt.title('Gradient SNR')\n",
    "\n",
    "    plt.subplot(155)\n",
    "    with plt.xkcd():\n",
    "        plt.plot(cos_sim[0], label=\"Backprop\", color='r')\n",
    "        plt.plot(cos_sim[1], label=\"FFA\", color='gold')\n",
    "        plt.plot(cos_sim[2], label=\"Node Perturbation\", color='c')\n",
    "        plt.plot(cos_sim[3], label=\"Kolen-Pollack\", color='k')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cosine Sim\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Cosine Similarity to Backprop\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning separately becuase of different lenghts and additional smoothing operation\n",
    "# plot\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(151)\n",
    "\n",
    "plt.plot(tr_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(tr_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(tr_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(tr_loss_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "\n",
    "plt.xlabel(\"Updates (every batch)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Training loss\")\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(te_acc_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_acc_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_acc_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_acc_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(te_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_loss_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Test loss\")\n",
    "\n",
    "plt.subplot(154)\n",
    "with plt.xkcd():\n",
    "        x = [0, 1, 2, 3]\n",
    "        snr_vals = [snr_online[1], snr_online[2], snr_online[3], snr_online[0]]\n",
    "        colors = ['gold', 'c', 'k', 'r']\n",
    "        labels = ['FFA', 'Node Perturbation', 'Kolen Pollack', 'Backprop']\n",
    "        plt.bar(x, snr_vals, color=colors, tick_label=labels)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('SNR')\n",
    "        plt.xlabel('Algorithm')\n",
    "        plt.title('Gradient SNR')\n",
    "\n",
    "plt.subplot(155)\n",
    "with plt.xkcd():\n",
    "    plt.plot(cosine_similarity_online[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(cosine_similarity_online[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(cosine_similarity_online[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(cosine_similarity_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cosine Sim\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cosine Similarity to Backprop\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
