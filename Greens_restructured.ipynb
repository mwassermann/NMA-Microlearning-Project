{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Much code taken from Neuromatch NeuroAI 2024 Microlearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'MLP' from 'd:\\\\MyFolders\\\\project\\\\2024summer\\\\NeuroAI\\\\NMA-Microlearning-Project\\\\MLP.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependencies\n",
    "from IPython.display import Image, SVG, display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "import io\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Plotting and metrics imports\n",
    "from metrics import get_plotting_color, plot_examples, plot_class_distribution, plot_results, plot_scores_per_class, plot_weights\n",
    "\n",
    "## Other functions imports\n",
    "from helpers import sigmoid, ReLU, add_bias, create_batches, calculate_accuracy, calculate_cosine_similarity, calculate_grad_snr\n",
    "\n",
    "## MLP imports\n",
    "import MLP\n",
    "from MLP import NodePerturbMLP, KolenPollackMLP\n",
    "\n",
    "## FF imports\n",
    "import net_ff_model\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import importlib\n",
    "importlib.reload(net_ff_model)\n",
    "importlib.reload(MLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST function\n",
    "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
    "\n",
    "  valid_prop = 1 - train_prop\n",
    "\n",
    "  discard_prop = 1 - keep_prop\n",
    "\n",
    "  transform = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "      torchvision.transforms.onlineize((0.1307,), (0.3081,))]\n",
    "      )\n",
    "\n",
    "\n",
    "  with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
    "    \n",
    "    rng_data = np.random.default_rng(seed=42)\n",
    "    train_num = 50000\n",
    "    shuffled_train_idx = rng_data.permutation(train_num)\n",
    "\n",
    "    full_train_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=True, download=True, transform=transform)\n",
    "    full_test_set = torchvision.datasets.MNIST(\n",
    "          root=\"./data/\", train=False, download=True, transform=transform)\n",
    "    \n",
    "    full_train_images = full_train_set.data.numpy().astype(float) / 255\n",
    "    train_images = full_train_images[shuffled_train_idx[:train_num]].reshape((-1, 784)).T.copy()\n",
    "    valid_images = full_train_images[shuffled_train_idx[train_num:]].reshape((-1, 784)).T.copy()\n",
    "    test_images = (full_test_set.data.numpy().astype(float) / 255).reshape((-1, 784)).T\n",
    "\n",
    "    full_train_labels = torch.nn.functional.one_hot(full_train_set.targets, num_classes=10).numpy()\n",
    "    train_labels = full_train_labels[shuffled_train_idx[:train_num]].T.copy()\n",
    "    valid_labels = full_train_labels[shuffled_train_idx[train_num:]].T.copy()\n",
    "    test_labels = torch.nn.functional.one_hot(full_test_set.targets, num_classes=10).numpy().T\n",
    "\n",
    "    train_set, valid_set, _ = torch.utils.data.random_split(\n",
    "      full_train_set, [train_prop * keep_prop, valid_prop * keep_prop, discard_prop])\n",
    "    test_set, _ = torch.utils.data.random_split(\n",
    "      full_test_set, [keep_prop, discard_prop])\n",
    "\n",
    "  print(\"Number of examples retained:\")\n",
    "  print(f\"  {len(train_set)} (training)\")\n",
    "  print(f\"  {len(valid_set)} (validation)\")\n",
    "  print(f\"  {len(test_set)} (test)\")\n",
    "\n",
    "  return train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples retained:\n",
      "  24001 (training)\n",
      "  5999 (validation)\n",
      "  5000 (test)\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set, train_images, valid_images, test_images, train_labels, valid_labels, test_labels = download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HYPERPARAMETERS\n",
    "NUM_INPUTS = 784\n",
    "NUM_OUTPUTS = 10\n",
    "numhidden = 500\n",
    "batchsize = 128\n",
    "initweight = 0.1\n",
    "learnrate = 0.001\n",
    "noise = 0.1\n",
    "numepochs = 25\n",
    "numrepeats = 1\n",
    "numbatches = int(train_images.shape[1] / batchsize)\n",
    "numupdates = numepochs * numbatches\n",
    "activation = 'sigmoid'\n",
    "report = True\n",
    "rep_rate = 1\n",
    "seed = 12345\n",
    "numupdates*batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "indices = np.random.choice(test_images.shape[1], size=1000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "...completed  1.0  epochs of training. Current training loss:  2.11  epochs of training. Current testing loss:  2.12\n",
      "...completed  2.0  epochs of training. Current training loss:  1.94  epochs of training. Current testing loss:  1.96\n",
      "...completed  3.0  epochs of training. Current training loss:  1.85  epochs of training. Current testing loss:  1.87\n",
      "...completed  4.0  epochs of training. Current training loss:  1.8  epochs of training. Current testing loss:  1.82\n",
      "...completed  5.0  epochs of training. Current training loss:  1.77  epochs of training. Current testing loss:  1.79\n",
      "...completed  6.0  epochs of training. Current training loss:  1.75  epochs of training. Current testing loss:  1.76\n",
      "...completed  7.0  epochs of training. Current training loss:  1.73  epochs of training. Current testing loss:  1.74\n",
      "...completed  8.0  epochs of training. Current training loss:  1.71  epochs of training. Current testing loss:  1.73\n",
      "...completed  9.0  epochs of training. Current training loss:  1.7  epochs of training. Current testing loss:  1.72\n",
      "...completed  10.0  epochs of training. Current training loss:  1.69  epochs of training. Current testing loss:  1.7\n",
      "...completed  11.0  epochs of training. Current training loss:  1.68  epochs of training. Current testing loss:  1.7\n",
      "...completed  12.0  epochs of training. Current training loss:  1.68  epochs of training. Current testing loss:  1.69\n",
      "...completed  13.0  epochs of training. Current training loss:  1.67  epochs of training. Current testing loss:  1.68\n",
      "...completed  14.0  epochs of training. Current training loss:  1.66  epochs of training. Current testing loss:  1.67\n",
      "...completed  15.0  epochs of training. Current training loss:  1.66  epochs of training. Current testing loss:  1.67\n",
      "...completed  16.0  epochs of training. Current training loss:  1.65  epochs of training. Current testing loss:  1.66\n",
      "...completed  17.0  epochs of training. Current training loss:  1.65  epochs of training. Current testing loss:  1.66\n",
      "...completed  18.0  epochs of training. Current training loss:  1.65  epochs of training. Current testing loss:  1.65\n",
      "...completed  19.0  epochs of training. Current training loss:  1.64  epochs of training. Current testing loss:  1.65\n",
      "...completed  20.0  epochs of training. Current training loss:  1.64  epochs of training. Current testing loss:  1.65\n",
      "...completed  21.0  epochs of training. Current training loss:  1.64  epochs of training. Current testing loss:  1.64\n",
      "...completed  22.0  epochs of training. Current training loss:  1.63  epochs of training. Current testing loss:  1.64\n",
      "...completed  23.0  epochs of training. Current training loss:  1.63  epochs of training. Current testing loss:  1.64\n",
      "...completed  24.0  epochs of training. Current training loss:  1.63  epochs of training. Current testing loss:  1.64\n",
      "...completed  25.0  epochs of training. Current training loss:  1.63  epochs of training. Current testing loss:  1.63\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# normal learning\n",
    "netbackprop = MLP.MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_normal, accuracy_bp_normal, test_loss_bp_normal, snr_bp_normal, cosine_similarity_bp_normal) = \\\n",
    "    netbackprop.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9750,) (25,) (25,) 0.10656327458858571 (25, 2)\n"
     ]
    }
   ],
   "source": [
    "print(losses_bp_normal.shape, accuracy_bp_normal.shape, test_loss_bp_normal.shape, snr_bp_normal, cosine_similarity_bp_normal.shape)\n",
    "np.save('results/netbackprop/losses_bp_normal.npy', losses_bp_normal)\n",
    "np.save('results/netbackprop/accuracy_bp_normal.npy', accuracy_bp_normal)\n",
    "np.save('results/netbackprop/test_loss_bp_normal.npy', test_loss_bp_normal)\n",
    "np.save('results/netbackprop/snr_bp_normal.npy', snr_bp_normal)\n",
    "np.save('results/netbackprop/cosine_similarity_bp_normal.npy', cosine_similarity_bp_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# accuracy_online = accuracy_online[::batchsize]\n",
    "#plt.plot(losses_feedback, label=\"Feedback Alignment\", color='g')\n",
    "plt.plot(accuracy_bp_normal, label=\"Backprop\", color='r')\n",
    "'''\n",
    "plt.plot(accuracy_kolepoll, label=\"Kolen-Pollack\", color='k')\n",
    "# plt.plot(accuracy_node_perturb, label=\"Node Perturbation\", color='c')\n",
    "plt.plot(accuracy_online, label=\"Online Learning\", color='gold')\n",
    "plt.plot(accuracy_nonstat, label=\"Non-stationary Data\", color='forestgreen')\n",
    "'''\n",
    "# plt.plot(accuracy_noisy_input, label=\"Noisy Input Data\", color='dodgerblue')\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "At iteration 1, the accuracy is 10.8\n",
      "At iteration 129, the accuracy is 7.9\n",
      "At iteration 257, the accuracy is 17.1\n",
      "At iteration 385, the accuracy is 11.700000000000001\n",
      "At iteration 513, the accuracy is 12.0\n",
      "At iteration 641, the accuracy is 9.6\n",
      "At iteration 769, the accuracy is 19.400000000000002\n",
      "At iteration 897, the accuracy is 10.5\n",
      "At iteration 1025, the accuracy is 16.5\n",
      "At iteration 1153, the accuracy is 29.4\n",
      "At iteration 1281, the accuracy is 10.0\n",
      "At iteration 1409, the accuracy is 24.4\n",
      "At iteration 1537, the accuracy is 38.2\n",
      "At iteration 1665, the accuracy is 22.2\n",
      "At iteration 1793, the accuracy is 30.5\n",
      "At iteration 1921, the accuracy is 30.8\n",
      "At iteration 2049, the accuracy is 43.2\n",
      "At iteration 2177, the accuracy is 34.8\n",
      "At iteration 2305, the accuracy is 14.499999999999998\n",
      "At iteration 2433, the accuracy is 34.1\n",
      "At iteration 2561, the accuracy is 27.0\n",
      "At iteration 2689, the accuracy is 35.0\n",
      "At iteration 2817, the accuracy is 41.0\n",
      "At iteration 2945, the accuracy is 27.800000000000004\n",
      "At iteration 3073, the accuracy is 33.1\n",
      "At iteration 3201, the accuracy is 53.7\n",
      "At iteration 3329, the accuracy is 55.50000000000001\n",
      "At iteration 3457, the accuracy is 50.2\n",
      "At iteration 3585, the accuracy is 35.8\n",
      "At iteration 3713, the accuracy is 36.9\n",
      "At iteration 3841, the accuracy is 55.400000000000006\n",
      "At iteration 3969, the accuracy is 49.7\n",
      "At iteration 4097, the accuracy is 44.5\n",
      "At iteration 4225, the accuracy is 49.0\n",
      "At iteration 4353, the accuracy is 55.7\n",
      "At iteration 4481, the accuracy is 52.800000000000004\n",
      "At iteration 4609, the accuracy is 53.300000000000004\n",
      "At iteration 4737, the accuracy is 60.3\n",
      "At iteration 4865, the accuracy is 52.800000000000004\n",
      "At iteration 4993, the accuracy is 60.199999999999996\n",
      "...completed  5000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  2.153 .\n",
      "At iteration 5121, the accuracy is 64.7\n",
      "At iteration 5249, the accuracy is 54.300000000000004\n",
      "At iteration 5377, the accuracy is 69.6\n",
      "At iteration 5505, the accuracy is 59.099999999999994\n",
      "At iteration 5633, the accuracy is 71.0\n",
      "At iteration 5761, the accuracy is 68.60000000000001\n",
      "At iteration 5889, the accuracy is 67.10000000000001\n",
      "At iteration 6017, the accuracy is 64.7\n",
      "At iteration 6145, the accuracy is 63.800000000000004\n",
      "At iteration 6273, the accuracy is 66.2\n",
      "At iteration 6401, the accuracy is 62.9\n",
      "At iteration 6529, the accuracy is 71.89999999999999\n",
      "At iteration 6657, the accuracy is 68.0\n",
      "At iteration 6785, the accuracy is 71.0\n",
      "At iteration 6913, the accuracy is 62.5\n",
      "At iteration 7041, the accuracy is 64.3\n",
      "At iteration 7169, the accuracy is 67.4\n",
      "At iteration 7297, the accuracy is 72.8\n",
      "At iteration 7425, the accuracy is 69.69999999999999\n",
      "At iteration 7553, the accuracy is 67.4\n",
      "At iteration 7681, the accuracy is 75.5\n",
      "At iteration 7809, the accuracy is 69.8\n",
      "At iteration 7937, the accuracy is 76.7\n",
      "At iteration 8065, the accuracy is 72.6\n",
      "At iteration 8193, the accuracy is 68.0\n",
      "At iteration 8321, the accuracy is 70.19999999999999\n",
      "At iteration 8449, the accuracy is 72.89999999999999\n",
      "At iteration 8577, the accuracy is 70.89999999999999\n",
      "At iteration 8705, the accuracy is 68.5\n",
      "At iteration 8833, the accuracy is 76.1\n",
      "At iteration 8961, the accuracy is 77.10000000000001\n",
      "At iteration 9089, the accuracy is 68.10000000000001\n",
      "At iteration 9217, the accuracy is 70.5\n",
      "At iteration 9345, the accuracy is 77.10000000000001\n",
      "At iteration 9473, the accuracy is 75.4\n",
      "At iteration 9601, the accuracy is 79.2\n",
      "At iteration 9729, the accuracy is 77.8\n",
      "At iteration 9857, the accuracy is 77.10000000000001\n",
      "At iteration 9985, the accuracy is 79.3\n",
      "...completed  10000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  1.997 .\n",
      "At iteration 10113, the accuracy is 76.1\n",
      "At iteration 10241, the accuracy is 73.5\n",
      "At iteration 10369, the accuracy is 75.4\n",
      "At iteration 10497, the accuracy is 77.9\n",
      "At iteration 10625, the accuracy is 80.80000000000001\n",
      "At iteration 10753, the accuracy is 80.2\n",
      "At iteration 10881, the accuracy is 78.2\n",
      "At iteration 11009, the accuracy is 78.7\n",
      "At iteration 11137, the accuracy is 78.5\n",
      "At iteration 11265, the accuracy is 76.8\n",
      "At iteration 11393, the accuracy is 72.89999999999999\n",
      "At iteration 11521, the accuracy is 74.9\n",
      "At iteration 11649, the accuracy is 79.2\n",
      "At iteration 11777, the accuracy is 78.2\n",
      "At iteration 11905, the accuracy is 76.9\n",
      "At iteration 12033, the accuracy is 79.7\n",
      "At iteration 12161, the accuracy is 84.2\n",
      "At iteration 12289, the accuracy is 77.8\n",
      "At iteration 12417, the accuracy is 80.5\n",
      "At iteration 12545, the accuracy is 76.7\n",
      "At iteration 12673, the accuracy is 77.60000000000001\n",
      "At iteration 12801, the accuracy is 80.80000000000001\n",
      "At iteration 12929, the accuracy is 80.0\n",
      "At iteration 13057, the accuracy is 79.4\n",
      "At iteration 13185, the accuracy is 83.1\n",
      "At iteration 13313, the accuracy is 80.5\n",
      "At iteration 13441, the accuracy is 81.0\n",
      "At iteration 13569, the accuracy is 84.3\n",
      "At iteration 13697, the accuracy is 81.89999999999999\n",
      "At iteration 13825, the accuracy is 80.0\n",
      "At iteration 13953, the accuracy is 84.7\n",
      "At iteration 14081, the accuracy is 79.9\n",
      "At iteration 14209, the accuracy is 78.2\n",
      "At iteration 14337, the accuracy is 80.80000000000001\n",
      "At iteration 14465, the accuracy is 80.7\n",
      "At iteration 14593, the accuracy is 83.5\n",
      "At iteration 14721, the accuracy is 81.8\n",
      "At iteration 14849, the accuracy is 84.1\n",
      "At iteration 14977, the accuracy is 84.7\n",
      "...completed  15000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  1.9136 .\n",
      "At iteration 15105, the accuracy is 83.1\n",
      "At iteration 15233, the accuracy is 81.8\n",
      "At iteration 15361, the accuracy is 83.5\n",
      "At iteration 15489, the accuracy is 81.39999999999999\n",
      "At iteration 15617, the accuracy is 80.7\n",
      "At iteration 15745, the accuracy is 83.8\n",
      "At iteration 15873, the accuracy is 84.7\n",
      "At iteration 16001, the accuracy is 79.60000000000001\n",
      "At iteration 16129, the accuracy is 83.3\n",
      "At iteration 16257, the accuracy is 83.39999999999999\n",
      "At iteration 16385, the accuracy is 84.1\n",
      "At iteration 16513, the accuracy is 85.2\n",
      "At iteration 16641, the accuracy is 83.1\n",
      "At iteration 16769, the accuracy is 83.1\n",
      "At iteration 16897, the accuracy is 83.8\n",
      "At iteration 17025, the accuracy is 84.89999999999999\n",
      "At iteration 17153, the accuracy is 85.1\n",
      "At iteration 17281, the accuracy is 84.3\n",
      "At iteration 17409, the accuracy is 83.8\n",
      "At iteration 17537, the accuracy is 84.7\n",
      "At iteration 17665, the accuracy is 85.2\n",
      "At iteration 17793, the accuracy is 84.2\n",
      "At iteration 17921, the accuracy is 85.5\n",
      "At iteration 18049, the accuracy is 86.5\n",
      "At iteration 18177, the accuracy is 84.5\n",
      "At iteration 18305, the accuracy is 83.6\n",
      "At iteration 18433, the accuracy is 82.69999999999999\n",
      "At iteration 18561, the accuracy is 83.0\n",
      "At iteration 18689, the accuracy is 85.2\n",
      "At iteration 18817, the accuracy is 84.8\n",
      "At iteration 18945, the accuracy is 83.39999999999999\n",
      "At iteration 19073, the accuracy is 83.5\n",
      "At iteration 19201, the accuracy is 85.1\n",
      "At iteration 19329, the accuracy is 82.39999999999999\n",
      "At iteration 19457, the accuracy is 82.3\n",
      "At iteration 19585, the accuracy is 86.3\n",
      "At iteration 19713, the accuracy is 84.7\n",
      "At iteration 19841, the accuracy is 85.6\n",
      "At iteration 19969, the accuracy is 82.69999999999999\n",
      "...completed  20000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  1.8817 .\n",
      "At iteration 20097, the accuracy is 83.2\n",
      "At iteration 20225, the accuracy is 86.0\n",
      "At iteration 20353, the accuracy is 85.5\n",
      "At iteration 20481, the accuracy is 85.9\n",
      "At iteration 20609, the accuracy is 86.4\n",
      "At iteration 20737, the accuracy is 86.7\n",
      "At iteration 20865, the accuracy is 85.5\n",
      "At iteration 20993, the accuracy is 85.9\n",
      "At iteration 21121, the accuracy is 85.1\n",
      "At iteration 21249, the accuracy is 85.8\n",
      "At iteration 21377, the accuracy is 83.7\n",
      "At iteration 21505, the accuracy is 86.7\n",
      "At iteration 21633, the accuracy is 87.1\n",
      "At iteration 21761, the accuracy is 85.7\n",
      "At iteration 21889, the accuracy is 85.8\n",
      "At iteration 22017, the accuracy is 86.8\n",
      "At iteration 22145, the accuracy is 85.6\n",
      "At iteration 22273, the accuracy is 86.1\n",
      "At iteration 22401, the accuracy is 86.7\n",
      "At iteration 22529, the accuracy is 86.5\n",
      "At iteration 22657, the accuracy is 87.0\n",
      "At iteration 22785, the accuracy is 85.6\n",
      "At iteration 22913, the accuracy is 86.7\n",
      "At iteration 23041, the accuracy is 86.7\n",
      "At iteration 23169, the accuracy is 86.5\n",
      "At iteration 23297, the accuracy is 87.5\n",
      "At iteration 23425, the accuracy is 86.4\n",
      "At iteration 23553, the accuracy is 86.8\n",
      "At iteration 23681, the accuracy is 85.5\n",
      "At iteration 23809, the accuracy is 85.5\n",
      "At iteration 23937, the accuracy is 86.8\n",
      "At iteration 24065, the accuracy is 87.6\n",
      "At iteration 24193, the accuracy is 85.2\n",
      "At iteration 24321, the accuracy is 86.0\n",
      "At iteration 24449, the accuracy is 86.8\n",
      "At iteration 24577, the accuracy is 86.1\n",
      "At iteration 24705, the accuracy is 86.0\n",
      "At iteration 24833, the accuracy is 87.2\n",
      "At iteration 24961, the accuracy is 87.9\n",
      "...completed  25000  iterations (corresponding to 1 epoch) of training data (single images). Current loss:  1.8118 .\n",
      "At iteration 25089, the accuracy is 87.5\n",
      "At iteration 25217, the accuracy is 86.7\n",
      "At iteration 25345, the accuracy is 87.6\n",
      "At iteration 25473, the accuracy is 87.4\n",
      "At iteration 25601, the accuracy is 87.2\n",
      "At iteration 25729, the accuracy is 87.7\n",
      "At iteration 25857, the accuracy is 86.3\n",
      "At iteration 25985, the accuracy is 86.3\n",
      "At iteration 26113, the accuracy is 87.1\n",
      "At iteration 26241, the accuracy is 85.1\n",
      "At iteration 26369, the accuracy is 87.2\n",
      "At iteration 26497, the accuracy is 87.4\n",
      "At iteration 26625, the accuracy is 87.4\n",
      "At iteration 26753, the accuracy is 86.6\n",
      "At iteration 26881, the accuracy is 86.6\n",
      "At iteration 27009, the accuracy is 87.3\n",
      "At iteration 27137, the accuracy is 84.8\n",
      "At iteration 27265, the accuracy is 85.9\n",
      "At iteration 27393, the accuracy is 86.2\n",
      "At iteration 27521, the accuracy is 86.8\n",
      "At iteration 27649, the accuracy is 87.6\n",
      "At iteration 27777, the accuracy is 87.2\n",
      "At iteration 27905, the accuracy is 87.2\n",
      "At iteration 28033, the accuracy is 87.5\n",
      "At iteration 28161, the accuracy is 87.3\n",
      "At iteration 28289, the accuracy is 87.5\n",
      "At iteration 28417, the accuracy is 87.8\n",
      "At iteration 28545, the accuracy is 87.3\n",
      "At iteration 28673, the accuracy is 86.4\n",
      "At iteration 28801, the accuracy is 88.5\n",
      "At iteration 28929, the accuracy is 88.0\n",
      "At iteration 29057, the accuracy is 88.2\n",
      "At iteration 29185, the accuracy is 87.9\n",
      "At iteration 29313, the accuracy is 87.2\n",
      "At iteration 29441, the accuracy is 87.8\n",
      "...completed  29441  iterations of training data (single images). Current loss:  1.56\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Online learning\n",
    "net_bp_online = MLP.MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_online, accuracy_bp_online, test_loss_bp_online, snr_bp_online, cosine_similarity_bp_online) = \\\n",
    "    net_bp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=100000, conv_loss = 1.75, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232,) (231,) (231,) 0.10301087401898926 (231, 2)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/net_bp_online/losses_bp_normal.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(losses_bp_online)\u001b[38;5;241m.\u001b[39mshape, \n\u001b[0;32m      3\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(accuracy_bp_online)\u001b[38;5;241m.\u001b[39mshape, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     np\u001b[38;5;241m.\u001b[39masarray(cosine_similarity_bp_online)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/net_bp_online/losses_bp_normal.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses_bp_online\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/net_bp_online/accuracy_bp_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(accuracy_bp_online))\n\u001b[0;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/net_bp_online/test_loss_bp_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(test_loss_bp_online))\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/net_bp_online/losses_bp_normal.npy'"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.asarray(losses_bp_online).shape, \n",
    "    np.asarray(accuracy_bp_online).shape, \n",
    "    np.asarray(test_loss_bp_online).shape, \n",
    "    np.asarray(snr_bp_online), \n",
    "    np.asarray(cosine_similarity_bp_online).shape\n",
    ")\n",
    "np.save('results/net_bp_online/losses_bp_normal.npy', np.asarray(losses_bp_online))\n",
    "np.save('results/net_bp_online/accuracy_bp_normal.npy', np.asarray(accuracy_bp_online))\n",
    "np.save('results/net_bp_online/test_loss_bp_normal.npy', np.asarray(test_loss_bp_online))\n",
    "np.save('results/net_bp_online/snr_bp_normal.npy', np.asarray(snr_bp_online))\n",
    "np.save('results/net_bp_online/cosine_similarity_bp_normal.npy', np.asarray(cosine_similarity_bp_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.92 GiB for an array with shape (1000, 500, 785) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Noisy Input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# create a network and train it using backprop\u001b[39;00m\n\u001b[0;32m      3\u001b[0m netbackprop_noisy \u001b[38;5;241m=\u001b[39m MLP\u001b[38;5;241m.\u001b[39mMLP(rng, numhidden, sigma\u001b[38;5;241m=\u001b[39minitweight, activation\u001b[38;5;241m=\u001b[39mactivation)\n\u001b[0;32m      4\u001b[0m (losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr_bp_noisy, cosine_similarity_bp_noisy) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mnetbackprop_noisy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearnrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackprop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnoise_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgauss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrep_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\MLP.py:393\u001b[0m, in \u001b[0;36mMLP.train\u001b[1;34m(self, rng, images, labels, num_epochs, test_images, test_labels, learning_rate, batch_size, algorithm, noise, noise_type, report, report_rate)\u001b[0m\n\u001b[0;32m    391\u001b[0m     targets \u001b[38;5;241m=\u001b[39m test_labels[:, [t]]\n\u001b[0;32m    392\u001b[0m     grad[t, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_grad(rng, inputs, targets, algorithm\u001b[38;5;241m=\u001b[39malgorithm, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[1;32m--> 393\u001b[0m snr \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_grad_snr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgauss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms&p\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    396\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malter_inputs(np\u001b[38;5;241m.\u001b[39mcopy(images), noise_type)\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\helpers.py:98\u001b[0m, in \u001b[0;36mcalculate_grad_snr\u001b[1;34m(grad, epsilon)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_grad_snr\u001b[39m(grad, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m):\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    Calculate the average SNR |mean|/std across all parameters in a gradient update\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mmean(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m epsilon))\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3645\u001b[0m, in \u001b[0;36mstd\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m   3642\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3643\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m std(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3646\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\core\\_methods.py:206\u001b[0m, in \u001b[0;36m_std\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    205\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 206\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m               \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    210\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
      "File \u001b[1;32md:\\.conda\\envs\\neuromatch\\Lib\\site-packages\\numpy\\core\\_methods.py:173\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    168\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marrmean\u001b[49m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.92 GiB for an array with shape (1000, 500, 785) and data type float64"
     ]
    }
   ],
   "source": [
    "# Noisy Input\n",
    "# create a network and train it using backprop\n",
    "netbackprop_noisy = MLP.MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_noisy, accuracy_bp_noisy, test_loss_bp_noisy, snr_bp_noisy, cosine_similarity_bp_noisy) = \\\n",
    "    netbackprop_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_bp_noisy.shape, accuracy_bp_noisy.shape, test_loss_bp_noisy.shape, snr_bp_noisy, cosine_similarity_bp_noisy.shape\n",
    "np.save('results/netbackprop/losses_bp_normal.npy', losses_bp_noisy)\n",
    "np.save('results/netbackprop/accuracy_bp_normal.npy', accuracy_bp_noisy)\n",
    "np.save('results/netbackprop/test_loss_bp_normal.npy', test_loss_bp_noisy)\n",
    "np.save('results/netbackprop/snr_bp_normal.npy', snr_bp_noisy)\n",
    "np.save('results/netbackprop/cosine_similarity_bp_normal.npy', cosine_similarity_bp_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_bp_nonstat = MLP.MLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_bp_nonstat, accuracy_bp_nonstat, test_loss_bp_nonstat, snr_bp_nonstat, cosine_similarity_bp_nonstat) = \\\n",
    "    net_bp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='backprop', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "device: cpu\n",
      "input:\n",
      "  path: datasets\n",
      "  batch_size: 128\n",
      "model:\n",
      "  peer_normalization: 0.03\n",
      "  momentum: 0.9\n",
      "  hidden_dim: 500\n",
      "  num_layers: 2\n",
      "training:\n",
      "  epochs: 100\n",
      "  learning_rate: 0.001\n",
      "  weight_decay: 0\n",
      "  momentum: 0\n",
      "  downstream_learning_rate: 0.001\n",
      "  downstream_weight_decay: 0\n",
      "  val_idx: 0\n",
      "  final_test: true\n",
      "\n",
      "FF_model(\n",
      "  (model): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      "  (ff_loss): BCEWithLogitsLoss()\n",
      "  (linear_classifier): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=10, bias=False)\n",
      "  )\n",
      "  (classification_loss): CrossEntropyLoss()\n",
      ") \n",
      "\n",
      "...completed  0  epochs of training. \n",
      "\n",
      "            Current innate loss:  58.63556840358636, \n",
      "\n",
      "            Current training cross_entropy loss:  1.7526681716625507, \n",
      "\n",
      "            Current testing cross_entropy loss:  1.2275530099868774, \n",
      "   \n",
      "            Current testing accuracy:  0.7907 \n",
      "         \n",
      "            \n",
      "...completed  1  epochs of training. \n",
      "\n",
      "            Current innate loss:  44.71706792146732, \n",
      "\n",
      "            Current training cross_entropy loss:  1.343608803015489, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.7227679491043091, \n",
      "   \n",
      "            Current testing accuracy:  0.8537 \n",
      "         \n",
      "            \n",
      "...completed  2  epochs of training. \n",
      "\n",
      "            Current innate loss:  35.94339129822886, \n",
      "\n",
      "            Current training cross_entropy loss:  1.1089714631820335, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.5488615036010742, \n",
      "   \n",
      "            Current testing accuracy:  0.8784 \n",
      "         \n",
      "            \n",
      "...completed  3  epochs of training. \n",
      "\n",
      "            Current innate loss:  29.887251834991652, \n",
      "\n",
      "            Current training cross_entropy loss:  0.9642073383507056, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.4746421277523041, \n",
      "   \n",
      "            Current testing accuracy:  0.8913 \n",
      "         \n",
      "            \n",
      "...completed  4  epochs of training. \n",
      "\n",
      "            Current innate loss:  25.790816731697475, \n",
      "\n",
      "            Current training cross_entropy loss:  0.8651232407643245, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.42174336314201355, \n",
      "   \n",
      "            Current testing accuracy:  0.8984 \n",
      "         \n",
      "            \n",
      "...completed  5  epochs of training. \n",
      "\n",
      "            Current innate loss:  22.71232744241372, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7925254569603847, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.39569881558418274, \n",
      "   \n",
      "            Current testing accuracy:  0.9032 \n",
      "         \n",
      "            \n",
      "...completed  6  epochs of training. \n",
      "\n",
      "            Current innate loss:  20.47049979718177, \n",
      "\n",
      "            Current training cross_entropy loss:  0.7362035607134466, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3689563572406769, \n",
      "   \n",
      "            Current testing accuracy:  0.9095 \n",
      "         \n",
      "            \n",
      "...completed  7  epochs of training. \n",
      "\n",
      "            Current innate loss:  18.512340567089044, \n",
      "\n",
      "            Current training cross_entropy loss:  0.691766263009646, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3579031825065613, \n",
      "   \n",
      "            Current testing accuracy:  0.9123 \n",
      "         \n",
      "            \n",
      "...completed  8  epochs of training. \n",
      "\n",
      "            Current innate loss:  17.03147812463619, \n",
      "\n",
      "            Current training cross_entropy loss:  0.6551408405614715, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3399561643600464, \n",
      "   \n",
      "            Current testing accuracy:  0.916 \n",
      "         \n",
      "            \n",
      "...completed  9  epochs of training. \n",
      "\n",
      "            Current innate loss:  15.832343904696978, \n",
      "\n",
      "            Current training cross_entropy loss:  0.624267397549672, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3215690851211548, \n",
      "   \n",
      "            Current testing accuracy:  0.9195 \n",
      "         \n",
      "            \n",
      "...completed  10  epochs of training. \n",
      "\n",
      "            Current innate loss:  14.789481434232982, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5976818415564257, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3121406137943268, \n",
      "   \n",
      "            Current testing accuracy:  0.9202 \n",
      "         \n",
      "            \n",
      "...completed  11  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.796160554376423, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5750501604225391, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.3080938458442688, \n",
      "   \n",
      "            Current testing accuracy:  0.9227 \n",
      "         \n",
      "            \n",
      "...completed  12  epochs of training. \n",
      "\n",
      "            Current innate loss:  13.044622030542682, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5548375979124677, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2955418527126312, \n",
      "   \n",
      "            Current testing accuracy:  0.9235 \n",
      "         \n",
      "            \n",
      "...completed  13  epochs of training. \n",
      "\n",
      "            Current innate loss:  12.338397360099105, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5370463697765118, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.28331825137138367, \n",
      "   \n",
      "            Current testing accuracy:  0.9245 \n",
      "         \n",
      "            \n",
      "...completed  14  epochs of training. \n",
      "\n",
      "            Current innate loss:  11.736596516890403, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5209330544359664, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2844664454460144, \n",
      "   \n",
      "            Current testing accuracy:  0.9248 \n",
      "         \n",
      "            \n",
      "...completed  15  epochs of training. \n",
      "\n",
      "            Current innate loss:  11.225830697917786, \n",
      "\n",
      "            Current training cross_entropy loss:  0.5064669040031731, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2718605101108551, \n",
      "   \n",
      "            Current testing accuracy:  0.9267 \n",
      "         \n",
      "            \n",
      "...completed  16  epochs of training. \n",
      "\n",
      "            Current innate loss:  10.737181999438848, \n",
      "\n",
      "            Current training cross_entropy loss:  0.49326414424100073, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2706184983253479, \n",
      "   \n",
      "            Current testing accuracy:  0.9271 \n",
      "         \n",
      "            \n",
      "...completed  17  epochs of training. \n",
      "\n",
      "            Current innate loss:  10.296551365169705, \n",
      "\n",
      "            Current training cross_entropy loss:  0.4812522433719404, \n",
      "\n",
      "            Current testing cross_entropy loss:  0.2650071680545807, \n",
      "   \n",
      "            Current testing accuracy:  0.9284 \n",
      "         \n",
      "            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     GlobalHydra()\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     10\u001b[0m netffa \u001b[38;5;241m=\u001b[39m net_ff_model\u001b[38;5;241m.\u001b[39mnet_FF_model(rng)\n\u001b[1;32m---> 11\u001b[0m (losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal) \u001b[38;5;241m=\u001b[39m \u001b[43mnetffa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_over\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mff_com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcross_entropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\net_ff_model.py:118\u001b[0m, in \u001b[0;36mnet_FF_model.train_over\u001b[1;34m(self, train_images, train_targets, test_images, test_targets, batch_size, epochs, model, return_loss)\u001b[0m\n\u001b[0;32m    112\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_conversion(inputs, labels, model)\n\u001b[0;32m    114\u001b[0m     running_real_losses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch(inputs, labels, model)\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m     running_return_losses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 118\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_inference_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    120\u001b[0m real_losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(running_real_losses))\n\u001b[0;32m    121\u001b[0m return_losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(running_return_losses))\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\net_ff_model.py:384\u001b[0m, in \u001b[0;36mnet_FF_model._get_inference_loss\u001b[1;34m(self, inputs, labels, model, return_loss)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_inference_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, labels, \n\u001b[0;32m    382\u001b[0m                         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_sim\u001b[39m\u001b[38;5;124m'\u001b[39m, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_entropy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_com\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 384\u001b[0m         preds_ori \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_ori\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m return_loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    386\u001b[0m             preds_ori \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(preds_ori, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\net_ff_model.py:362\u001b[0m, in \u001b[0;36mnet_FF_model.inference_ori\u001b[1;34m(self, inputs, labels, model)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference_ori\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, labels, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_sim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_com\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 362\u001b[0m         scalar_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_downstream_classification_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m scalar_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_sim\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\MyFolders\\project\\2024summer\\NeuroAI\\NMA-Microlearning-Project\\forward_forward_complex\\src\\ff_model.py:128\u001b[0m, in \u001b[0;36mFF_model.forward_downstream_classification_model\u001b[1;34m(self, inputs, labels, scalar_outputs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_downstream_classification_model\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs, labels, scalar_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scalar_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m         scalar_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    129\u001b[0m         }\n\u001b[0;32m    131\u001b[0m     z \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    132\u001b[0m     z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mreshape(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Normal learning\n",
    "# create a network and train it using ffa\n",
    "# netffa = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal, snr_ffa_normal, cosine_similarity_ffa_normal) = \\\n",
    "#     netffa.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=rep_rate)\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "netffa = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_normal, accuracy_ffa_normal, test_loss_ffa_normal) = netffa.train_over(\n",
    "    train_images, train_labels, \n",
    "    test_images, test_labels,\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlosses_ffa_normal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, accuracy_ffa_normal\u001b[38;5;241m.\u001b[39mshape, test_loss_ffa_normal\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/netffa/losses_ffa_normal.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39masarray(losses_ffa_normal))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "losses_ffa_normal.shape, accuracy_ffa_normal.shape, test_loss_ffa_normal.shape\n",
    "np.save('results/netffa/losses_ffa_normal.npy', np.asarray(losses_ffa_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "netffa_online = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_online, accuracy_ffa_online, test_loss_ffa_online) = netffa_online.train_online(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    max_it=25000, conv_loss=0.1, \n",
    "    report_rate=100, lr=0.01,\n",
    "    model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_ffa_online.shape, accuracy_ffa_online.shape, test_loss_ffa_online.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network and train it using ffa\n",
    "# net_ffa_noisy = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy, snr_ffa_noisy, cosine_similarity_ffa_noisy) = \\\n",
    "#     net_ffa_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       noise_type='gauss',report=report, report_rate=rep_rate)\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "net_ffa_noisy = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_noisy, accuracy_ffa_noisy, test_loss_ffa_noisy) = net_ffa_noisy.train_nonstationary(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# net_ffa_nonstat = MLP(rng_bp2, numhidden, sigma=initweight, activation=activation)\n",
    "# (losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat, snr_ffa_nonstat, cosine_similarity_ffa_nonstat) = \\\n",
    "#     net_ffa_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=learnrate, batch_size=batchsize, algorithm='ffa', noise=noise, \\\n",
    "#                       report=report, report_rate=1)\n",
    "if GlobalHydra().is_initialized():\n",
    "    GlobalHydra().clear()\n",
    "net_ffa_nonstat = net_ff_model.net_FF_model(rng)\n",
    "(losses_ffa_nonstat, accuracy_ffa_nonstat, test_loss_ffa_nonstat) = net_ffa_nonstat.train_noisydata(\n",
    "    train_images, train_labels, \n",
    "    test_images[:, indices], test_labels[:, indices],\n",
    "    epochs=numepochs, model='ff_com', return_loss='cross_entropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/test/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_test = data[0]\n",
    "accuracy_node_perturb_test = data[1]\n",
    "test_losses_node_perturb_test = data[2]\n",
    "snr_node_perturb_test = data[3]\n",
    "cosine_similarity_node_perturb_test = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "# create a network and train it using node perturbation\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     netnodeperturb = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr_np_normal, cosine_similarity_np_normal) = \\\n",
    "#         netnodeperturb.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                              learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, report=report, report_rate=rep_rate)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/normal/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_normal = data[0]\n",
    "accuracy_node_perturb_normal = data[1]\n",
    "test_losses_node_perturb_normal = data[2]\n",
    "snr_node_perturb_normal = data[3]\n",
    "cosine_similarity_node_perturb_normal = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     netnodeperturb_online = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_online, accuracy_np_online, test_loss_np_online, snr_np_online, cosine_similarity_np_online) = \\\n",
    "#         netnodeperturb_online.train_online(rng_bp2, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                       learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-4, algorithm='node_perturb', noise=noise, \\\n",
    "#                       report=report, report_rate=batchsize)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/online/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_online = data[0]\n",
    "accuracy_node_perturb_online = data[1]\n",
    "test_losses_node_perturb_online = data[2]\n",
    "snr_node_perturb_online = data[3]\n",
    "cosine_similarity_node_perturb_online = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     nodeperturb_noisy = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr_np_noisy, cosine_similarity_np_noisy) = \\\n",
    "#         nodeperturb_noisy.train(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                         learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, \\\n",
    "#                         noise_type='gauss',report=report, report_rate=rep_rate)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/noisy/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_noisy = data[0]\n",
    "accuracy_node_perturb_noisy = data[1]\n",
    "test_losses_node_perturb_noisy = data[2]\n",
    "snr_node_perturb_noisy = data[3]\n",
    "cosine_similarity_node_perturb_noisy = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "# with contextlib.redirect_stdout(io.StringIO()):\n",
    "#     nodeperturb_nonstat = NodePerturbMLP(rng_bp2, numhidden, num_inputs = 784, sigma=initweight, activation=activation)\n",
    "#     (losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr_np_nonstat, cosine_similarity_np_nonstat) = \\\n",
    "#         nodeperturb_nonstat.train_nonstat_data(rng_bp2, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "#                         learning_rate=learnrate, batch_size=batchsize, algorithm='node_perturb', noise=noise, \\\n",
    "#                         report=report, report_rate=1)\n",
    "\n",
    "# to load the files again, in the main doc:\n",
    "filenames= [\"losses_node_perturb\", \"accuracy_node_perturb\", \"test_losses_node_perturb\", \"snr_node_perturb\", \"cosine_similarity_node_perturb\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, name in enumerate(filenames):\n",
    "  data.append(np.fromfile(f\"data/node_perturb/non-stat/{filenames[i]}.csv\", dtype=float, sep=\",\"))\n",
    "\n",
    "losses_node_perturb_non_stat = data[0]\n",
    "accuracy_node_perturb_non_stat = data[1]\n",
    "test_losses_node_perturb_non_stat = data[2]\n",
    "snr_node_perturb_non_stat = data[3]\n",
    "cosine_similarity_node_perturb_non_stat = data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolen-Pollack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Learning\n",
    "netkolepoll = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_normal, accuracy_kp_normal, test_loss_kp_normal, snr_kp_normal, cosine_similarity_kp_normal) = \\\n",
    "    netkolepoll.train(rng_kp, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=rep_rate)\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning\n",
    "net_kp_online = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_online, accuracy_kp_online, test_loss_kp_online, snr_kp_online, cosine_similarity_kp_online) = \\\n",
    "    net_kp_online.train_online(rng, train_images, train_labels, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=0.01, max_it=numupdates*batchsize, conv_loss = 1e-1, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "net_kp_noisy = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_noisy, accuracy_kp_noisy, test_loss_kp_noisy, snr_kp_noisy, cosine_similarity_kp_noisy) = \\\n",
    "    net_kp_noisy.train(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      noise_type='gauss',report=report, report_rate=rep_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Stationary Data\n",
    "net_kp_nonstat = KolenPollackMLP(rng, numhidden, sigma=initweight, activation=activation)\n",
    "(losses_kp_nonstat, accuracy_kp_nonstat, test_loss_kp_nonstat, snr_kp_nonstat, cosine_similarity_kp_nonstat) = \\\n",
    "    net_kp_nonstat.train_nonstat_data(rng, train_images, train_labels, numepochs, test_images[:, indices], test_labels[:, indices], \\\n",
    "                      learning_rate=learnrate, batch_size=batchsize, algorithm='kolepoll', noise=noise, \\\n",
    "                      report=report, report_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run without node perturb and ffa\n",
    "# FFA\n",
    "snr_ffa_normal, cosine_similarity_ffa_normal = [0], [0]\n",
    "snr_ffa_online, cosine_similarity_ffa_online = [0], [0]\n",
    "snr_ffa_noisy, cosine_similarity_ffa_noisy = [0], [0]\n",
    "snr_ffa_nonstat, cosine_similarity_ffa_nonstat = [0], [0]\n",
    "\n",
    "# Node Perturb\n",
    "losses_np_normal, accuracy_np_normal, test_loss_np_normal, snr_np_normal, cosine_similarity_np_normal = [0], [0], [0], [0], [0]\n",
    "losses_np_online, accuracy_np_online, test_loss_np_online, snr_np_online, cosine_similarity_np_online = [0], [0], [0], [0], [0]\n",
    "losses_np_noisy, accuracy_np_noisy, test_loss_np_noisy, snr_np_noisy, cosine_similarity_np_noisy = [0], [0], [0], [0], [0]\n",
    "losses_np_nonstat, accuracy_np_nonstat, test_loss_np_nonstat, snr_np_nonstat, cosine_similarity_np_nonstat = [0], [0], [0], [0], [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays\n",
    "# normal \n",
    "tr_loss_normal = [losses_bp_normal, losses_ffa_normal, losses_np_normal, losses_kp_normal]\n",
    "te_acc_normal = [accuracy_bp_normal, accuracy_ffa_normal, accuracy_np_normal, accuracy_kp_normal]\n",
    "te_loss_normal = [test_loss_bp_normal, test_loss_ffa_normal, test_loss_np_normal, test_loss_kp_normal]\n",
    "snr_normal = [snr_bp_normal, snr_ffa_normal, snr_np_normal, snr_kp_normal]\n",
    "cosine_similarity_normal = [cosine_similarity_bp_normal, cosine_similarity_ffa_normal, cosine_similarity_np_normal, cosine_similarity_kp_normal]\n",
    "\n",
    "# online\n",
    "# Calculate the moving average\n",
    "window_size = 100\n",
    "losses_bp_online_mean = uniform_filter1d(losses_bp_online, size=window_size)\n",
    "losses_ffa_online_mean = uniform_filter1d(losses_ffa_online, size=window_size)\n",
    "losses_np_online_mean = uniform_filter1d(losses_np_online, size=window_size)\n",
    "losses_kp_online_mean = uniform_filter1d(losses_kp_online, size=window_size)\n",
    "tr_loss_online_mean = [losses_bp_online_mean, losses_ffa_online_mean, losses_np_online_mean, losses_kp_online_mean]\n",
    "\n",
    "tr_loss_online = [losses_bp_online, losses_ffa_online, losses_np_online, losses_kp_online]\n",
    "te_acc_online = [accuracy_bp_online, accuracy_ffa_online, accuracy_np_online, accuracy_kp_online]\n",
    "te_loss_online = [test_loss_bp_online, test_loss_ffa_online, test_loss_np_online, test_loss_kp_online]\n",
    "snr_online = [snr_bp_online, snr_ffa_online, snr_np_online, snr_kp_online]\n",
    "cosine_similarity_online = [cosine_similarity_bp_online, cosine_similarity_ffa_online, cosine_similarity_np_online, cosine_similarity_kp_online]\n",
    "\n",
    "# noisy\n",
    "tr_loss_noisy = [losses_bp_noisy, losses_ffa_noisy, losses_np_noisy, losses_kp_noisy]\n",
    "te_acc_noisy = [accuracy_bp_noisy, accuracy_ffa_noisy, accuracy_np_noisy, accuracy_kp_noisy]\n",
    "te_loss_noisy = [test_loss_bp_noisy, test_loss_ffa_noisy, test_loss_np_noisy, test_loss_kp_noisy]\n",
    "snr_noisy = [snr_bp_noisy, snr_ffa_noisy, snr_np_noisy, snr_kp_noisy]\n",
    "cosine_similarity_noisy = [cosine_similarity_bp_noisy, cosine_similarity_ffa_noisy, cosine_similarity_np_noisy, cosine_similarity_kp_noisy]\n",
    "\n",
    "# nonstat\n",
    "tr_loss_nonstat = [losses_bp_nonstat, losses_ffa_nonstat, losses_np_nonstat, losses_kp_nonstat]\n",
    "te_acc_nonstat = [accuracy_bp_nonstat, accuracy_ffa_nonstat, accuracy_np_nonstat, accuracy_kp_nonstat]\n",
    "te_loss_nonstat = [test_loss_bp_nonstat, test_loss_ffa_nonstat, test_loss_np_nonstat, test_loss_kp_nonstat]\n",
    "snr_nonstat = [snr_bp_nonstat, snr_ffa_nonstat, snr_np_nonstat, snr_kp_nonstat]\n",
    "cosine_similarity_nonstat = [cosine_similarity_bp_nonstat, cosine_similarity_ffa_nonstat, cosine_similarity_np_nonstat, cosine_similarity_kp_nonstat]\n",
    "\n",
    "# algorithms\n",
    "# algos = ['normal', 'online', 'noisy', 'nonstat']\n",
    "algos = ['normal', 'noisy', 'nonstat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "for i, algo in enumerate(algos):\n",
    "    if algo == 'normal':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_normal, te_acc_normal, te_loss_normal, snr_normal, cosine_similarity_normal\n",
    "    # elif algo == 'online':\n",
    "    #     tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_online, te_acc_online, te_loss_online, snr_online, cosine_similarity_online\n",
    "    elif algo == 'noisy':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_noisy, te_acc_noisy, te_loss_noisy, snr_noisy, cosine_similarity_noisy\n",
    "    elif algo == 'nonstat':\n",
    "        tr_loss, te_acc, te_loss, snr, cos_sim = tr_loss_nonstat, te_acc_nonstat, te_loss_nonstat, snr_nonstat, cosine_similarity_nonstat\n",
    "        \n",
    "    # plot\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(151)\n",
    "\n",
    "    plt.plot(tr_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(tr_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(tr_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(tr_loss[3], label=\"Kolen-Pollack\", color='k')\n",
    "    \n",
    "    plt.xlabel(\"Updates (every batch)\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training loss\")\n",
    "\n",
    "    plt.subplot(152)\n",
    "    plt.plot(te_acc[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_acc[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_acc[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_acc[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Performance over time\")\n",
    "\n",
    "    plt.subplot(153)\n",
    "    plt.plot(te_loss[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(te_loss[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(te_loss[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(te_loss[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Test loss\")\n",
    "\n",
    "    plt.subplot(154)\n",
    "    with plt.xkcd():\n",
    "            x = [0, 1, 2, 3]\n",
    "            snr_vals = [snr[1], snr[2], snr[3], snr[0]]\n",
    "            colors = ['gold', 'c', 'k', 'r']\n",
    "            labels = ['FFA', 'Node Perturbation', 'Kolen Pollack', 'Backprop']\n",
    "            plt.bar(x, snr_vals, color=colors, tick_label=labels)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.ylabel('SNR')\n",
    "            plt.xlabel('Algorithm')\n",
    "            plt.title('Gradient SNR')\n",
    "\n",
    "    plt.subplot(155)\n",
    "    with plt.xkcd():\n",
    "        plt.plot(cos_sim[0], label=\"Backprop\", color='r')\n",
    "        plt.plot(cos_sim[1], label=\"FFA\", color='gold')\n",
    "        plt.plot(cos_sim[2], label=\"Node Perturbation\", color='c')\n",
    "        plt.plot(cos_sim[3], label=\"Kolen-Pollack\", color='k')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cosine Sim\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Cosine Similarity to Backprop\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Learning separately becuase of different lenghts and additional smoothing operation\n",
    "# plot\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(151)\n",
    "\n",
    "plt.plot(tr_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(tr_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(tr_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(tr_loss_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "\n",
    "plt.xlabel(\"Updates (every batch)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Training loss\")\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(te_acc_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_acc_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_acc_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_acc_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Performance over time\")\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(te_loss_online[0], label=\"Backprop\", color='r')\n",
    "plt.plot(te_loss_online[1], label=\"FFA\", color='gold')\n",
    "plt.plot(te_loss_online[2], label=\"Node Perturbation\", color='c')\n",
    "plt.plot(te_loss_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Test loss\")\n",
    "\n",
    "plt.subplot(154)\n",
    "with plt.xkcd():\n",
    "        x = [0, 1, 2, 3]\n",
    "        snr_vals = [snr_online[1], snr_online[2], snr_online[3], snr_online[0]]\n",
    "        colors = ['gold', 'c', 'k', 'r']\n",
    "        labels = ['FFA', 'Node Perturbation', 'Kolen Pollack', 'Backprop']\n",
    "        plt.bar(x, snr_vals, color=colors, tick_label=labels)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('SNR')\n",
    "        plt.xlabel('Algorithm')\n",
    "        plt.title('Gradient SNR')\n",
    "\n",
    "plt.subplot(155)\n",
    "with plt.xkcd():\n",
    "    plt.plot(cosine_similarity_online[0], label=\"Backprop\", color='r')\n",
    "    plt.plot(cosine_similarity_online[1], label=\"FFA\", color='gold')\n",
    "    plt.plot(cosine_similarity_online[2], label=\"Node Perturbation\", color='c')\n",
    "    plt.plot(cosine_similarity_online[3], label=\"Kolen-Pollack\", color='k')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cosine Sim\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cosine Similarity to Backprop\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
